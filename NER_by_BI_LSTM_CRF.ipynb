{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_by_BI_LSTM_CRF_Cantemist_Competicion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5Mt4hDPDvdSb",
        "tagsZfDrd1lM",
        "WUJWVCSt1vaK",
        "iSUH0wMWG0zR",
        "s5MrHj696mhm",
        "vRf4BOW56rK7",
        "8V2lAEJV7AOi",
        "UhKmxb0hl13V",
        "L-mN5DYWZ0uh",
        "EaHqdexo5E3g",
        "o6K2WvTU8beY",
        "lRq1HLOuDBx8",
        "2-9_4szHtKnN",
        "9dmLW649-DGt",
        "9JNqyxG6-DG7",
        "DZEq3O_oQz23",
        "vSOdaru0GZ3A",
        "dkizQFT1vXst",
        "SV_9xgkaEUjv",
        "So-cd9ei5Q56",
        "Uc3Nf32476ps",
        "ksICZHtsK5M7",
        "KncMRgVpK-LG",
        "fxYDbUUWRbkj",
        "i4fxeZFiW7UK",
        "WrAwc2QwW7UQ",
        "d1-KebHvRSCL",
        "Sm0OBCaiW7Ux",
        "q_1I4vs-W7U0",
        "HC8RAR8_W7U4",
        "1dqwHKViW7VB",
        "-SO6iuhOW7Ui",
        "91fOSmdg2Cp0",
        "-rQCZ1HM12KC",
        "VSR5xn3ORxPn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa8LE4iUs5op",
        "colab_type": "text"
      },
      "source": [
        "# **NER_by_BI_LSTM_CRF**\n",
        "\n",
        "## **Author:** Gema De Vargas Romero\n",
        "\n",
        "## **Master Thesis:** \"Development of a Named Entity Recognition System to automatically assign tumor morphology entity mentions to health-related documents in Spanish.\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05N_SwgGTL6t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "54c9038d-47e9-4389-e5a6-f060d2398bc9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "path='drive/My Drive/Ejemplos NER - TFM/'\n",
        "!ls 'drive/My Drive/Ejemplos NER - TFM/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            " bert\n",
            " check_results2.ipynb\n",
            " check_results.ipynb\n",
            " data\n",
            " dev_set\n",
            " dev_set2\n",
            "'Dictionary based NER (spacy).ipynb'\n",
            "'Ehealth_Dictionary based NER (spacy).ipynb'\n",
            " last_step_cantemist.ipynb\n",
            " last_step_cantemist_TEST.ipynb\n",
            " mismatch_bert.xlsx\n",
            " mismatch_bilstm1.xlsx\n",
            " mismatch_bilstm2.xlsx\n",
            " mismatch_bilstm3.xlsx\n",
            " NER_by_BERT_Cantemist_BIOESV.ipynb\n",
            " NER_by_BERT_Cantemist_Competicion.ipynb\n",
            " NER_by_BERT_Cantemist.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist_BIOESV_2.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist_BIOESV.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist_Competicion.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist.ipynb\n",
            " NER_by_CRF_Cantemist_Competicion.ipynb\n",
            " NER_by_CRF_Cantemist.ipynb\n",
            " NER_by_CRF.ipynb\n",
            " Preprocessing_NER_Cantemist.ipynb\n",
            " resources\n",
            " results_bert\n",
            " results_bert2\n",
            " results_BILSTM_ap1\n",
            " results_BILSTM_ap2\n",
            " results_BILSTM_ap3\n",
            " results_CRF\n",
            " sample_set\n",
            " Scielo+Wiki_skipgram_cased.bin\n",
            " Scielo+Wiki_skipgram_cased.vec\n",
            " test-background-set-to-publish\n",
            " test_set\n",
            " test_set_predictions\n",
            " train_set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mt4hDPDvdSb",
        "colab_type": "text"
      },
      "source": [
        "## **Loading libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scn6jrRSUoKx",
        "colab_type": "text"
      },
      "source": [
        "First, we must load the libraries that we will use in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtRt1ONNabh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "332f4645-420f-4cd6-bfc5-fdfb441ddba0"
      },
      "source": [
        "!pip install sklearn-crfsuite\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite.metrics import flat_f1_score\n",
        "from sklearn_crfsuite.metrics import flat_precision_score\n",
        "from sklearn_crfsuite.metrics import flat_recall_score\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzcWUWl2aWqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "aef6dfc6-ab68-49b6-d40c-50dd72d922ec"
      },
      "source": [
        "# Library spacy\n",
        "!pip install -U spacy \n",
        "#!python -m spacy validate\n",
        "!python -m spacy download es_core_news_lg\n",
        "\n",
        "import spacy\n",
        "\n",
        "# nlp = spacy.load(\"es\") # no longer works with updated version of spacy 2.3.1\n",
        "import es_core_news_lg\n",
        "nlp = es_core_news_lg.load()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Collecting thinc==7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.3.2 thinc-7.4.1\n",
            "Collecting es_core_news_lg==2.3.1\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-2.3.1/es_core_news_lg-2.3.1.tar.gz (573.1MB)\n",
            "\u001b[K     |████████████████████████████████| 573.1MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from es_core_news_lg==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (50.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.1.0)\n",
            "Building wheels for collected packages: es-core-news-lg\n",
            "  Building wheel for es-core-news-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-lg: filename=es_core_news_lg-2.3.1-cp36-none-any.whl size=573139081 sha256=54d09cf13f8d12edeab96c9feeaa613771f4065fbe44a9acd9a155cf12c648b9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-846a9t9a/wheels/48/59/33/558e7f48e924c6cac0cbd3679ee7c84f5ae02964c335232e5a\n",
            "Successfully built es-core-news-lg\n",
            "Installing collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tagsZfDrd1lM"
      },
      "source": [
        "## **Read the files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FpxWth4pd1lN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8cfd3b55-1c67-4016-f164-0f0015cb4558"
      },
      "source": [
        "!ls 'drive/My Drive/Ejemplos NER - TFM/data'\n",
        "import pickle as pkl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_data_complete.csv\tfiles_txt_dev\t     sentences_dev_by_cc2\n",
            "df_data_dev2_2.csv\tfiles_txt_test\t     sentences_test\n",
            "df_data_dev2.csv\tfiles_txt_test_true  sentences_test_by_cc\n",
            "df_data_test.csv\tsentences_dev\t     sentences_test_true\n",
            "df_data_test_true2.csv\tsentences_dev2\t     sentences_test_true_by_cc\n",
            "df_data_train2.csv\tsentences_dev_by_cc  sentences_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj1_4U9IeC-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train dataset\n",
        "with open(path+'data/sentences_train', 'rb') as file: \n",
        "  sentences_train = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_train2 = pd.read_csv(path+'data/df_data_train2.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_LAMsMgeIY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Development dataset 1\n",
        "with open(path+'data/sentences_dev', 'rb') as file: \n",
        "  sentences_dev = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'data/sentences_dev_by_cc', 'rb') as file: \n",
        "  sentences_dev_by_cc = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_dev2 = pd.read_csv(path+'data/df_data_dev2.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ImCDQ44Bd1lP",
        "colab": {}
      },
      "source": [
        "# Development dataset 2\n",
        "with open(path+'data/sentences_dev2', 'rb') as file: \n",
        "  sentences_dev2 = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'data/sentences_dev_by_cc2', 'rb') as file: \n",
        "  sentences_dev_by_cc2 = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_dev2_2 = pd.read_csv(path+'data/df_data_dev2_2.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANP2WPx49-S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST and background FILES:\n",
        "with open(path+'data/sentences_test', 'rb') as file: \n",
        "  sentences_test = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'data/sentences_test_by_cc', 'rb') as file: \n",
        "  sentences_test_by_cc = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_test = pd.read_csv(path+'data/df_data_test.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfzizMpH3q8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "14d7d7c0-3ed5-4e90-a748-822deb264e79"
      },
      "source": [
        "len_sent_train = [len(sent) for sent in sentences_train]\n",
        "plt.hist(len_sent_train)\n",
        "plt.title(\"Distribution of sequence lengths\")\n",
        "plt.xlabel(\"Number of words in a sequence\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "print(np.mean(len_sent_train))\n",
        "print(np.max(len_sent_train))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23.306840324069327\n",
            "220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJT93E3rl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "9c6e227b-1abe-4e46-a239-ff1c7a16f5ca"
      },
      "source": [
        "len_sent_dev = [len(sent) for sent in sentences_dev]\n",
        "plt.hist(len_sent_dev)\n",
        "plt.title(\"Distribution of sequence lengths\")\n",
        "plt.xlabel(\"Number of words in a sequence\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "print(np.mean(len_sent_dev))\n",
        "print(np.max(len_sent_dev))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23.612403100775193\n",
            "142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSXM8hfw3vt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "ba02adfa-6ba4-44d1-e987-661b31191941"
      },
      "source": [
        "len_sent_dev2 = [len(sent) for sent in sentences_dev2]\n",
        "plt.hist(len_sent_dev2)\n",
        "plt.title(\"Distribution of sequence lengths\")\n",
        "plt.xlabel(\"Number of words in a sequence\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "print(np.mean(len_sent_dev2))\n",
        "print(np.max(len_sent_dev2))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21.099401059663673\n",
            "161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf10lEQVR4nO3debwcVZ338c+XBAh7gESEJHADxCUqCgbER1E2Iez4DCAKEhBFZxgUBTUsCqjMAzqKMuMCQmRTIYJKBByIbOLMIyFsgYBIJAESAgmEhE2BwG/+OKdJcdM3p29yewn3+369+nWrTlWd+vW53f3rOlV9ShGBmZnZsqzS7gDMzKzzOVmYmVmRk4WZmRU5WZiZWZGThZmZFTlZmJlZkZOFLUXSTyR9rY/q2lTSc5IG5PmbJH26L+rO9f1e0ri+qq8X+/2WpCclPd7qfXcqSadKuqRN++7T15UtbWC7A7DWkjQL2AhYDLwC3AdcBJwbEa8CRMTnelHXpyPiDz2tExGPAGuvWNSv7e9UYMuIOLRS/x59UXcv49gUOA7YLCLmtXr//V2914E1n48s+qd9ImIdYDPgDOCrwPl9vRNJb9QvI5sCTzlRWH/iZNGPRcSiiJgEfAwYJ+mdAJIukPStPD1E0lWSFkpaIOkWSatIupj0ofm73M30FUldkkLSkZIeAW6olFUTxxaSpkh6RtKVkjbI+9pR0uxqjJJmSdpV0ljgROBjeX935+WvdT/kuE6W9LCkeZIukrReXlaLY5ykR3IX0kk9tY2k9fL283N9J+f6dwUmA5vkOC6os23dNsvLNpF0Ra53pqTPV7ZbI7f905Luk/Tlanvk+LeszL/2f8rze0u6K+/3fyRt1a0dj5c0TdIiSZdJGlRZvl/e9hlJf8vtXWuH8yXNlTQnd78N6KndurXD9jmOhZLulrRjZdlNkr4p6b8lPSvpOklDKssPy+3+lKSvlV4H2Wb16pM0SNIlua6Fkm6TtFEjz8GWcLIwImIKMBvYoc7i4/KyoaTuqxPTJvFJ4BHSUcraEfHtyjYfBt4O7N7DLg8DPgVsTOoOO7uBGP8L+Dfgsry/d9dZ7fD82AnYnNT99Z/d1vkg8FZgF+Drkt7ewy7/A1gv1/PhHPMRucttD+CxHMfhdbat22Y5YfwOuBsYlmM4VlKtnU4BtsiP3YGGz8VI2hqYAHwW2BA4B5gkafXKagcBY4GRwFaktkLSdqSuyC8Dg4EPAbPyNheQ/kdbAlsDuwHFcwOShgFXA98CNgCOB66QNLSy2ieAI4A3AavldZA0GvgRcAjpNbIeqb1Kr4O69ZHacT1gRG6bzwF/Lz0Hez0nC6t5jPSm7u5l0ht2s4h4OSJuifKAYqdGxPMR0dMb8uKIuDcinge+BhzU6LfVgkOA70XEQxHxHHACcHC3o5rTIuLvEXE36UN7qaSTYzkYOCEino2IWcB3gU82GEdPbbYtMDQivhERL0XEQ8BP874gfZifHhELIuJRGkiiFUcB50TErRHxSkRcCLwIbF9Z5+yIeCwiFpCS1nty+ZHAhIiYHBGvRsSciPhL/va9J3Bs/n/OA86qxLsshwLXRMQ1uc7JwNRcX83PIuKv+XUysRLPAcDvIuJPEfES8HWgkUHseqrvZVKS2DK3ze0R8UwD9VmFk4XVDAMW1Cn/DjADuE7SQ5LGN1DXo71Y/jCwKjCkh3V7Y5NcX7XugaRv9zXVq5deoP7J9yE5pu51DWswjp7abDNS99XC2oN01FGLbxOWbptGbQYc163uEbnOmp6e+wjgbz3UuSowt1LnOaRv7o3Ec2C3eD5ISqKleF7XDhHxAvBUA/vsqb6LgWuBSyU9JunbklZtoD6reKOegLRekLQt6YPwT92XRcSzpG6V45TOadwg6baIuJ6ev+2VvgWOqExvSvrm9yTwPLBmJa4BpK6cRut9jPQhVa17MfAEMLywbdWTOabNSFeL1eqa08jGPbUZ6QNwZkSM6mHTuaS2mV7ZZ9ULVNoHeDOpu4tc9+kRcXojMXbzKKnrq175i8CQiFi8HHVeHBGfWY545pK6CoF0Lod0ZFDTq6GyI+Jl4DTgNEldwDXAAzThoo43Mh9Z9GOS1pW0N3ApcElE3FNnnb0lbSlJwCLS5bav5sVPkPr0e+tQSaMlrQl8A7g8Il4B/goMkrRX/uZ3MlDtc38C6KqdLK7jl8AXJY2UtDZL+rZ79UGXY5kInC5pHUmbAV8CGvoNwTLabArwrKSv5pPZAyS9Mydr8j5PkLS+pOHAMd2qvgv4RN5uLOlcSs1Pgc9Jep+StXI7rtNAyOcDR0jaRekk/jBJb4uIucB1wHfza2UVSVtI+nChPkhttY+k3XO8g5QuYGgkaV+et/0/klYDTgVUWV56HbyOpJ0kvSt/+XiG9EXg1cJm1o2TRf/0O0nPkr79nQR8j3RisJ5RwB+A54D/D/woIm7My/4fcHLuZji+h+3ruZh04vRxYBDweUhXZwH/ApxH+hb/PEu+OQP8Kv99StIddeqdkOv+IzAT+AdLf+A26pi8/4dIR1y/yPU3om6b5SS0N6kvfSbpCOY80slXSN9+H87LrsvPpeoLwD7AQtL5md/WFkTEVOAzpBP6T5O6wQ5vJNh8gcMRpPMRi4CbWXKEdhjpZPF9ud7LeX1XUk91PgrsR+pmm096rX2ZBj5zImI6qf0vJR1lPAfMIx3lQPl10N2bc9zPAPeTnl/3trUC+eZHZp1J6VLTSyKiN11obzj5KHEhMCoiZrY7nv7KRxZm1nEk7SNpTUlrAf8O3MOSy3mtDZwszKwT7Ue6YOExUrfewQ1csm1N5G4oMzMr8pGFmZkVvSF/ZzFkyJDo6upqdxhmZiuV22+//cmIGFpv2RsyWXR1dTF16tR2h2FmtlKR1OOoAe6GMjOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIicLMzMrcrIwM7OiN+QvuFdU1/ir27LfWWfs1Zb9mpmV+MjCzMyKnCzMzKzIycLMzIqcLMzMrMjJwszMipwszMysyMnCzMyKnCzMzKzIycLMzIqcLMzMrMjJwszMipwszMysqOnJQtIASXdKuirPj5R0q6QZki6TtFouXz3Pz8jLuyp1nJDLH5C0e7NjNjOz12vFkcUXgPsr82cCZ0XElsDTwJG5/Ejg6Vx+Vl4PSaOBg4F3AGOBH0ka0IK4zcwsa2qykDQc2As4L88L2Bm4PK9yIbB/nt4vz5OX75LX3w+4NCJejIiZwAxgu2bGbWZmr9fsI4vvA18BXs3zGwILI2Jxnp8NDMvTw4BHAfLyRXn918rrbPMaSUdJmipp6vz58/v6eZiZ9WtNSxaS9gbmRcTtzdpHVUScGxFjImLM0KFDW7FLM7N+o5l3yvsAsK+kPYFBwLrAD4DBkgbmo4fhwJy8/hxgBDBb0kBgPeCpSnlNdRszM2uBph1ZRMQJETE8IrpIJ6hviIhDgBuBA/Jq44Ar8/SkPE9efkNERC4/OF8tNRIYBUxpVtxmZra0dtyD+6vApZK+BdwJnJ/LzwculjQDWEBKMETEdEkTgfuAxcDREfFK68M2M+u/WpIsIuIm4KY8/RB1rmaKiH8AB/aw/enA6c2L0MzMlsW/4DYzsyInCzMzK3KyMDOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzoqYlC0mDJE2RdLek6ZJOy+UjJd0qaYakyyStlstXz/Mz8vKuSl0n5PIHJO3erJjNzKy+Zh5ZvAjsHBHvBt4DjJW0PXAmcFZEbAk8DRyZ1z8SeDqXn5XXQ9Jo4GDgHcBY4EeSBjQxbjMz66ZpySKS5/LsqvkRwM7A5bn8QmD/PL1fnicv30WScvmlEfFiRMwEZgDbNStuMzNbWlPPWUgaIOkuYB4wGfgbsDAiFudVZgPD8vQw4FGAvHwRsGG1vM421X0dJWmqpKnz589vxtMxM+u3mposIuKViHgPMJx0NPC2Ju7r3IgYExFjhg4d2qzdmJn1Sy25GioiFgI3Au8HBksamBcNB+bk6TnACIC8fD3gqWp5nW3MzKwFmnk11FBJg/P0GsBHgPtJSeOAvNo44Mo8PSnPk5ffEBGRyw/OV0uNBEYBU5oVt5mZLW1geZXltjFwYb5yaRVgYkRcJek+4FJJ3wLuBM7P658PXCxpBrCAdAUUETFd0kTgPmAxcHREvNLEuM3MrJumJYuImAZsXaf8IepczRQR/wAO7KGu04HT+zpGMzNrjH/BbWZmRU4WZmZW5GRhZmZFThZmZlbkZGFmZkUNJQtJ72p2IGZm1rkaPbL4UR5u/F8krdfUiMzMrOM0lCwiYgfgENKwG7dL+oWkjzQ1MjMz6xgN/ygvIh6UdDIwFTgb2DoPIX5iRPy6WQH2J13jr27LfmedsVdb9mtmK49Gz1lsJeks0thOOwP7RMTb8/RZTYzPzMw6QKNHFv8BnEc6ivh7rTAiHstHG2Zm9gbWaLLYC/h7bQA/SasAgyLihYi4uGnRmZlZR2j0aqg/AGtU5tfMZWZm1g80miwGVe6nTZ5eszkhmZlZp2k0WTwvaZvajKT3An9fxvpmZvYG0ug5i2OBX0l6DBDwZuBjTYvKzMw6SkPJIiJuk/Q24K256IGIeLl5YZmZWSfpzZ3ytgW68jbbSCIiLmpKVGZm1lEaShaSLga2AO4Cave/DsDJwsysH2j0yGIMMDoiopnBmJlZZ2r0aqh7SSe1zcysH2r0yGIIcJ+kKcCLtcKI2LcpUZmZWUdpNFmc2swgzMysszV66ezNkjYDRkXEHyStCQxobmhmZtYpGh2i/DPA5cA5uWgY8NtmBWVmZp2l0RPcRwMfAJ6BdCMk4E3NCsrMzDpLo8nixYh4qTYjaSDpdxZmZtYPNJosbpZ0IrBGvvf2r4DfNS8sMzPrJI0mi/HAfOAe4LPANYDvkGdm1k80ejXUq8BP88PMzPqZRseGmkmdcxQRsXmfR2RmZh2nN2ND1QwCDgQ26PtwzMysEzV0ziIinqo85kTE94G9mhybmZl1iEa7obapzK5COtLozb0wzMxsJdboB/53K9OLgVnAQX0ejZmZdaRGr4baqdmBmJlZ52q0G+pLy1oeEd/rm3DMzKwT9eZqqG2BSXl+H2AK8GAzgjIzs87S6C+4hwPbRMRxEXEc8F5g04g4LSJOq7eBpBGSbpR0n6Tpkr6QyzeQNFnSg/nv+rlcks6WNEPStOpJdUnj8voPShq3Yk/ZzMx6q9FksRHwUmX+pVy2LIuB4yJiNLA9cLSk0aShQ66PiFHA9XkeYA9gVH4cBfwYUnIBTgHeB2wHnFJLMGZm1hqNdkNdBEyR9Js8vz9w4bI2iIi5wNw8/ayk+0n3wdgP2DGvdiFwE/DVXH5RRATwZ0mDJW2c150cEQsAJE0GxgK/bDB2MzNbQY1eDXW6pN8DO+SiIyLizkZ3IqkL2Bq4FdgoJxKAx1lyhDIMeLSy2exc1lO5mZm1SKPdUABrAs9ExA+A2ZJGNrKRpLWBK4BjI+KZ6rJ8FNEn98WQdJSkqZKmzp8/vy+qNDOzrNHbqp5C6io6IRetClzSwHarkhLFzyPi17n4idy9RP47L5fPAUZUNh+ey3oqf52IODcixkTEmKFDhzbytMzMrEGNHll8FNgXeB4gIh4D1lnWBpIEnA/c3+13GJOA2hVN44ArK+WH5auitgcW5e6qa4HdJK2fT2zvlsvMzKxFGj3B/VJEhKQAkLRWA9t8APgkcI+ku3LZicAZwERJRwIPs2TYkGuAPYEZwAvAEQARsUDSN4Hb8nrfqJ3sNjOz1mg0WUyUdA4wWNJngE9RuBFSRPwJUA+Ld6mzfgBH91DXBGBCg7GamVkfKyaL3J10GfA24BngrcDXI2Jyk2MzM7MOUUwWufvpmoh4F+AEYWbWDzV6gvsOSds2NRIzM+tYjZ6zeB9wqKRZpCuiRDro2KpZgZmZWedYZrKQtGlEPALs3qJ4zMysA5WOLH5LGm32YUlXRMQ/tSIoMzPrLKVzFtVLXzdvZiBmZta5Sskiepg2M7N+pNQN9W5Jz5COMNbI07DkBPe6TY3OzMw6wjKTRUQMaFUgZmbWuXozRLmZmfVTThZmZlbkZGFmZkVOFmZmVuRkYWZmRU4WZmZW5GRhZmZFThZmZlbkZGFmZkVOFmZmVuRkYWZmRU4WZmZW5GRhZmZFThZmZlbkZGFmZkVOFmZmVuRkYWZmRU4WZmZW5GRhZmZFThZmZlbkZGFmZkVOFmZmVuRkYWZmRU4WZmZW5GRhZmZFThZmZlY0sN0BWPt1jb+6bfuedcZebdu3mTXORxZmZlbkZGFmZkVNSxaSJkiaJ+neStkGkiZLejD/XT+XS9LZkmZImiZpm8o24/L6D0oa16x4zcysZ808srgAGNutbDxwfUSMAq7P8wB7AKPy4yjgx5CSC3AK8D5gO+CUWoIxM7PWaVqyiIg/Agu6Fe8HXJinLwT2r5RfFMmfgcGSNgZ2ByZHxIKIeBqYzNIJyMzMmqzV5yw2ioi5efpxYKM8PQx4tLLe7FzWU/lSJB0laaqkqfPnz+/bqM3M+rm2neCOiACiD+s7NyLGRMSYoUOH9lW1ZmZG65PFE7l7ifx3Xi6fA4yorDc8l/VUbmZmLdTqZDEJqF3RNA64slJ+WL4qantgUe6uuhbYTdL6+cT2brnMzMxaqGm/4Jb0S2BHYIik2aSrms4AJko6EngYOCivfg2wJzADeAE4AiAiFkj6JnBbXu8bEdH9pLmZmTVZ05JFRHy8h0W71Fk3gKN7qGcCMKEPQzMzs17yL7jNzKzIycLMzIqcLMzMrMjJwszMipwszMysyMnCzMyKnCzMzKzIycLMzIqcLMzMrMjJwszMipwszMysyMnCzMyKnCzMzKzIycLMzIqcLMzMrMjJwszMipwszMysyMnCzMyKnCzMzKzIycLMzIqcLMzMrMjJwszMipwszMysyMnCzMyKnCzMzKxoYLsDsP6ta/zVbdnvrDP2ast+zVZWPrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIicLMzMrcrIwM7MiJwszMytysjAzsyInCzMzK3KyMDOzIo8NZf1Su8akAo9LZSsnH1mYmVnRSpMsJI2V9ICkGZLGtzseM7P+ZKXohpI0APgh8BFgNnCbpEkRcV97IzPrPQ/LbiujlSJZANsBMyLiIQBJlwL7AU4WZg3yeRpbEStLshgGPFqZnw28r7qCpKOAo/Lsc5Ie6EX9Q4AnVyjC5nBcvdepsfXruHRmrzfp1PaCzo2tL+LarKcFK0uyKIqIc4Fzl2dbSVMjYkwfh7TCHFfvdWpsjqt3OjUu6NzYmh3XynKCew4wojI/PJeZmVkLrCzJ4jZglKSRklYDDgYmtTkmM7N+Y6XohoqIxZL+FbgWGABMiIjpfbiL5eq+agHH1XudGpvj6p1OjQs6N7amxqWIaGb9Zmb2BrCydEOZmVkbOVmYmVlRv04WnTKEiKQRkm6UdJ+k6ZK+kMs3kDRZ0oP57/ptim+ApDslXZXnR0q6NbfbZfmig3bENVjS5ZL+Iul+Se/vhDaT9MX8f7xX0i8lDWpXm0maIGmepHsrZXXbSMnZOcZpkrZpcVzfyf/LaZJ+I2lwZdkJOa4HJO3eyrgqy46TFJKG5PmWtdeyYpN0TG636ZK+XSnv2zaLiH75IJ0o/xuwObAacDcwuk2xbAxsk6fXAf4KjAa+DYzP5eOBM9sU35eAXwBX5fmJwMF5+ifAP7cprguBT+fp1YDB7W4z0g9IZwJrVNrq8Ha1GfAhYBvg3kpZ3TYC9gR+DwjYHri1xXHtBgzM02dW4hqd35+rAyPz+3ZAq+LK5SNIF9g8DAxpdXsto812Av4ArJ7n39SsNmv6i7VTH8D7gWsr8ycAJ7Q7rhzLlaRxsB4ANs5lGwMPtCGW4cD1wM7AVfmN8WTlTf26dmxhXOvlD2V1K29rm7FktIENSFcbXgXs3s42A7q6fcDUbSPgHODj9dZrRVzdln0U+Hmeft17M39ov7+VcQGXA+8GZlWSRUvbq4f/5URg1zrr9Xmb9eduqHpDiAxrUyyvkdQFbA3cCmwUEXPzoseBjdoQ0veBrwCv5vkNgYURsTjPt6vdRgLzgZ/lLrLzJK1Fm9ssIuYA/w48AswFFgG30xltVtNTG3XSe+JTpG/t0Oa4JO0HzImIu7st6oT2eguwQ+7ivFnSts2KrT8ni44jaW3gCuDYiHimuizS14OWXucsaW9gXkTc3sr9Nmgg6ZD8xxGxNfA8qUvlNW1qs/VJg1yOBDYB1gLGtjKG3mhHG5VIOglYDPy8A2JZEzgR+Hq7Y+nBQNJR7PbAl4GJktSMHfXnZNFRQ4hIWpWUKH4eEb/OxU9I2jgv3xiY1+KwPgDsK2kWcCmpK+oHwGBJtR90tqvdZgOzI+LWPH85KXm0u812BWZGxPyIeBn4NakdO6HNanpqo7a/JyQdDuwNHJITWbvj2oKU+O/O74PhwB2S3tzmuGpmA7+OZAqpB2BIM2Lrz8miY4YQyd8Ezgfuj4jvVRZNAsbl6XGkcxktExEnRMTwiOgitc8NEXEIcCNwQLviyrE9Djwq6a25aBfSkPVtbTNS99P2ktbM/9daXG1vs4qe2mgScFi+ymd7YFGlu6rpJI0ldXnuGxEvdIv3YEmrSxoJjAKmtCKmiLgnIt4UEV35fTCbdDHK47S5vbLfkk5yI+ktpAs9nqQZbdbMkzGd/iBdzfBX0pUCJ7Uxjg+SugKmAXflx56k8wPXAw+SrnjYoI0x7siSq6E2zy+8GcCvyFditCGm9wBTc7v9Fli/E9oMOA34C3AvcDHpipS2tBnwS9K5k5dJH3RH9tRGpIsXfpjfD/cAY1oc1wxSP3vtPfCTyvon5bgeAPZoZVzdls9iyQnulrXXMtpsNeCS/Fq7A9i5WW3m4T7MzKyoP3dDmZlZg5wszMysyMnCzMyKnCzMzKzIycLMzIqcLGy55NE3v1uZP17SqX1U9wWSDiivucL7OVBptNobm72vvL9TJR3f4LpjJJ3d7JjMGuVkYcvrReD/1oZr7hSVX0k34kjgMxGxUxPikKTlfn9FxNSI+HxfxmS2IpwsbHktJt3z94vdF3Q/MpD0XP67Yx7s7EpJD0k6Q9IhkqZIukfSFpVqdpU0VdJf8xhVtftqfEfSbfn+AZ+t1HuLpEmkX0t3j+fjuf57JZ2Zy75O+jHk+ZK+0239H0raN0//RtKEPP0pSafn6S/l+u6VdGwu68r3DriI9COpEZJOys/hT8BbK/v4vNL9S6ZJurROzDtqyf1DTlW6l8FNud3qJhFJP85tNl3SaT2ss9R+Ja2V65+iNCjjfrl8DUmX5qOv3ygNVjem+j/N0wdIuiBPD5V0Rf4f3SbpA6XnIOmwHM/dki5eVj3WRq34Fakfb7wH8BywLukXresBxwOn5mUXAAdU181/dwQWkobFXp00Vs1pedkXgO9Xtv8v0peZUaRfqw4CjgJOzuusTvr19shc7/PAyDpxbkIagmMoadC1G4D987KbqPOrW9LQJt/J01OAP+fpn5GGG38v6Re7awFrA9NJIwV3kcbm2T6vX1tvzdxWM4Dj87LHWHIPgsF1YtiRJb+YPxX4n/ychwBPAavW2ab2S+wB+bltVWedpfYL/BtwaK2MNKrBWqT7mEzI5VuRviCMqf5P8/QBwAV5+hfAB/P0pqQhbHp8DsA78v6GdHsOdevxo30PH1nYcos0Mu5FQG+6S26LiLkR8SJpKILrcvk9pA/bmokR8WpEPAg8BLyNdHOcwyTdRRrCfUNSMgGYEhEz6+xvW+CmSAP71UYy/VAhxltIwz6PJh2p1Abeez/pA++DwG8i4vmIeI40WOAOeduHI+LPeXqHvN4Lua2qY49NA34u6VDSh3DJ1RHxYkQ8SRr4r97Q6wdJugO4k/QhPLrOOvX2uxswPrfrTaTEvCmpnS4BiIhpeduSXYH/zHVNAtZVGk25p+ewM/CrXEZELGigHmuD3vTvmtXzfdKYND+rlC0md3HmfvvqLURfrEy/Wpl/lde/HruPQxOksXiOiYhrqwsk7Ug6sugTETFH6ZaeY4E/koaAPoj0bfpZLXsE6Ebj2Iv0YbwPcJKkd8WS+13UU223V+j23lUaLO54YNuIeDp3Cw1qZL+kdv2niHigW53Lir/6/6nuZxXSkdU/6tS1zOfQTd16rH18ZGErJH8TnEg6WVwzi9QFA7Avqbuhtw6UtEo+j7E5aTC0a4F/VhrOHUlvUbrh0bJMAT4saYikAcDHgZsb2P+fgWNJyeIW0gfxLXnZLcD+SiPLrkW6q9stder4Y15vDUnrkD6gawl0RETcCHyV1I23ot+a1yUlqkWSNgL26L7CMvZ7LXCM8ie6pK0r8X8il72T1BVV84Skt+c6P1opvw44prLP9xTivoH0v94wr7/BctZjTeYjC+sL3wX+tTL/U+BKSXeTzj0sz7f+R0gf9OsCn4uIf0g6j9RVdUf+YJsP7L+sSiJirqTxpCHCReoKaWR48FuA3SJihqSHSUcXt+Q678jf3GtDPp8XEXcq3eWwuu87JF1GuhfyPNKw+JDOKVwiab0c09kRsbCBmHoUEXdLupM02u2jwH/XWa3ufiV9k3SEOC1/+M8k3VPix6Q7Ed4P3E+641/NeNItY+eTzh3Vkt3ngR9Kmkb6fPkj8LllxD09XzRws6RXSF1oh/e2Hms+jzprZg2RdBPpBP3UdsdireduKDMzK/KRhZmZFfnIwszMipwszMysyMnCzMyKnCzMzKzIycLMzIr+F9zPCGX5QvReAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUJWVCSt1vaK",
        "colab_type": "text"
      },
      "source": [
        "## **COMBINE TRAIN AND DEVELOPMENT FILES TO FEED FINAL MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZoqrhBS6Km8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_complete = pd.read_csv(path+'data/df_data_complete.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JumvH_Sp6NqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "94dcf039-235e-4169-df06-2ef14133d383"
      },
      "source": [
        "df_data_complete"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Index</th>\n",
              "      <th>Sentence_Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>start</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ANAMNESIS</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n</td>\n",
              "      <td>SPACE</td>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mujer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>16</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>NUM</td>\n",
              "      <td>19</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862674</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>exitus</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5646</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862675</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5653</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862676</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>la</td>\n",
              "      <td>DET</td>\n",
              "      <td>5656</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862677</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>paciente</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5659</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862678</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>5667</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>862679 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        File_Index  Sentence_Index       Word    POS  start Tag\n",
              "0                1               1  ANAMNESIS   NOUN      0   O\n",
              "1                1               1         \\n  SPACE      9   O\n",
              "2                1               1      Mujer   NOUN     10   O\n",
              "3                1               1         de    ADP     16   O\n",
              "4                1               1         67    NUM     19   O\n",
              "...            ...             ...        ...    ...    ...  ..\n",
              "862674        1001           37730     exitus   NOUN   5646   O\n",
              "862675        1001           37730         de    ADP   5653   O\n",
              "862676        1001           37730         la    DET   5656   O\n",
              "862677        1001           37730   paciente   NOUN   5659   O\n",
              "862678        1001           37730          .  PUNCT   5667   O\n",
              "\n",
              "[862679 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLQoGh8N1656",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "89d666d7-c028-4774-da55-6dccf05743c6"
      },
      "source": [
        "print(\"Number of sentences in training dataset 1: %d\" %len(sentences_train))\n",
        "print(\"Number of sentences in development dataset 1: %d\" %len(sentences_dev))\n",
        "print(\"Number of sentences in development dataset 2: %d\" %len(sentences_dev2))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in training dataset 1: 19502\n",
            "Number of sentences in development dataset 1: 9546\n",
            "Number of sentences in development dataset 2: 8682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je3QcnOM12BE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "feb50091-7bea-4f27-f95a-d31abc5bc76f"
      },
      "source": [
        "print(sentences_dev[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Anamnesis', 'NOUN', 'O', 0), ('\\n', 'SPACE', 'O', 9), ('Varón', 'PROPN', 'O', 10), ('de', 'ADP', 'O', 16), ('74', 'NUM', 'O', 19), ('años', 'NOUN', 'O', 22), (',', 'PUNCT', 'O', 26), ('exfumador', 'NOUN', 'O', 28), ('desde', 'ADP', 'O', 38), ('hace', 'AUX', 'O', 44), ('15', 'NUM', 'O', 49), ('años', 'NOUN', 'O', 52), (',', 'PUNCT', 'O', 56), ('con', 'ADP', 'O', 58), ('único', 'ADJ', 'O', 62), ('antecedente', 'NOUN', 'O', 68), ('de', 'ADP', 'O', 80), ('hipertensión', 'NOUN', 'O', 83), (',', 'PUNCT', 'O', 95), ('dislipemia', 'NOUN', 'O', 97), ('y', 'CCONJ', 'O', 108), ('apendicectomizado', 'ADJ', 'O', 110), (';', 'PUNCT', 'O', 127), ('se', 'PRON', 'O', 129), ('diagnostica', 'VERB', 'O', 132), ('en', 'ADP', 'O', 144), ('marzo', 'INTJ', 'O', 147), ('de', 'ADP', 'O', 153), ('2013', 'NUM', 'O', 156), ('de', 'ADP', 'O', 161), ('carcinoma', 'INTJ', 'B-MOR', 164), ('de', 'ADP', 'I-MOR', 174), ('células', 'NOUN', 'I-MOR', 177), ('transicionales', 'ADJ', 'I-MOR', 185), ('de', 'ADP', 'I-MOR', 200), ('vejiga', 'PROPN', 'I-MOR', 203), ('E-IV', 'PROPN', 'I-MOR', 210), ('(', 'PUNCT', 'E-MOR', 215), ('pulmonares', 'ADJ', 'O', 216), ('y', 'CCONJ', 'O', 227), ('óseas', 'ADJ', 'O', 229), (')', 'PUNCT', 'O', 234), ('.', 'PUNCT', 'O', 235)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBArwgQn2QPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_complete = sentences_train.copy()\n",
        "sentences_complete.extend(sentences_dev)\n",
        "sentences_complete.extend(sentences_dev2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa0qveNj46dU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4687336e-891e-4c42-a879-b3949774a0e6"
      },
      "source": [
        "print(\"Number of sentences in training dataset 1: %d\" %len(sentences_train))\n",
        "print(\"Number of sentences in development dataset 1: %d\" %len(sentences_dev))\n",
        "print(\"Number of sentences in development dataset 2: %d\" %len(sentences_dev2))\n",
        "print(\"Number of sentences in combined dataset: %d\" %len(sentences_complete))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in training dataset 1: 19502\n",
            "Number of sentences in development dataset 1: 9546\n",
            "Number of sentences in development dataset 2: 8682\n",
            "Number of sentences in combined dataset: 37730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0stvUK8je47",
        "colab_type": "text"
      },
      "source": [
        "## **Model 2: BI-LSTM-CRF**\n",
        "\n",
        "BI-LSTM-CRF combines the bidirectional LSTM model with the CRF model. This is a state-of-the approach to named entity recognition. The LSTM (Long Short Term Memory) is a special type of Recurrent Neural Network to process the sequence of data.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "https://www.aitimejournal.com/@akshay.chavan/complete-tutorial-on-named-entity-recognition-ner-using-python-and-keras\n",
        "\n",
        "https://github.com/Akshayc1/named-entity-recognition/blob/master/NER%20using%20Bidirectional%20LSTM%20-%20CRF%20.ipynb\n",
        "\n",
        "-----\n",
        "\n",
        "**Keras:** https://keras.io/about/\n",
        "\n",
        "Keras is a deep learning API written in python that runs on top of Tensorflow, which is an end-to-end, open-source machine learning platform. A key advantage of Keras is that it provides \n",
        "\n",
        "Keras is structured into layers and models.\n",
        "\n",
        "**keras - contrib**\n",
        "\n",
        "keras-contrib contains additional layers, activations, loss functions, optimizers, etc. that are not yet available within Keras itself. These additional modules can be used core Keras models and modules.\n",
        "\n",
        "An linear chain CRF is defined to maximize the following likelihood function:\n",
        "\n",
        "$$ L(W, U, b; y_1, ..., y_n) := \\frac{1}{Z}\n",
        "    \\sum_{y_1, ..., y_n} \\exp(-a_1' y_1 - a_n' y_n - \\sum_{k=1^n}((f(x_k' W + b) y_k) + y_1' U y_2)), $$\n",
        "    where:\n",
        "        $Z$: normalization constant\n",
        "        $x_k, y_k$:  inputs and outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQnSrR1kjjvR",
        "colab_type": "text"
      },
      "source": [
        "### **Loading libraries**\n",
        "\n",
        "sequeval allows to compute f1 score for named-entity recognition in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItFdOcMujmH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "b2e5c59d-24d4-4fb4-ac08-047182f24073"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n",
        "from keras.models import Model, Input\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib import losses\n",
        "from keras_contrib import metrics\n",
        "\n",
        "!pip install seqeval\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "!pip install gensim\n",
        "import gensim"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-lsfgqd4o\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-lsfgqd4o\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=715a6152821860266ad70f9e77f93bf91c11f045e7cc15a47c9cffbc945c4207\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-te8ectli/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.2.4->seqeval) (1.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=0efefe77905ed781dc3c3122813119a8fdcc312ece3725b1f7e6fc96cab76fd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.59)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.59)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDt0vr9V6BX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJeI3ZYjtBH",
        "colab_type": "text"
      },
      "source": [
        "### **Model parameters:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQBpHOLi5i56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a42e5c28-b483-4d2a-f5e0-2249798329db"
      },
      "source": [
        "len_sent_train = [len(sent) for sent in sentences_train]\n",
        "len_sent_dev = [len(sent) for sent in sentences_dev]\n",
        "len_sent_dev2 = [len(sent) for sent in sentences_dev2]\n",
        "len_sent_test = [len(sent) for sent in sentences_test]\n",
        "\n",
        "\n",
        "print(\"Maximum sentence length in train set: %d\" %max(len_sent_train))\n",
        "print(\"Average sentence length in train set: %d\" %np.mean(len_sent_train))\n",
        "print()\n",
        "print(\"Maximum sentence length in development set: %d\" %max(len_sent_dev))\n",
        "print(\"Average sentence length in development set: %d\" %np.mean(len_sent_dev))\n",
        "print()\n",
        "print(\"Maximum sentence length in development set2: %d\" %max(len_sent_dev2))\n",
        "print(\"Average sentence length in development set2: %d\" %np.mean(len_sent_dev2))\n",
        "print()\n",
        "print(\"Maximum sentence length in test set: %d\" %max(len_sent_test))\n",
        "print(\"Average sentence length in test set: %d\" %np.mean(len_sent_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum sentence length in train set: 220\n",
            "Average sentence length in train set: 23\n",
            "\n",
            "Maximum sentence length in development set: 142\n",
            "Average sentence length in development set: 23\n",
            "\n",
            "Maximum sentence length in development set2: 161\n",
            "Average sentence length in development set2: 21\n",
            "\n",
            "Maximum sentence length in test set: 267\n",
            "Average sentence length in test set: 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9b0keVjuoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 8\n",
        "max_len = 75\n",
        "embedding = 40"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsVbCFQrVXDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess = False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlNWJ-WHlOvQ",
        "colab_type": "text"
      },
      "source": [
        "### **Model architecture**\n",
        "https://keras.io/api/layers/core_layers/embedding/\n",
        "\n",
        "**1. Embedding layer**\n",
        "\n",
        "**2. Bidirectional layer**\n",
        "\n",
        "**3. TimeDistributed layer**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP-jd5EBj1h7",
        "colab_type": "text"
      },
      "source": [
        "### **Approach 1: BI-LSTM-CRF with random initialization of vectors**\n",
        "\n",
        "Text files needs to be read into string tensors, then split into words. Finally, the words need to be indexed & turned into integer tensors.\n",
        "\n",
        "Keras accepts numpy arrays.\n",
        "\n",
        "- **word_to_index**: is a dictionary to convert a word into an index value.\n",
        "- **tag_to_index** is a dictionary to convert a label into an index value. \n",
        "\n",
        "As a result, each word is represented as integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSUH0wMWG0zR",
        "colab_type": "text"
      },
      "source": [
        "#### **Preprocessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8NXV4oRq29M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_sentences(sentences): \n",
        "  sentences2 = []\n",
        "  for sent in sentences:\n",
        "    if len(sent) > 2.5*max_len:\n",
        "      sentences2.append(sent[:max_len])\n",
        "      sentences2.append(sent[max_len:2*max_len])\n",
        "      sentences2.append(sent[2*max_len:])\n",
        "\n",
        "    elif len(sent) > 1.5*max_len:\n",
        "      sentences2.append(sent[:max_len])\n",
        "      sentences2.append(sent[max_len:])\n",
        "\n",
        "    else:\n",
        "      sentences2.append(sent)\n",
        "  return sentences2"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ky_BV0Qq2xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentences_train = split_sentences(sentences_train)\n",
        "#sentences_dev = split_sentences(sentences_dev)\n",
        "\n",
        "#sentences_dev_by_cc = []\n",
        "#for cc in sentences_dev_by_cc:\n",
        "#  sentences_dev_by_cc.append(split_sentences(cc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syklm7yo5bCm",
        "colab_type": "text"
      },
      "source": [
        "#### **Train and evaluate the BiLSTM-CRF approach 1 model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MrHj696mhm",
        "colab_type": "text"
      },
      "source": [
        "##### **Feature preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TIJ3Hd5558Y",
        "colab_type": "text"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQBIyx055Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting unique words and labels from data \n",
        "words = list(df_data_train2['Word'].unique())\n",
        "if (preprocess==True):\n",
        "  words = [preprocess_word(w) for w in words]\n",
        "  words = np.unique(words)\n",
        "          \n",
        "tags = list(df_data_train2['Tag'].unique())\n",
        "\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "word_to_index[\"UNK\"] = 1\n",
        "word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "# label is key and value is index.\n",
        "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
        "tag_to_index[\"PAD\"] = 0\n",
        "\n",
        "idx2word = {i: w for w, i in word_to_index.items()}\n",
        "idx2tag = {i: w for w, i in tag_to_index.items()}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hiJ3yNm6D3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting each sentence into list of index from list of tokens\n",
        "X_train = [[word_to_index.get(w[0], 1) for w in s] for s in sentences_train]\n",
        "\n",
        "# Padding each sequence to have same length  of each word\n",
        "X_train = pad_sequences(maxlen = max_len, sequences = X_train, padding = \"post\", value = word_to_index[\"PAD\"])\n",
        "\n",
        "#Convert label to index\n",
        "y_train = [[tag_to_index[w[2]] for w in s] for s in sentences_train]\n",
        "\n",
        "# padding\n",
        "y_train = pad_sequences(maxlen = max_len, sequences = y_train, padding = \"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "num_tag = df_data_train2['Tag'].nunique()\n",
        "\n",
        "# One hot encoded labels\n",
        "y_train = [to_categorical(i, num_classes = num_tag + 1) for i in y_train]\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rvFnuU-6IXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f42410f3-9397-4ad2-af57-3f9ee718b8a5"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10,  5, 11, 12, 13,  8, 14, 15, 16,\n",
              "       17,  5, 18, 19, 20, 21, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRCXS2oW6LLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c622a2e-e6fd-4422-9495-e04c7b425fe8"
      },
      "source": [
        "print(tag_to_index)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'O': 1, 'B-MOR': 2, 'E-MOR': 3, 'S-MOR': 4, 'I-MOR': 5, 'V-MOR': 6, 'PAD': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSVpG4uv5-Qt",
        "colab_type": "text"
      },
      "source": [
        "**Development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkEplJpa6Azv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEVELOPMENT SET (without considering clinical cases independently)\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_dev = [[word_to_index.get(w[0], 1) for w in s] for s in sentences_dev] \n",
        "# if the word is not in the vocabulary, is set to 1, which is the label \"UNK\" (unknown)\n",
        "\n",
        "# Padding each sequence to have same length of each word\n",
        "X_dev = pad_sequences(maxlen = max_len, sequences = X_dev, padding = \"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "# Convert label to index\n",
        "y_dev = [[tag_to_index[w[2]] for w in s] for s in sentences_dev]\n",
        "\n",
        "# padding\n",
        "y_dev = pad_sequences(maxlen = max_len, sequences = y_dev, padding = \"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "# One hot encoded labels\n",
        "y_dev = [to_categorical(i, num_classes = num_tag + 1) for i in y_dev]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# DEVELOPMENT SET (done by clinical case)\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_dev_cc = []\n",
        "y_dev_cc = []\n",
        "start_char_dev_cc = []\n",
        "token_dev_cc = []\n",
        "\n",
        "for cc in sentences_dev_by_cc:\n",
        "  x_i = [[word_to_index.get(w[0], 1) for w in s] for s in cc]\n",
        "  y_i = [[tag_to_index[w[2]] for w in s] for s in cc] #Convert label to index\n",
        "\n",
        "  start_i = [[w[3] for w in s] for s in cc]\n",
        "  token_i = [[w[0] for w in s] for s in cc]\n",
        "\n",
        "  # Padding each sequence to have same length  of each word\n",
        "  X_dev_cc.append(pad_sequences(maxlen = max_len, sequences = x_i, padding = \"post\", value = word_to_index[\"PAD\"]))\n",
        "  y_dev_cc.append(pad_sequences(maxlen = max_len, sequences = y_i, padding = \"post\", value = tag_to_index[\"PAD\"]))\n",
        "  \n",
        "  start_char_dev_cc.append(pad_sequences(maxlen=max_len, sequences = start_i, value=-1, padding=\"post\", truncating=\"post\"))\n",
        "  token_dev_cc.append(pad_sequences(maxlen=max_len, sequences = token_i, value=\"PAD\", padding=\"post\", truncating=\"post\",dtype= object))\n",
        "\n",
        "## One hot encoded labels\n",
        "y_dev_cc = [to_categorical(i, num_classes = num_tag + 1) for i in y_dev_cc]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRf4BOW56rK7",
        "colab_type": "text"
      },
      "source": [
        "##### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0tDwJMt6knO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d31150d9-b4c2-40c4-b98a-3887c270ab48"
      },
      "source": [
        "num_tags = df_data_train2['Tag'].nunique()\n",
        "\n",
        "# Model architecture\n",
        "\n",
        "#inputs = keras.Input(shape=(None, None, 3)) # if it was an RGB image --> None states that the dimension can vary\n",
        "input = Input(shape = (max_len,))\n",
        "# max_len defines the number of words that with be considered as input to the model (can be considered as a window)\n",
        "# Hyperparameter tuning?\n",
        "# 2D tensor with shape: (batch_size, input_length). --> <tf.Tensor 'input_5:0' shape=(None, 75) dtype=float32>\n",
        "\n",
        "# 1. Embedding layer\n",
        "model = Embedding(input_dim = len(words) + 2, output_dim = embedding, input_length = max_len, mask_zero = False)(input)\n",
        "# only works if mask_zero = False\n",
        "# words is our dictionary of unique words\n",
        "# output_shape: (batch_size, input_length, output_dim) --> (None, 75, 40)\n",
        "\n",
        "# 2. Bidirectional layer\n",
        "model = Bidirectional(layer = LSTM(units = 50, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "# output shape: (None, 75, 100) --> output_dim is 100 because its concatenating \n",
        "# the outputs of the forward LSTM and backward LSTM instead of combining them\n",
        "\n",
        "# 3. TimeDistributed layer\n",
        "model = TimeDistributed(Dense(50, activation=\"relu\"))(model) \n",
        "# combines the 100 outputs of the time stamps specified by index (75) into 50 values using ReLU function. \n",
        "\n",
        "# 4. CRF layer (the classifier)\n",
        "crf = CRF(num_tags+1)  \n",
        "out = crf(model)  # output\n",
        "\n",
        "# instantiate the model; out must be the final layer\n",
        "model = Model(input, out) \n",
        "#model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "model.compile(optimizer=\"rmsprop\", loss=losses.crf_loss, metrics=[metrics.crf_accuracy])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 75)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 75, 40)            1277520   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 75, 100)           36400     \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 75, 50)            5050      \n",
            "_________________________________________________________________\n",
            "crf_3 (CRF)                  (None, 75, 7)             420       \n",
            "=================================================================\n",
            "Total params: 1,319,390\n",
            "Trainable params: 1,319,390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCJQxfOs61kP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath = 'models_CRF_LSTM',\n",
        "                       verbose = 0,\n",
        "                       mode = 'auto',\n",
        "                       save_best_only = True,\n",
        "                       monitor='val_loss') "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vePUhAoAGsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d998396e-d552-42c9-a132-fa7c721ba1c4"
      },
      "source": [
        "batch_size"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8sAKAAr64gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(0)\n",
        "history = model.fit(X_train2, np.array(y_train2), batch_size = batch_size,\n",
        "                    epochs = epochs, validation_split = 0.1, callbacks = [checkpointer],)\n",
        "# checkpointer is saving the model at the end of each epoch\n",
        "\n",
        "history.history.keys() \n",
        "# history.history is a dictionary that contains per-epoch timeseries of metrics values "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V2lAEJV7AOi",
        "colab_type": "text"
      },
      "source": [
        "##### **Visualizing the performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71c94IzM7Fdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['crf_viterbi_accuracy']\n",
        "val_acc = history.history['val_crf_viterbi_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.figure(figsize = (8, 8))\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eiUMBxH7DzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (8,8))\n",
        "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
        "plt.title(\"Train and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhKmxb0hl13V",
        "colab_type": "text"
      },
      "source": [
        "##### **Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQMgtiZM4gXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels used in the classification report\n",
        "labels = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkbUV9wGv9sn",
        "colab_type": "text"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e_YpyjF7bYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation over train set\n",
        "y_pred = model.predict(X_valid)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_valid_true = np.argmax(y_valid, -1)\n",
        "\n",
        "# Convert the index to tag\n",
        "y_pred = [[idx2tag[i] for i in row] for row in y_pred]\n",
        "y_valid_true = [[idx2tag[i] for i in row] for row in y_valid_true]\n",
        "\n",
        "print(\"F1-score is : {:.1%}\".format(flat_f1_score(y_valid_true, y_pred, average = 'micro', labels = labels)))\n",
        "\n",
        "report = flat_classification_report(y_true = y_valid_true, y_pred = y_pred, labels = labels)\n",
        "\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOkSgL7v7ix7",
        "colab_type": "text"
      },
      "source": [
        "**Development set**\n",
        "\n",
        "The predictions over the development set are obtained by clinical case. Therefore, loop over all clinical cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bGKKfZwPGVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words(tokens_sent,label_sent, true_label_sent, start_char_pos):\n",
        "  new_tok, new_lab, true_lab, new_start_pos = [], [], [], []\n",
        "\n",
        "  for tokens, labels, true_labels, start_chars in zip(tokens_sent, label_sent, true_label_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, true_lab_aux, new_start_pos_aux = [], [], [], []\n",
        "    for token, label, true_label, start_char_i in zip(tokens, labels,true_labels,start_chars):\n",
        "      if token != \"PAD\":\n",
        "        new_tok_aux.append(token)\n",
        "        new_lab_aux.append(label)\n",
        "        true_lab_aux.append(true_label)\n",
        "        new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    true_lab.append(true_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, true_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3saxCU1Orco",
        "colab_type": "text"
      },
      "source": [
        "**Global metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcgNrJYn7hvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation over train set\n",
        "y_pred_dev = model.predict(X_dev)\n",
        "y_pred_dev = np.argmax(y_pred_dev, axis=-1)\n",
        "y_dev_true = np.argmax(y_dev, -1)\n",
        "\n",
        "# Convert the index to tag\n",
        "y_pred_dev = [[idx2tag[i] for i in row] for row in y_pred_dev]\n",
        "y_dev_true = [[idx2tag[i] for i in row] for row in y_dev_true]\n",
        "\n",
        "print(\"F1-score is : {:.1%}\".format(flat_f1_score(y_dev_true, y_pred_dev, average = 'micro', labels = labels)))\n",
        "\n",
        "report_dev_glob = flat_classification_report(y_true = y_dev_true, y_pred = y_pred_dev, labels = labels)\n",
        "print(report_dev_glob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yni9wVIEOtGg",
        "colab_type": "text"
      },
      "source": [
        "**Metrics by clincal case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxfSqQBVOMst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, true_labels_cc, new_start_pos_cc = [], [], [], []\n",
        "new_tokens_all, new_labels_all, true_labels_all, new_start_pos_all = [], [], [], []\n",
        "\n",
        "label_indices_sent_cc = []\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(X_dev_cc)):\n",
        "  y_pred_dev = model.predict(X_dev_cc[cc])\n",
        "  y_pred_dev = np.argmax(y_pred_dev, axis=-1)\n",
        "  y_dev_true = np.argmax(y_dev_cc[cc], -1)\n",
        "\n",
        "  # Convert the index to tag\n",
        "  y_pred_dev = [[idx2tag[i] for i in row] for row in y_pred_dev]\n",
        "  y_dev_true = [[idx2tag[i] for i in row] for row in y_dev_true]\n",
        "\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = [], [], [], []\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = tokens_to_words(token_dev_cc[cc], \n",
        "                                        y_pred_dev, y_dev_true, start_char_dev_cc[cc])\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  true_labels_cc.append(true_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  true_labels_all.extend(true_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)\n",
        "\n",
        "report_dev_glob = flat_classification_report(y_true = true_labels_all, y_pred = new_labels_all, labels = labels)\n",
        "print(report_dev_glob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-mN5DYWZ0uh",
        "colab_type": "text"
      },
      "source": [
        "##### **Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Or1WpDk3xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(actual, predicted):\n",
        "    #classes       = np.unique(np.concatenate((actual,predicted)))\n",
        "    classes = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR', 'O']\n",
        "    confusion_mtx = np.empty((len(classes),len(classes)),dtype=np.int)\n",
        "    for i,a in enumerate(classes):\n",
        "        for j,p in enumerate(classes):\n",
        "            value = sum([sum([np.where((actual[sent][word]==a)*(predicted[sent][word]==p))[0].shape[0] \n",
        "                              for word in range(len(actual[sent]))]) for sent in range(len(actual))])\n",
        "            #confusion_mtx[i,j] = sum([np.where((actual[sent]==a)*(predicted[sent]==p))[0].shape[0] for sent in range(len(actual))])\n",
        "            confusion_mtx[i,j] = value\n",
        "    return confusion_mtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1vgH_4xovbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code to extract METRICS FOR ENTITY AND NON-ENTITY from the confusion matrix\n",
        "\n",
        "def metrics_from_cm(cm,labels):\n",
        "  TP = [[v for v in value[0:len(labels)]] for value in cm[0:len(labels)]]\n",
        "  print(TP)\n",
        "  TP = np.array(TP)\n",
        "  TP = sum(sum(TP))\n",
        "\n",
        "  FN = [value[-1] for value in cm[0:len(labels)]] # last column is O\n",
        "  print(FN)\n",
        "  FN = np.array(FN) \n",
        "  FN = sum(FN)\n",
        "\n",
        "  FP = cm[len(labels)][0:len(labels)] # last column is O\n",
        "  print(FP)\n",
        "  FP = sum(FP)\n",
        "\n",
        "  TN = cm[len(labels)][len(labels)] # last column is O\n",
        "  print(TN)\n",
        "\n",
        "  return TP, FN, FP, TN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rUYozEOe-ac",
        "colab_type": "text"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANw7GPuAfE66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ad78f446-8817-45bc-d0c4-a0ec7b32dd19"
      },
      "source": [
        "# not done by clinical case\n",
        "actual    = np.array(y_valid_true)\n",
        "predicted = np.array(y_pred)\n",
        "confusion_matrix_train = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(confusion_matrix_train, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>423</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>7</td>\n",
              "      <td>654</td>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>370</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>564</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>84</td>\n",
              "      <td>223</td>\n",
              "      <td>113</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>88076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR      O\n",
              "B-MOR    423     22      0     52      0    131\n",
              "I-MOR      7    654     35      1      0    372\n",
              "E-MOR      0     52    370     36      0    171\n",
              "S-MOR     13      1      9    564      0     30\n",
              "V-MOR      0      1      0      0      0      0\n",
              "O         84    223    113     38      0  88076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nX_c4Kmn3Yc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0cf870fa-8d43-43f0-c89b-1de9fbc5d2ed"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(confusion_matrix_train,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[423, 22, 0, 52, 0], [7, 654, 35, 1, 0], [0, 52, 370, 36, 0], [13, 1, 9, 564, 0], [0, 1, 0, 0, 0]]\n",
            "[131, 372, 171, 30, 0]\n",
            "[ 84 223 113  38   0]\n",
            "88076\n",
            "2240\n",
            "704\n",
            "458\n",
            "88076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k22_gc1be7su",
        "colab_type": "text"
      },
      "source": [
        "**Development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGtSmGDOk8JV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "dfe2eb0f-af65-477b-ee05-3ad91765f3c4"
      },
      "source": [
        "# Example of confusion matrix over first clinical case\n",
        "actual    = np.array(y_dev_by_cc[0])\n",
        "predicted = np.array(y_pred_dev_by_cc[0])\n",
        "cm = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(cm, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df\n",
        "\n",
        "# columns: Predicted \n",
        "# rows: True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR    O\n",
              "B-MOR      5      0      0      0      0    3\n",
              "I-MOR      1     11      1      0      0    7\n",
              "E-MOR      0      1      4      0      0    3\n",
              "S-MOR      0      0      0      3      0    1\n",
              "V-MOR      0      0      0      0      0    0\n",
              "O          2      0      3      0      0  513"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThD6FMTTPf2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# confusion matrix computed over all the clinical cases \n",
        "\n",
        "actual    = np.array(true_labels_all)\n",
        "predicted = np.array(new_labels_all)\n",
        "cm = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(cm, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBSHrMLzp61v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2a2b12c5-f9c4-4032-ce1c-1fef1f41cecf"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(cm,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1022, 48, 7, 126, 0], [26, 1390, 113, 14, 0], [3, 97, 904, 62, 0], [27, 5, 21, 1458, 0], [4, 7, 3, 2, 0]]\n",
            "[417, 1012, 551, 161, 2]\n",
            "[130 260 178  50   0]\n",
            "215875\n",
            "5339\n",
            "2143\n",
            "618\n",
            "215875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B2ZUKJtL-Ky",
        "colab_type": "text"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaHqdexo5E3g",
        "colab_type": "text"
      },
      "source": [
        "#### **Final BiLTM-CRF approach 1 model**\n",
        "\n",
        "Once the optimal model has been obtained, it is trained using the complete dataset, formed by train and development datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6K2WvTU8beY",
        "colab_type": "text"
      },
      "source": [
        "##### **Feature preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CM2OPUFvFEY",
        "colab_type": "text"
      },
      "source": [
        "**Complete set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbVyVEPwj47c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting unique words and labels from data \n",
        "words = list(df_data_complete['Word'].unique())\n",
        "if (preprocess==True):\n",
        "  words = [preprocess_word(w) for w in words]\n",
        "  words = np.unique(words)\n",
        "          \n",
        "tags = list(df_data_complete['Tag'].unique())\n",
        "\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "word_to_index[\"UNK\"] = 1\n",
        "word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "# label is key and value is index.\n",
        "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
        "tag_to_index[\"PAD\"] = 0\n",
        "\n",
        "idx2word = {i: w for w, i in word_to_index.items()}\n",
        "idx2tag = {i: w for w, i in tag_to_index.items()}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUx1cehKryOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "11a2da70-a968-453e-cc5a-ace89b0ed097"
      },
      "source": [
        "sentences_complete[0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ANAMNESIS', 'NOUN', 'O', 0),\n",
              " ('\\n', 'SPACE', 'O', 9),\n",
              " ('Mujer', 'NOUN', 'O', 10),\n",
              " ('de', 'ADP', 'O', 16),\n",
              " ('67', 'NUM', 'O', 19),\n",
              " ('años', 'NOUN', 'O', 22),\n",
              " ('con', 'ADP', 'O', 27),\n",
              " ('antecedentes', 'NOUN', 'O', 31),\n",
              " ('personales', 'ADJ', 'O', 44),\n",
              " ('de', 'ADP', 'O', 55),\n",
              " ('hipotiroidismo', 'NOUN', 'O', 58),\n",
              " ('en', 'ADP', 'O', 73),\n",
              " ('tratamiento', 'NOUN', 'O', 76),\n",
              " ('con', 'ADP', 'O', 88),\n",
              " ('levotiroxina', 'NOUN', 'O', 92),\n",
              " ('y', 'CCONJ', 'O', 105),\n",
              " ('fumadora', 'ADJ', 'O', 107),\n",
              " ('activa', 'ADJ', 'O', 116),\n",
              " ('de', 'ADP', 'O', 123),\n",
              " ('12.5', 'NUM', 'O', 126),\n",
              " ('paquetes', 'NOUN', 'O', 131),\n",
              " ('/', 'PUNCT', 'O', 139),\n",
              " ('año', 'NOUN', 'O', 140),\n",
              " ('.', 'PUNCT', 'O', 143)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnIcryBukH0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting each sentence into list of index from list of tokens\n",
        "X_train = [[word_to_index.get(w[0], 1) for w in s] for s in sentences_complete]\n",
        "\n",
        "# Padding each sequence to have same length  of each word\n",
        "X_train = pad_sequences(maxlen = max_len, sequences = X_train, padding = \"post\",truncating=\"post\", value = word_to_index[\"PAD\"])\n",
        "\n",
        "#Convert label to index\n",
        "y_train = [[tag_to_index[w[2]] for w in s] for s in sentences_complete]\n",
        "\n",
        "# padding\n",
        "y_train = pad_sequences(maxlen = max_len, sequences = y_train, padding = \"post\", truncating=\"post\",value = tag_to_index[\"PAD\"])\n",
        "\n",
        "num_tag = df_data_complete['Tag'].nunique()\n",
        "\n",
        "# One hot encoded labels\n",
        "y_train = [to_categorical(i, num_classes = num_tag + 1) for i in y_train]\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4a86tzM4Y7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a006f9f3-1b79-4ccd-b3a6-e318c5ea9f21"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10,  5, 11, 12, 13,  8, 14, 15, 16,\n",
              "       17,  5, 18, 19, 20, 21, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVoePCFe7ipL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f620d1a0-9f23-4b3d-d763-4100692a16e3"
      },
      "source": [
        "print(tag_to_index)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'O': 1, 'B-MOR': 2, 'E-MOR': 3, 'S-MOR': 4, 'I-MOR': 5, 'V-MOR': 6, 'PAD': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgkaIonfq6mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST SET (done by clinical case)\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_test_cc = []\n",
        "start_char_test_cc = []\n",
        "token_test_cc = []\n",
        "for cc in sentences_test_by_cc:\n",
        "  x_i = [[word_to_index.get(w[0], 1) for w in s] for s in cc]\n",
        "  start_i = [[w[2] for w in s] for s in cc]\n",
        "  token_i = [[w[0] for w in s] for s in cc]\n",
        "\n",
        "  # Padding each sequence to have same length  of each word\n",
        "  X_test_cc.append(pad_sequences(maxlen = max_len, sequences = x_i, padding = \"post\", truncating=\"post\", value = word_to_index[\"PAD\"]))\n",
        "  start_char_test_cc.append(pad_sequences(maxlen=max_len, sequences = start_i, value=-1, padding=\"post\", truncating=\"post\"))\n",
        "  token_test_cc.append(pad_sequences(maxlen=max_len, sequences = token_i, value=\"PAD\", padding=\"post\", truncating=\"post\",dtype= object))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRq1HLOuDBx8",
        "colab_type": "text"
      },
      "source": [
        "##### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbtov-hFlRaZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "01f49db1-a31b-4a57-e71a-65442d0e4260"
      },
      "source": [
        "num_tags = df_data_complete['Tag'].nunique()\n",
        "\n",
        "# Model architecture\n",
        "\n",
        "#inputs = keras.Input(shape=(None, None, 3)) # if it was an RGB image --> None states that the dimension can vary\n",
        "input = Input(shape = (max_len,))\n",
        "# max_len defines the number of words that with be considered as input to the model (can be considered as a window)\n",
        "# Hyperparameter tuning\n",
        "# 2D tensor with shape: (batch_size, input_length). --> <tf.Tensor 'input_5:0' shape=(None, 75) dtype=float32>\n",
        "\n",
        "# 1. Embedding layer\n",
        "model = Embedding(input_dim = len(words) + 2, output_dim = embedding, input_length = max_len, mask_zero = False)(input)\n",
        "# only works if mask_zero = False\n",
        "# words is our dictionary of unique words\n",
        "# output_shape: (batch_size, input_length, output_dim) --> (None, 75, 40)\n",
        "\n",
        "# 2. Bidirectional layer\n",
        "model = Bidirectional(layer = LSTM(units = 50, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "# output shape: (None, 75, 100) --> output_dim is 100 because its concatenating \n",
        "# the outputs of the forward LSTM and backward LSTM instead of combining them\n",
        "\n",
        "# 3. TimeDistributed layer\n",
        "model = TimeDistributed(Dense(50, activation=\"relu\"))(model) \n",
        "# combines the 100 outputs of the time stamps specified by index (75) into 50 values using ReLU function. \n",
        "\n",
        "# 4. CRF layer (the classifier)\n",
        "crf = CRF(num_tags+1)  \n",
        "out = crf(model)  # output\n",
        "\n",
        "# instantiate the model; out must be the final layer\n",
        "model = Model(input, out) \n",
        "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 75)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 75, 40)            1277520   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 75, 100)           36400     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 75, 50)            5050      \n",
            "_________________________________________________________________\n",
            "crf_1 (CRF)                  (None, 75, 7)             420       \n",
            "=================================================================\n",
            "Total params: 1,319,390\n",
            "Trainable params: 1,319,390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2oOi46gllAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath = 'models_CRF_LSTM',\n",
        "                       verbose = 0,\n",
        "                       mode = 'auto',\n",
        "                       save_best_only = True,\n",
        "                       monitor='val_loss') # indicate F1 score???"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayZIaYPhlmdn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "b936b28e-7035-495d-b93e-8badfc71335e"
      },
      "source": [
        "random.seed(0)\n",
        "history = model.fit(X_train, np.array(y_train), batch_size = batch_size,\n",
        "                    epochs = epochs, validation_split = 0.1, callbacks = [checkpointer], )\n",
        "# checkpointer is saving the model at the end of each epoch\n",
        "\n",
        "history.history.keys() \n",
        "# history.history is a dictionary that contains per-epoch timeseries of metrics values "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-dc4cb7426579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(X_train, np.array(y_train), batch_size = batch_size,\n\u001b[0;32m----> 3\u001b[0;31m                     epochs = epochs, validation_split = 0.1, callbacks = [checkpointer], )\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# checkpointer is saving the model at the end of each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/keras_contrib/losses/crf_losses.py:54 crf_loss  *\n        crf, idx = y_pred._keras_history[:2]\n\n    AttributeError: 'Tensor' object has no attribute '_keras_history'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-9_4szHtKnN",
        "colab_type": "text"
      },
      "source": [
        "##### **Test set Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vWbvN3DtG2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words_test(tokens_sent,label_sent, start_char_pos):\n",
        "  new_tok, new_lab, new_start_pos = [], [], []\n",
        "\n",
        "  for tokens, labels, start_chars in zip(tokens_sent, label_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, new_start_pos_aux = [], [], []\n",
        "    for token, label, start_char_i in zip(tokens, labels,start_chars):\n",
        "      if token != \"PAD\":\n",
        "        new_tok_aux.append(token)\n",
        "        new_lab_aux.append(label)\n",
        "        new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONqpGRjutTNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, new_start_pos_cc = [], [], []\n",
        "new_tokens_all, new_labels_all, new_start_pos_all = [], [], []\n",
        "\n",
        "label_indices_sent_cc = []\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(X_test_cc)):\n",
        "  y_pred_test = model.predict(X_test_cc[cc])\n",
        "  y_pred_test = np.argmax(y_pred_test, axis=-1)\n",
        "\n",
        "  # Convert the index to tag\n",
        "  y_pred_test = [[idx2tag[i] for i in row] for row in y_pred_test]\n",
        "\n",
        "  new_tokens, new_labels, new_start_pos = [], [], []\n",
        "  new_tokens, new_labels, new_start_pos = tokens_to_words_test(token_test_cc[cc], \n",
        "                                        y_pred_test, start_char_test_cc[cc])\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i26-pQ5Tvh4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9031abb5-29ae-4984-e1c2-836fe58501eb"
      },
      "source": [
        "# Example:\n",
        "for token, label, new_start in zip(new_tokens_cc[0][0], new_labels_cc[0][0], new_start_pos_cc[0][0]):\n",
        "    print(\"{}\\t{}\\t{}\".format(label, token,new_start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\tPaciente\t0\n",
            "O\tmujer\t9\n",
            "O\t,\t14\n",
            "O\t75\t16\n",
            "O\taños\t19\n",
            "O\tconsulta\t24\n",
            "O\tel\t33\n",
            "O\t4-6-2003\t36\n",
            "O\t,\t44\n",
            "O\trefiriendo\t46\n",
            "O\tcomo\t57\n",
            "O\tantecedentes\t62\n",
            "O\tpersonales\t75\n",
            "O\t:\t85\n",
            "O\tAlergia\t87\n",
            "O\ta\t95\n",
            "O\tsalicilatos\t97\n",
            "O\t.\t108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qftmimmXtRUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(path+'results_BILSTM_ap1/predictions/new_tokens_cc', 'wb') as file: \n",
        "  pkl.dump(new_tokens_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_BILSTM_ap1/predictions/new_labels_cc', 'wb') as file: \n",
        "  pkl.dump(new_labels_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_BILSTM_ap1/predictions/new_start_pos_cc', 'wb') as file: \n",
        "  pkl.dump(new_start_pos_cc, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F-LjOEPMC-g",
        "colab_type": "text"
      },
      "source": [
        "### **Approach 2: BI-LSTM-CRF with a pre-trained word embedding in Spanish**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGj1rXwb8MWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f48a500d-4187-4a4c-b47f-9f96a4b79855"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# Load vectors directly from the file\n",
        "#sbwce_model = KeyedVectors.load_word2vec_format('https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5-skipgram.bin.gz', binary=True)\n",
        "#scielo_wiki_model = KeyedVectors.load_word2vec_format(path+'Scielo+Wiki_skipgram_cased.bin', binary=True)\n",
        "scielo_wiki_model = KeyedVectors.load_word2vec_format(path+'Scielo+Wiki_skipgram_cased.vec') # this is in txt format\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQyeAIK-Ak-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 8\n",
        "max_len = 75\n",
        "embedding = 40"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzmdsm2wkGX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0aa36de1-aec8-4588-a6c7-03df1fb1b991"
      },
      "source": [
        "print(sentences_test_by_cc[0]) # the sentences contain the start char position of each word\n",
        "print(sentences_train[0]) # the sentences contain the start char position of each word"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('Paciente', 'PROPN', 0), ('mujer', 'NOUN', 9), (',', 'PUNCT', 14), ('75', 'NUM', 16), ('años', 'NOUN', 19), ('consulta', 'VERB', 24), ('el', 'DET', 33), ('4-6-2003', 'NOUN', 36), (',', 'PUNCT', 44), ('refiriendo', 'VERB', 46), ('como', 'SCONJ', 57), ('antecedentes', 'NOUN', 62), ('personales', 'ADJ', 75), (':', 'PUNCT', 85), ('Alergia', 'PROPN', 87), ('a', 'ADP', 95), ('salicilatos', 'NOUN', 97), ('.', 'PUNCT', 108)], [('A', 'ADP', 110), ('los', 'DET', 112), ('59', 'NUM', 116), ('años', 'NOUN', 119), ('fué', 'AUX', 124), ('diagnosticada', 'VERB', 128), ('de', 'ADP', 142), ('fiebre', 'NOUN', 145), ('de', 'ADP', 152), ('probable', 'ADJ', 155), ('etiología', 'NOUN', 164), ('específica', 'ADJ', 174), (',', 'PUNCT', 184), ('tratada', 'ADJ', 186), ('con', 'ADP', 194), ('tuberculostáticos', 'NOUN', 198), (',', 'PUNCT', 215), ('según', 'ADP', 217), ('pauta', 'NOUN', 223), ('habitual', 'ADJ', 229), ('.', 'PUNCT', 237)], [('En', 'ADP', 239), ('1.998', 'NUM', 242), (',', 'PUNCT', 247), ('es', 'AUX', 249), ('diagnosticada', 'VERB', 252), ('de', 'ADP', 266), ('fiebre', 'NOUN', 269), ('mediterránea', 'ADJ', 276), ('familiar', 'ADJ', 289), ('después', 'ADV', 298), ('de', 'ADP', 306), ('exclusión', 'NOUN', 309), ('de', 'ADP', 319), ('otros', 'DET', 322), ('procesos', 'NOUN', 328), ('.', 'PUNCT', 336)], [('Osteoporosis', 'NOUN', 338), ('.', 'PUNCT', 350)], [('Está', 'AUX', 352), ('en', 'ADP', 357), ('tratamiento', 'NOUN', 360), ('con', 'ADP', 372), ('Colchicina', 'PROPN', 376), ('(', 'PUNCT', 387), ('0,5', 'NUM', 388), ('mg', 'NOUN', 392), ('/', 'PUNCT', 394), ('día),Aines', 'PROPN', 395), ('(', 'PUNCT', 406), ('50', 'NUM', 407), ('mg', 'INTJ', 410), ('/', 'PUNCT', 412), ('día', 'NOUN', 413), (')', 'PUNCT', 416), (',', 'PUNCT', 417), ('deflazacor', 'PROPN', 419), ('(', 'PUNCT', 430), ('6', 'NUM', 431), ('mg', 'INTJ', 433), ('/', 'PUNCT', 435), ('día', 'NOUN', 436), (')', 'PUNCT', 439), ('y', 'CCONJ', 441), ('Omeprazol', 'PROPN', 443), ('.', 'PUNCT', 452), ('\\n', 'SPACE', 453)], [('Historia', 'PROPN', 454), ('actual', 'ADJ', 463), (':', 'PUNCT', 469), ('La', 'DET', 471), ('paciente', 'NOUN', 474), ('tuvo', 'VERB', 483), ('una', 'DET', 488), ('caída', 'NOUN', 492), ('casual', 'ADJ', 498), ('en', 'ADP', 505), ('octubre', 'INTJ', 508), ('de', 'ADP', 516), ('2.002', 'NUM', 519), ('sobre', 'ADP', 525), ('flanco', 'INTJ', 531), ('izquierdo', 'ADJ', 538), ('presentando', 'VERB', 548), ('10', 'NUM', 560), ('días', 'NOUN', 563), ('después', 'ADV', 568), ('un', 'DET', 576), ('episodio', 'NOUN', 579), ('de', 'ADP', 588), ('hematuria', 'NOUN', 591), ('monosintomática', 'PROPN', 601), ('.', 'PUNCT', 616)], [('En', 'ADP', 618), ('mayo', 'INTJ', 621), ('del', 'ADP', 626), ('2.003', 'NUM', 630), (',', 'PUNCT', 635), ('tuvo', 'VERB', 637), ('fiebre', 'NOUN', 642), ('y', 'CCONJ', 649), ('dolor', 'NOUN', 651), ('lumbar', 'ADJ', 657), ('izquierdo', 'ADJ', 664), ('ingresando', 'VERB', 674), (',', 'PUNCT', 684), ('en', 'ADP', 686), ('un', 'DET', 689), ('centro', 'NOUN', 692), ('hospitalario', 'ADJ', 699), (',', 'PUNCT', 711), ('donde', 'PRON', 713), ('se', 'PRON', 719), ('realiza', 'VERB', 722), ('un', 'DET', 730), ('TAC', 'PROPN', 733), ('abdominal', 'ADJ', 737), ('que', 'SCONJ', 747), ('se', 'PRON', 751), ('informa', 'VERB', 754), ('como', 'SCONJ', 762), ('hipernefroma', 'NOUN', 767), ('izquierdo', 'ADJ', 780), ('con', 'ADP', 790), ('riñón', 'NOUN', 794), ('derecho', 'ADJ', 800), ('normal', 'ADJ', 808), ('.', 'PUNCT', 814)], [('Se', 'PRON', 816), ('le', 'PRON', 819), ('trata', 'VERB', 822), ('con', 'ADP', 828), ('quinolonas', 'NOUN', 832), ('.', 'PUNCT', 842)], [('No', 'ADV', 844), ('se', 'PRON', 847), ('realiza', 'VERB', 850), ('cultivo', 'NOUN', 858), ('.', 'PUNCT', 865)], [('En', 'ADP', 867), ('el', 'DET', 870), ('momento', 'NOUN', 873), ('de', 'ADP', 881), ('la', 'DET', 884), ('consulta', 'NOUN', 887), ('se', 'PRON', 896), ('encuentra', 'VERB', 899), ('asintomática', 'ADJ', 909), (',', 'PUNCT', 921), ('sin', 'ADP', 923), ('fiebre', 'NOUN', 927), (',', 'PUNCT', 933), ('ni', 'CCONJ', 935), ('dolor', 'NOUN', 938), ('lumbar', 'ADJ', 944), ('.', 'PUNCT', 950), ('\\n', 'SPACE', 951)], [('Exploración', 'PROPN', 952), (':', 'PUNCT', 963), ('no', 'ADV', 965), ('se', 'PRON', 968), ('palpan', 'AUX', 971), ('masas', 'NOUN', 978), ('abdominales', 'ADJ', 984), ('.', 'PUNCT', 995), ('\\n', 'SPACE', 996)], [('Analítica', 'PROPN', 997), (':', 'PUNCT', 1006), ('Hemoglobina', 'PROPN', 1008), ('12,3', 'NUM', 1020), ('.', 'PUNCT', 1024)], [('Hematocrito', 'PROPN', 1026), ('37,1', 'NUM', 1038), ('.', 'PUNCT', 1042)], [('Leucocitos', 'PROPN', 1044), ('11.200', 'NUM', 1055), ('(', 'PUNCT', 1062), ('Neutrofilos', 'PROPN', 1063), ('76).Glucosa', 'NUM', 1075), (',', 'PUNCT', 1086), ('urea', 'NOUN', 1088), (',', 'PUNCT', 1092), ('creatinina', 'NOUN', 1094), (',', 'PUNCT', 1104), ('BT', 'PROPN', 1106), (',', 'PUNCT', 1108), ('BD', 'PROPN', 1110), (',', 'PUNCT', 1112), ('GOT', 'PROPN', 1114), ('y', 'CCONJ', 1118), ('GPT', 'PROPN', 1120), (',', 'PUNCT', 1123), ('son', 'VERB', 1125), ('normales', 'ADJ', 1129), ('.', 'PUNCT', 1137)], [('Sedimento', 'PRON', 1139), ('de', 'ADP', 1149), ('orina', 'NOUN', 1152), (',', 'PUNCT', 1157), ('10-15', 'NUM', 1159), ('leucocitos', 'NOUN', 1165), ('/', 'PUNCT', 1175), ('campo', 'NOUN', 1176), ('.', 'PUNCT', 1181)], [('Urinocultivo', 'PROPN', 1183), (':', 'PUNCT', 1195), ('E.', 'PROPN', 1197), ('Coli', 'PROPN', 1200), ('.', 'PUNCT', 1204)], [('Ecografía', 'PROPN', 1206), ('renal', 'ADJ', 1216), (':', 'PUNCT', 1221), ('Masa', 'PROPN', 1223), ('renal', 'PROPN', 1228), ('izquierda', 'ADJ', 1234), ('sólido-quística', 'PROPN', 1244), ('de', 'ADP', 1260), ('12', 'NUM', 1263), ('cm', 'NOUN', 1266), ('compatible', 'ADJ', 1269), ('con', 'ADP', 1280), ('hipernefroma', 'NOUN', 1284), ('.', 'PUNCT', 1296)], [('Riñón', 'PROPN', 1297), ('derecho', 'NOUN', 1303), ('normal', 'ADJ', 1311), ('.', 'PUNCT', 1317), ('\\n', 'SPACE', 1318)], [('TAC', 'PROPN', 1319), ('Abdominal', 'PROPN', 1323), (':', 'PUNCT', 1332), ('(', 'PUNCT', 1334), ('10-06-03', 'NUM', 1335), (')', 'PUNCT', 1343), ('Masa', 'PROPN', 1345), ('en', 'ADP', 1350), ('la', 'DET', 1353), ('cara', 'NOUN', 1356), ('lateral', 'ADJ', 1361), ('del', 'ADP', 1369), ('riñón', 'NOUN', 1373), ('izquierdo', 'ADJ', 1379), ('que', 'SCONJ', 1389), ('produce', 'VERB', 1393), ('un', 'DET', 1401), ('desplazamiento', 'NOUN', 1404), ('y', 'CCONJ', 1419), ('una', 'DET', 1421), ('horizontalización', 'NOUN', 1425), ('del', 'ADP', 1443), ('riñón', 'NOUN', 1447), ('que', 'SCONJ', 1453), ('tiene', 'VERB', 1457), ('un', 'DET', 1463), ('diámetro', 'NOUN', 1466), ('máximo', 'ADJ', 1475), ('de', 'ADP', 1482), ('12', 'NUM', 1485), ('cm', 'NOUN', 1488), ('presentando', 'VERB', 1491), ('grandes', 'ADJ', 1503), ('áreas', 'NOUN', 1511), ('quísticas', 'ADJ', 1517), ('y', 'CCONJ', 1527), ('otra', 'DET', 1529), ('central', 'NOUN', 1534), ('de', 'ADP', 1542), ('componente', 'NOUN', 1545), ('sólido', 'ADJ', 1556), (',', 'PUNCT', 1562), ('pero', 'CCONJ', 1564), ('incluso', 'ADV', 1569), ('las', 'DET', 1577), ('áreas', 'NOUN', 1581), ('quísticas', 'ADJ', 1587), ('presentan', 'AUX', 1597), ('una', 'DET', 1607), ('pared', 'NOUN', 1611), ('engrosada', 'ADJ', 1617), (',', 'PUNCT', 1626), ('por', 'ADP', 1628), ('lo', 'PRON', 1632), ('que', 'SCONJ', 1635), ('debe', 'AUX', 1639), ('tratarse', 'VERB', 1644), ('de', 'ADP', 1653), ('una', 'DET', 1656), ('tumoración', 'NOUN', 1660), ('sólida', 'ADJ', 1671), ('.', 'PUNCT', 1677)], [('Otra', 'DET', 1679), ('posibilidad', 'NOUN', 1684), ('es', 'AUX', 1696), ('que', 'SCONJ', 1699), ('se', 'PRON', 1703), ('trate', 'VERB', 1706), ('de', 'ADP', 1712), ('un', 'DET', 1715), ('nefroma', 'NOUN', 1718), ('quístico', 'ADJ', 1726), (',', 'PUNCT', 1734), ('pero', 'CCONJ', 1736), ('en', 'ADP', 1741), ('cualquier', 'DET', 1744), ('caso', 'NOUN', 1754), ('la', 'DET', 1759), ('lesión', 'NOUN', 1762), ('es', 'AUX', 1769), ('de', 'ADP', 1772), ('aspecto', 'NOUN', 1775), ('tumoral', 'ADJ', 1783), ('y', 'CCONJ', 1791), ('tiene', 'VERB', 1793), ('un', 'DET', 1799), ('contorno', 'NOUN', 1802), ('mal', 'ADV', 1811), ('definido', 'ADJ', 1815), (',', 'PUNCT', 1823), ('por', 'ADP', 1825), ('lo', 'PRON', 1829), ('que', 'SCONJ', 1832), ('parece', 'VERB', 1836), ('que', 'SCONJ', 1843), ('existe', 'VERB', 1847), ('infiltración', 'NOUN', 1854), ('de', 'ADP', 1867), ('la', 'DET', 1870), ('grasa', 'NOUN', 1873), ('perirrenal', 'ADJ', 1879), ('e', 'CCONJ', 1890), ('incluso', 'ADV', 1892), ('en', 'ADP', 1900), ('la', 'DET', 1903), ('parte', 'NOUN', 1906), ('posterior', 'ADJ', 1912), ('esta', 'DET', 1922), ('lesión', 'NOUN', 1927), ('está', 'VERB', 1934), ('en', 'ADP', 1939), ('contacto', 'NOUN', 1942), ('con', 'ADP', 1951), ('la', 'DET', 1955), ('pared', 'NOUN', 1958), ('abdominal', 'ADJ', 1964), ('en', 'ADP', 1974), ('la', 'DET', 1977), ('región', 'NOUN', 1980), ('lumbar', 'ADJ', 1987), ('con', 'ADP', 1994), ('engrosamiento', 'NOUN', 1998), ('de', 'ADP', 2012), ('la', 'DET', 2015), ('musculatura', 'NOUN', 2018), ('por', 'ADP', 2030), ('encima', 'ADV', 2034), ('de', 'ADP', 2041), ('la', 'DET', 2044), ('cresta', 'NOUN', 2047), ('iliaca', 'ADJ', 2054), (',', 'PUNCT', 2060), ('sugestivo', 'ADJ', 2062), ('de', 'ADP', 2072), ('infiltración', 'NOUN', 2075), ('de', 'ADP', 2088), ('los', 'DET', 2091), ('músculos', 'NOUN', 2095), ('de', 'ADP', 2104), ('la', 'DET', 2107), ('pared', 'NOUN', 2110), (',', 'PUNCT', 2115), ('a', 'ADP', 2117), ('nivel', 'INTJ', 2119), ('del', 'ADP', 2125), ('cuadrado', 'NOUN', 2129), ('lumbar', 'ADJ', 2138), ('.', 'PUNCT', 2144)], [('Riñón', 'PROPN', 2146), ('derecho', 'NOUN', 2152), ('normal', 'ADJ', 2160), ('.', 'PUNCT', 2166), ('\\n', 'SPACE', 2167)], [('Adenopatías', 'ADV', 2168), ('en', 'ADP', 2180), ('ilio', 'PROPN', 2183), ('hepático', 'ADJ', 2188), ('superiores', 'ADJ', 2197), ('a', 'ADP', 2208), ('un', 'DET', 2210), ('cm', 'INTJ', 2213), ('.', 'PUNCT', 2215), ('\\n', 'SPACE', 2216)], [('La', 'DET', 2217), ('paciente', 'NOUN', 2220), ('se', 'PRON', 2229), ('programa', 'NOUN', 2232), ('para', 'ADP', 2241), ('estudio', 'NOUN', 2246), ('preoperatorio', 'ADJ', 2254), ('ambulatorio', 'ADJ', 2268), ('y', 'CCONJ', 2280), ('nefrectomía', 'NOUN', 2282), ('.', 'PUNCT', 2293), ('\\n', 'SPACE', 2294)], [('El', 'DET', 2295), ('25-06-03', 'NOUN', 2298), (',', 'PUNCT', 2306), ('la', 'DET', 2308), ('paciente', 'NOUN', 2311), ('ingresó', 'VERB', 2320), ('por', 'ADP', 2328), ('presentar', 'VERB', 2332), ('fiebre', 'NOUN', 2342), ('de', 'ADP', 2349), ('38ºC', 'NUM', 2352), ('y', 'CCONJ', 2357), ('dolor', 'NOUN', 2359), ('lumbar', 'ADJ', 2365), ('moderado', 'ADJ', 2372), ('sin', 'ADP', 2381), ('afectación', 'NOUN', 2385), ('del', 'ADP', 2396), ('estado', 'PROPN', 2400), ('general', 'ADJ', 2407), ('.', 'PUNCT', 2414), ('\\n', 'SPACE', 2415)], [('Exploración', 'PROPN', 2416), (':', 'PUNCT', 2427), ('masa', 'NOUN', 2429), ('lumbar', 'ADJ', 2434), ('izquierda', 'NOUN', 2441), ('.', 'PUNCT', 2450)], [('Puño', 'VERB', 2452), ('percusión', 'NOUN', 2457), ('renal', 'ADJ', 2467), ('izquierda', 'ADJ', 2473), ('positiva', 'ADJ', 2483), ('.', 'PUNCT', 2491), ('\\n', 'SPACE', 2492)], [('Analítica', 'PROPN', 2493), (':', 'PUNCT', 2502), ('Hemoglobina', 'PROPN', 2504), ('9,9', 'NUM', 2516), ('.', 'PUNCT', 2519)], [('Hematocrito', 'PROPN', 2521), ('28,1', 'NUM', 2533), ('.', 'PUNCT', 2537)], [('Leucocitos', 'PROPN', 2539), ('18.400', 'NUM', 2550), ('.', 'PUNCT', 2556)], [('Neutrofilos', 'PRON', 2558), ('87,6', 'NUM', 2570), ('.', 'PUNCT', 2574)], [('Cayados', 'VERB', 2576), ('5', 'NUM', 2584), ('.', 'PUNCT', 2585)], [('Plaquetas', 'PROPN', 2587), ('328.000', 'NUM', 2597), ('.', 'PUNCT', 2604)], [('Urea', 'PROPN', 2606), (',', 'PUNCT', 2610), ('Creatinina', 'PROPN', 2612), ('y', 'CCONJ', 2623), ('pruebas', 'NOUN', 2625), ('de', 'ADP', 2633), ('función', 'NOUN', 2636), ('hepática', 'ADJ', 2644), (',', 'PUNCT', 2652), ('normales', 'ADJ', 2654), ('.', 'PUNCT', 2662)], [('Sedimento', 'PROPN', 2664), ('de', 'ADP', 2674), ('orina', 'NOUN', 2677), (',', 'PUNCT', 2682), ('100', 'NUM', 2684), ('leucocitos', 'NOUN', 2688), ('/campo', 'PROPN', 2699), ('.', 'PUNCT', 2705)], [('Urinocultivo', 'PROPN', 2707), (':', 'PUNCT', 2719), ('E.', 'PROPN', 2721), ('Coli', 'PROPN', 2724), ('.', 'PUNCT', 2728), ('\\n', 'SPACE', 2729)], [('Se', 'PRON', 2730), ('realiza', 'VERB', 2733), ('Ecografía', 'NOUN', 2741), ('abdominal', 'ADJ', 2751), ('que', 'SCONJ', 2761), ('se', 'PRON', 2765), ('informa', 'VERB', 2768), ('de', 'ADP', 2776), ('una', 'DET', 2779), ('gran', 'ADJ', 2783), ('masa', 'NOUN', 2788), ('sólida', 'ADJ', 2793), (',', 'PUNCT', 2799), ('heterogénea', 'ADJ', 2801), (',', 'PUNCT', 2812), ('polilobulada', 'ADJ', 2814), ('con', 'ADP', 2827), ('zonas', 'NOUN', 2831), ('quísticas', 'ADJ', 2837), ('ocupando', 'VERB', 2847), ('los', 'DET', 2856), ('dos', 'NUM', 2860), ('tercios', 'NOUN', 2864), ('inferiores', 'ADJ', 2872), ('del', 'ADP', 2883), ('riñón', 'NOUN', 2887), ('izquierdo', 'ADJ', 2893), ('y', 'CCONJ', 2903), ('que', 'SCONJ', 2905), ('se', 'PRON', 2909), ('extiende', 'VERB', 2912), ('hasta', 'ADP', 2921), ('la', 'DET', 2927), ('pala', 'NOUN', 2930), ('iliaca', 'ADJ', 2935), ('y', 'CCONJ', 2942), ('contacta', 'VERB', 2944), ('con', 'ADP', 2953), ('la', 'DET', 2957), ('pared', 'NOUN', 2960), ('abdominal', 'ADJ', 2966), ('posterior', 'ADJ', 2976), ('.', 'PUNCT', 2985), ('\\n', 'SPACE', 2986)], [('Un', 'DET', 2987), ('TAC', 'PROPN', 2990), ('abdominal', 'ADJ', 2994), ('se', 'PRON', 3004), ('informa', 'VERB', 3007), ('como', 'SCONJ', 3015), ('masa', 'NOUN', 3020), ('compleja', 'ADJ', 3025), ('en', 'ADP', 3034), ('relación', 'INTJ', 3037), ('con', 'ADP', 3046), ('el', 'DET', 3050), ('polo', 'NOUN', 3053), ('inferior', 'ADJ', 3058), ('del', 'ADP', 3067), ('riñón', 'NOUN', 3071), ('izquierdo', 'ADJ', 3077), ('Figuras', 'PROPN', 3087), ('2', 'NUM', 3095), ('y', 'CCONJ', 3097), ('3).Esta', 'PROPN', 3099), ('masa', 'NOUN', 3107), ('tiene', 'VERB', 3112), ('unos', 'DET', 3118), ('bordes', 'NOUN', 3123), ('irregulares', 'ADJ', 3130), (',', 'PUNCT', 3141), ('densidad', 'NOUN', 3143), ('mixta', 'ADJ', 3152), (',', 'PUNCT', 3157), ('con', 'ADP', 3159), ('zonas', 'NOUN', 3163), ('de', 'ADP', 3169), ('alta', 'ADJ', 3172), ('densidad', 'NOUN', 3177), ('y', 'CCONJ', 3186), ('otras', 'PRON', 3188), ('realmente', 'ADV', 3194), ('quísticas', 'ADJ', 3204), ('.', 'PUNCT', 3213)], [('La', 'DET', 3215), ('masa', 'NOUN', 3218), ('crece', 'VERB', 3223), ('fuera', 'ADV', 3229), ('de', 'ADP', 3235), ('la', 'DET', 3238), ('celda', 'NOUN', 3241), ('renal', 'ADJ', 3247), (',', 'PUNCT', 3252), ('invadiendo', 'VERB', 3254), ('el', 'DET', 3265), ('espacio', 'NOUN', 3268), ('pararrenal', 'ADJ', 3276), ('posterior', 'ADJ', 3287), ('y', 'CCONJ', 3297), ('destruyendo', 'VERB', 3299), ('la', 'DET', 3311), ('pared', 'NOUN', 3314), ('abdominal', 'ADJ', 3320), (',', 'PUNCT', 3329), ('a', 'ADP', 3331), ('nivel', 'INTJ', 3333), ('de', 'ADP', 3339), ('la', 'DET', 3342), ('inserción', 'NOUN', 3345), ('de', 'ADP', 3355), ('los', 'DET', 3358), ('músculos', 'NOUN', 3362), ('oblicuos', 'ADJ', 3371), ('y', 'CCONJ', 3380), ('transversos', 'ADJ', 3382), ('en', 'ADP', 3394), ('el', 'DET', 3397), ('borde', 'NOUN', 3400), ('superior', 'ADJ', 3406), ('del', 'ADP', 3415), ('ala', 'NOUN', 3419), ('iliaca', 'ADJ', 3423), ('izquierda', 'ADJ', 3430), ('.', 'PUNCT', 3439)], [('El', 'DET', 3441), ('diámetro', 'NOUN', 3444), ('craneo', 'ADJ', 3453), ('caudal', 'NOUN', 3460), ('de', 'ADP', 3467), ('la', 'DET', 3470), ('masa', 'NOUN', 3473), ('es', 'AUX', 3478), ('de', 'ADP', 3481), ('12', 'NUM', 3484), ('cm', 'NOUN', 3487), ('.', 'PUNCT', 3489)], [('y', 'CCONJ', 3491), ('muestra', 'VERB', 3493), ('un', 'DET', 3501), ('diámetro', 'NOUN', 3504), ('similar', 'ADJ', 3513), ('.', 'PUNCT', 3520)], [('Se', 'PRON', 3522), ('coloca', 'VERB', 3525), ('un', 'DET', 3532), ('drenaje', 'NOUN', 3535), ('de', 'ADP', 3543), ('las', 'DET', 3546), ('zonas', 'NOUN', 3550), ('quísticas', 'ADJ', 3556), ('saliendo', 'VERB', 3566), ('abundante', 'ADJ', 3575), ('pus', 'NOUN', 3585), ('.', 'PUNCT', 3588)], [('El', 'DET', 3590), ('cultivo', 'NOUN', 3593), ('del', 'ADP', 3601), ('pus', 'NOUN', 3605), ('drenado', 'ADJ', 3609), ('es', 'AUX', 3617), ('E.', 'PROPN', 3620), ('Coli', 'PROPN', 3623), ('.', 'PUNCT', 3627)], [('Instaurando', 'ADJ', 3629), ('tratamiento', 'NOUN', 3641), ('con', 'ADP', 3653), ('Ciprofloxacino', 'PROPN', 3657), ('IV', 'PROPN', 3672), ('200', 'NUM', 3675), ('mgr', 'INTJ', 3679), ('/cada', 'PROPN', 3683), ('12', 'NUM', 3689), ('horas', 'NOUN', 3692), ('.', 'PUNCT', 3697), ('\\n', 'SPACE', 3698)], [('El', 'DET', 3699), ('4-07-03', 'NOUN', 3702), (',', 'PUNCT', 3709), ('se', 'PRON', 3711), ('realizó', 'VERB', 3714), ('nefrectomía', 'NOUN', 3722), ('izquierda', 'ADJ', 3734), ('por', 'ADP', 3744), ('lumbotomía', 'PROPN', 3748), ('sin', 'ADP', 3759), ('incidencias', 'NOUN', 3763), ('.', 'PUNCT', 3774), ('\\n', 'SPACE', 3775)], [('El', 'DET', 3776), ('diagnóstico', 'NOUN', 3779), ('anatomopatológico', 'ADJ', 3791), ('pone', 'VERB', 3809), ('de', 'ADP', 3814), ('manifiesto', 'NOUN', 3817), ('unas', 'DET', 3828), ('células', 'NOUN', 3833), ('histiocitarias', 'ADJ', 3841), ('poligonales', 'ADJ', 3856), ('o', 'CCONJ', 3868), ('fusocelulares', 'PROPN', 3870), ('eosinofílas', 'ADJ', 3884), ('con', 'ADP', 3896), ('cuerpos', 'NOUN', 3900), ('de', 'ADP', 3908), ('Michaelis-Gutmann', 'PROPN', 3911), ('y', 'CCONJ', 3929), ('linfocitos', 'NOUN', 3931), ('acompañantes', 'ADJ', 3942), ('.', 'PUNCT', 3954), ('\\n', 'SPACE', 3955)], [('El', 'DET', 3956), ('postoperatorio', 'NOUN', 3959), ('cursó', 'VERB', 3974), ('sin', 'ADP', 3980), ('complicaciones', 'NOUN', 3984), ('.', 'PUNCT', 3998)], [('Se', 'PRON', 4000), ('mantuvo', 'VERB', 4003), ('el', 'DET', 4011), ('tratamiento', 'NOUN', 4014), ('con', 'ADP', 4026), ('ciprofloxacino', 'NOUN', 4030), ('oral', 'ADJ', 4045), ('a', 'ADP', 4050), ('dosis', 'NOUN', 4052), ('habituales', 'ADJ', 4058), ('durante', 'ADP', 4069), ('un', 'DET', 4077), ('mes', 'NOUN', 4080), ('.', 'PUNCT', 4083), ('\\n', 'SPACE', 4084)]]\n",
            "[('ANAMNESIS', 'NOUN', 'O', 0), ('\\n', 'SPACE', 'O', 9), ('Mujer', 'NOUN', 'O', 10), ('de', 'ADP', 'O', 16), ('67', 'NUM', 'O', 19), ('años', 'NOUN', 'O', 22), ('con', 'ADP', 'O', 27), ('antecedentes', 'NOUN', 'O', 31), ('personales', 'ADJ', 'O', 44), ('de', 'ADP', 'O', 55), ('hipotiroidismo', 'NOUN', 'O', 58), ('en', 'ADP', 'O', 73), ('tratamiento', 'NOUN', 'O', 76), ('con', 'ADP', 'O', 88), ('levotiroxina', 'NOUN', 'O', 92), ('y', 'CCONJ', 'O', 105), ('fumadora', 'ADJ', 'O', 107), ('activa', 'ADJ', 'O', 116), ('de', 'ADP', 'O', 123), ('12.5', 'NUM', 'O', 126), ('paquetes', 'NOUN', 'O', 131), ('/', 'PUNCT', 'O', 139), ('año', 'NOUN', 'O', 140), ('.', 'PUNCT', 'O', 143)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFSTdmUV-DG0",
        "colab": {}
      },
      "source": [
        "def modif_get_vector(model,word):\n",
        "  try:\n",
        "    vector = model.get_vector(word)\n",
        "  except:\n",
        "    vector = np.ones(300,dtype = 'float32') # 300 is the size of the embedding \n",
        "  return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqSZGH6OM03u",
        "colab_type": "text"
      },
      "source": [
        "#### **Train and evaluate the BiLTSM-CRF approach 2 model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dmLW649-DGt"
      },
      "source": [
        "##### **Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hWc-r1x6-DGt",
        "colab": {}
      },
      "source": [
        "#Getting unique words and labels from data \n",
        "words = list(df_data_train2['Word'].unique())\n",
        "if (preprocess==True):\n",
        "  words = [preprocess_word(w) for w in words]\n",
        "  words = np.unique(words)\n",
        "  \n",
        "tags = list(df_data_train2['Tag'].unique())\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "#word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "#word_to_index[\"UNK\"] = 1\n",
        "#word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "# label is key and value is index.\n",
        "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
        "tag_to_index[\"PAD\"] = 0\n",
        "\n",
        "#idx2word = {i: w for w, i in word_to_index.items()}\n",
        "idx2tag = {i: w for w, i in tag_to_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKCyHK7Hqiq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b92627b-94c2-487e-eae7-68c47210db98"
      },
      "source": [
        "len(scielo_wiki_model.get_vector('pulmón')) # the pretrained word embedding represents each word as a vector of 300 components"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EEiPVFvxU-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adf3348d-b032-47e9-acc0-3263a11a3dfc"
      },
      "source": [
        "lens = [len(sent) for sent in sentences_train]\n",
        "np.argmax(lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcXjS2E5xzl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_sentences(sentences): \n",
        "  sentences2 = []\n",
        "  for sent in sentences:\n",
        "    if len(sent) > 2.5*max_len:\n",
        "      sentences2.append(sent[:max_len])\n",
        "      sentences2.append(sent[max_len:2*max_len])\n",
        "      sentences2.append(sent[2*max_len:])\n",
        "\n",
        "    elif len(sent) > 1.5*max_len:\n",
        "      sentences2.append(sent[:max_len])\n",
        "      sentences2.append(sent[max_len:])\n",
        "\n",
        "    else:\n",
        "      sentences2.append(sent)\n",
        "  return sentences2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR1eMq2Y1DdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentences_train = split_sentences(sentences_train)\n",
        "#sentences_dev = split_sentences(sentences_dev)\n",
        "\n",
        "#sentences_dev_by_cc = []\n",
        "#for cc in sentences_dev_by_cc:\n",
        "#  sentences_dev_by_cc.append(split_sentences(cc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f6nuDAgn-DG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c6297283-3a5a-419f-8912-aabd2084dade"
      },
      "source": [
        "# Converting each sentence into list of vectors from list of tokens\n",
        "#X_train = [[model_word2vec[w[0]] for w in s] for s in sentences_train]\n",
        "X_train = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in sentences_train]\n",
        "\n",
        "#X_train = [[model_word2vec.wv.most_similar_cosmul(w[0]) for w in s] for s in sentences_train]\n",
        "\n",
        "print(\"Vectorized sequence:\")\n",
        "print(type(X_train[0][0][0]))\n",
        "print(\"Sequence length of the first sentence: %d\" %len(X_train[0]))\n",
        "\n",
        "# Padding each sequence to have same length  of each word\n",
        "X_train = pad_sequences(maxlen = max_len, sequences = X_train, padding = \"post\", truncating=\"post\",  dtype= np.float32)\n",
        "\n",
        "print(\"\\nPadded sequence: \")\n",
        "print(type(X_train[0][0][0]))\n",
        "print(\"Sequence length of the first sentence: %d\" %len(X_train[0]))\n",
        "\n",
        "#Convert label to index\n",
        "y_train = [[tag_to_index[w[2]] for w in s] for s in sentences_train]\n",
        "\n",
        "# padding\n",
        "y_train = pad_sequences(maxlen = max_len, sequences = y_train, padding = \"post\", truncating=\"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "num_tags = df_data_train2['Tag'].nunique()\n",
        "\n",
        "# One hot encoded labels\n",
        "y_train = [to_categorical(i, num_classes = num_tags + 1) for i in y_train]\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorized sequence:\n",
            "<class 'numpy.float32'>\n",
            "Sequence length of the first sentence: 24\n",
            "\n",
            "Padded sequence: \n",
            "<class 'numpy.float32'>\n",
            "Sequence length of the first sentence: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ANFtwuKk-DG4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "146416b4-1f42-4a96-aa2f-616907d398a3"
      },
      "source": [
        "X_train[0] # the padded elements are arrays of 0's"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.      ,  1.      ,  1.      , ...,  1.      ,  1.      ,\n",
              "         1.      ],\n",
              "       [ 1.      ,  1.      ,  1.      , ...,  1.      ,  1.      ,\n",
              "         1.      ],\n",
              "       [-0.10394 , -0.57349 ,  0.49501 , ..., -0.25701 ,  0.34359 ,\n",
              "        -0.085882],\n",
              "       ...,\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6EaSIsIQ-DG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22a1f6d9-6981-4b48-98a2-26751bab7ef8"
      },
      "source": [
        "print(tag_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'O': 1, 'B-MOR': 2, 'E-MOR': 3, 'S-MOR': 4, 'I-MOR': 5, 'V-MOR': 6, 'PAD': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9JNqyxG6-DG7"
      },
      "source": [
        "##### **Development set**\n",
        "\n",
        "Repeat for development set 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "doHCoZ3Q-DHA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "900acc93-2025-4266-f8c5-7f783359a117"
      },
      "source": [
        "# DEVELOPMENT SET (without considering clinical cases independently)\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "#X_dev = [[model_word2vec.get(w[0], 1) for w in s] for s in sentences_dev] \n",
        "X_dev = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in sentences_dev] \n",
        "\n",
        "# if the word is not in the vocabulary, is set to 1, which is the label \"UNK\" (unknown)\n",
        "\n",
        "# Padding each sequence to have same length of each word\n",
        "X_dev = pad_sequences(maxlen = max_len, sequences = X_dev, padding = \"post\", truncating=\"post\", dtype= np.float32)\n",
        "\n",
        "# Convert label to index\n",
        "y_dev = [[tag_to_index[w[2]] for w in s] for s in sentences_dev]\n",
        "\n",
        "# padding\n",
        "y_dev = pad_sequences(maxlen = max_len, sequences = y_dev, padding = \"post\", truncating=\"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "# One hot encoded labels\n",
        "y_dev = [to_categorical(i, num_classes = num_tags + 1) for i in y_dev]\n",
        "\n",
        "# Start char position\n",
        "start_char_dev = [[w[3] for w in s] for s in sentences_dev]\n",
        "\n",
        "start_char_dev = pad_sequences(start_char_dev,\n",
        "                     maxlen=max_len, value=-1, padding=\"post\", truncating=\"post\")\n",
        "print(\"\\nStart char positions the first sentence: \\n %s\" %start_char_dev[0])\n",
        "\n",
        "# true token \n",
        "token_dev = [[w[0] for w in s] for s in sentences_dev] \n",
        "token_dev = pad_sequences(maxlen = max_len, sequences = token_dev, padding = \"post\", truncating=\"post\", value = \"PAD\",dtype= object)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# DEVELOPMENT SET (done by clinical case)\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_dev_cc = []\n",
        "y_dev_cc = []\n",
        "start_char_dev_cc = []\n",
        "token_dev_cc = []\n",
        "for cc in sentences_dev_by_cc:\n",
        "  x_i = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in cc]\n",
        "  y_i = [[tag_to_index[w[2]] for w in s] for s in cc] #Convert label to index\n",
        "  start_i = [[w[3] for w in s] for s in cc]\n",
        "  token_i = [[w[0] for w in s] for s in cc]\n",
        "\n",
        "  # Padding each sequence to have same length  of each word\n",
        "  X_dev_cc.append(pad_sequences(maxlen = max_len, sequences = x_i, padding = \"post\", truncating=\"post\", dtype= np.float32))\n",
        "  y_dev_cc.append(pad_sequences(maxlen = max_len, sequences = y_i, padding = \"post\", truncating=\"post\", value = tag_to_index[\"PAD\"]))\n",
        "  start_char_dev_cc.append(pad_sequences(maxlen=max_len, sequences = start_i, value=-1, padding=\"post\", truncating=\"post\"))\n",
        "  token_dev_cc.append(pad_sequences(maxlen=max_len, sequences = token_i, value=\"PAD\", padding=\"post\", truncating=\"post\",dtype= object))\n",
        "## One hot encoded labels\n",
        "y_dev_cc = [to_categorical(i, num_classes = num_tags + 1) for i in y_dev_cc]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start char positions the first sentence: \n",
            " [  0   9  10  16  19  22  26  28  38  44  49  52  56  58  62  68  80  83\n",
            "  95  97 108 110 127 129 132 144 147 153 156 161 164 174 177 185 200 203\n",
            " 210 215 216 227 229 234 235  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFwpuxEErv1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9ce9dd9d-327b-4635-82b7-be5da06f00cd"
      },
      "source": [
        "token_dev_cc[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Anamnesis', '\\n', 'Varón', 'de', '74', 'años', ',', 'exfumador',\n",
              "       'desde', 'hace', '15', 'años', ',', 'con', 'único', 'antecedente',\n",
              "       'de', 'hipertensión', ',', 'dislipemia', 'y', 'apendicectomizado',\n",
              "       ';', 'se', 'diagnostica', 'en', 'marzo', 'de', '2013', 'de',\n",
              "       'carcinoma', 'de', 'células', 'transicionales', 'de', 'vejiga',\n",
              "       'E-IV', '(', 'pulmonares', 'y', 'óseas', ')', '.', 'PAD', 'PAD',\n",
              "       'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD',\n",
              "       'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD',\n",
              "       'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD',\n",
              "       'PAD', 'PAD', 'PAD'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KH9jQfGj-DHC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "970586db-4e17-4a72-f0eb-63a1cf5ff512"
      },
      "source": [
        "len(X_dev[0])\n",
        "len(sentences_dev[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xaFOiPsY-DHE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "204920e7-f801-4f97-e687-5ec53660f419"
      },
      "source": [
        "X_dev[0] # first sentence of first clinical case"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.20988 , -0.90066 ,  0.15487 , ..., -0.47857 , -0.10083 ,\n",
              "        -0.47624 ],\n",
              "       [ 1.      ,  1.      ,  1.      , ...,  1.      ,  1.      ,\n",
              "         1.      ],\n",
              "       [ 0.27202 ,  0.15571 ,  0.015991, ..., -0.38315 ,  0.24421 ,\n",
              "         0.087124],\n",
              "       ...,\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zq3YqrZS-DHF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b538ce5e-794c-408f-dbf1-afb1b2e9dcc8"
      },
      "source": [
        "X_dev_cc[0][0] # first sentence of first clinical case"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.20988 , -0.90066 ,  0.15487 , ..., -0.47857 , -0.10083 ,\n",
              "        -0.47624 ],\n",
              "       [ 1.      ,  1.      ,  1.      , ...,  1.      ,  1.      ,\n",
              "         1.      ],\n",
              "       [ 0.27202 ,  0.15571 ,  0.015991, ..., -0.38315 ,  0.24421 ,\n",
              "         0.087124],\n",
              "       ...,\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNJ9nkU9QzYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DZEq3O_oQz23"
      },
      "source": [
        "##### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NbF2Td-OQz28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "43234191-5690-4f42-ac0e-b769dbd86511"
      },
      "source": [
        "num_tags = df_data_train2['Tag'].nunique()\n",
        "\n",
        "# Model architecture\n",
        "\n",
        "#inputs = keras.Input(shape=(None, None, 3)) # if it was an RGB image --> None states that the dimension can vary\n",
        "input = Input(shape = X_train2[0].shape)\n",
        "\n",
        "# max_len defines the number of words that with be considered as input to the model (can be considered as a window)\n",
        "# Hyperparameter tuning?\n",
        "# 2D tensor with shape: (batch_size, input_length). --> <tf.Tensor 'input_5:0' shape=(None, 75) dtype=float32>\n",
        "\n",
        "# 1. Embedding layer - already done \n",
        "\n",
        "# 2. Bidirectional layer\n",
        "model = Bidirectional(layer = LSTM(units = 50, return_sequences=True, recurrent_dropout=0.1))(input)\n",
        "# output shape: (None, 75, 100) --> output_dim is 100 because its concatenating \n",
        "# the outputs of the forward LSTM and backward LSTM instead of combining them\n",
        "\n",
        "# 3. TimeDistributed layer\n",
        "model = TimeDistributed(Dense(50, activation=\"relu\"))(model) \n",
        "# combines the 100 outputs of the time stamps specified by index (75) into 50 values using ReLU function. \n",
        "\n",
        "# 4. CRF layer (the classifier)\n",
        "crf = CRF(num_tags+1)  \n",
        "out = crf(model)  # output\n",
        "\n",
        "# instantiate the model; out must be the final layer\n",
        "model = Model(input, out) \n",
        "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 75, 300)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 75, 100)           140400    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 75, 50)            5050      \n",
            "_________________________________________________________________\n",
            "crf_1 (CRF)                  (None, 75, 7)             420       \n",
            "=================================================================\n",
            "Total params: 145,870\n",
            "Trainable params: 145,870\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4FtyDCGQz2-",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath = 'models_CRF_LSTM',\n",
        "                       verbose = 0,\n",
        "                       mode = 'auto',\n",
        "                       save_best_only = True,\n",
        "                       monitor='val_loss') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cQhnUN4xQz3B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "63341c3c-a289-45e1-d86f-fbc9caf96d85"
      },
      "source": [
        "random.seed(0)\n",
        "history = model.fit(X_train2, np.array(y_train2), batch_size = batch_size,\n",
        "                    epochs = epochs, validation_split = 0.1, callbacks = [checkpointer], )\n",
        "# checkpointer is saving the model at the end of each epoch\n",
        "\n",
        "history.history.keys() \n",
        "# history.history is a dictionary that contains per-epoch timeseries of metrics values "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33957 samples, validate on 3773 samples\n",
            "Epoch 1/8\n",
            "33957/33957 [==============================] - 92s 3ms/step - loss: 0.0461 - crf_viterbi_accuracy: 0.9915 - val_loss: 0.0183 - val_crf_viterbi_accuracy: 0.9950\n",
            "Epoch 2/8\n",
            "33957/33957 [==============================] - 91s 3ms/step - loss: 0.0123 - crf_viterbi_accuracy: 0.9953 - val_loss: 0.0107 - val_crf_viterbi_accuracy: 0.9932\n",
            "Epoch 3/8\n",
            "33957/33957 [==============================] - 92s 3ms/step - loss: 0.0022 - crf_viterbi_accuracy: 0.9961 - val_loss: -0.0017 - val_crf_viterbi_accuracy: 0.9961\n",
            "Epoch 4/8\n",
            "33957/33957 [==============================] - 90s 3ms/step - loss: -0.0063 - crf_viterbi_accuracy: 0.9965 - val_loss: -0.0091 - val_crf_viterbi_accuracy: 0.9963\n",
            "Epoch 5/8\n",
            "33957/33957 [==============================] - 90s 3ms/step - loss: -0.0142 - crf_viterbi_accuracy: 0.9968 - val_loss: -0.0163 - val_crf_viterbi_accuracy: 0.9961\n",
            "Epoch 6/8\n",
            "33957/33957 [==============================] - 91s 3ms/step - loss: -0.0217 - crf_viterbi_accuracy: 0.9970 - val_loss: -0.0243 - val_crf_viterbi_accuracy: 0.9967\n",
            "Epoch 7/8\n",
            "33957/33957 [==============================] - 91s 3ms/step - loss: -0.0292 - crf_viterbi_accuracy: 0.9972 - val_loss: -0.0311 - val_crf_viterbi_accuracy: 0.9965\n",
            "Epoch 8/8\n",
            "33957/33957 [==============================] - 90s 3ms/step - loss: -0.0366 - crf_viterbi_accuracy: 0.9975 - val_loss: -0.0385 - val_crf_viterbi_accuracy: 0.9966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_crf_viterbi_accuracy', 'loss', 'crf_viterbi_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vSOdaru0GZ3A"
      },
      "source": [
        "##### **Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mPxArHA5GZ3B",
        "colab": {}
      },
      "source": [
        "# labels used in the classification report\n",
        "labels = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c9bmJf6_GZ3D"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JXsO_4MkGZ3D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "c8cd6f6f-70aa-42d2-fe16-4f68ca5cbaef"
      },
      "source": [
        "# Evaluation over train set\n",
        "y_pred = model.predict(X_valid)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_valid_true = np.argmax(y_valid, -1)\n",
        "\n",
        "# Convert the index to tag\n",
        "y_pred = [[idx2tag[i] for i in row] for row in y_pred]\n",
        "y_valid_true = [[idx2tag[i] for i in row] for row in y_valid_true]\n",
        "\n",
        "print(\"F1-score is : {:.1%}\".format(flat_f1_score(y_valid_true, y_pred, average = 'micro', labels = labels)))\n",
        "\n",
        "report = flat_classification_report(y_true = y_valid_true, y_pred = y_pred, labels = labels)\n",
        "\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score is : 76.9%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-MOR       0.84      0.77      0.80       627\n",
            "       I-MOR       0.74      0.67      0.71      1024\n",
            "       E-MOR       0.74      0.68      0.71       628\n",
            "       S-MOR       0.93      0.87      0.90       653\n",
            "       V-MOR       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.80      0.74      0.77      2933\n",
            "   macro avg       0.65      0.60      0.62      2933\n",
            "weighted avg       0.80      0.74      0.77      2933\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k48APebRozaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words(tokens_sent,label_sent, true_label_sent, start_char_pos):\n",
        "  new_tok, new_lab, true_lab, new_start_pos = [], [], [], []\n",
        "\n",
        "  for tokens, labels, true_labels, start_chars in zip(tokens_sent, label_sent, true_label_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, true_lab_aux, new_start_pos_aux = [], [], [], []\n",
        "    for token, label, true_label, start_char_i in zip(tokens, labels,true_labels,start_chars):\n",
        "      if token != \"PAD\":\n",
        "        new_tok_aux.append(token)\n",
        "        new_lab_aux.append(label)\n",
        "        true_lab_aux.append(true_label)\n",
        "        new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    true_lab.append(true_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, true_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AAITmZofGZ3H"
      },
      "source": [
        "**Development set**\n",
        "\n",
        "The predictions over the development set are obtained by clinical case. Therefore, loop over all clinical cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WvWtCQ-5GZ3H"
      },
      "source": [
        "**Global metrics over the development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXkklLMdpyaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b3fd2d09-9c59-4b58-ed2b-923f465d726f"
      },
      "source": [
        "X_dev[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.20988 , -0.90066 ,  0.15487 , ..., -0.47857 , -0.10083 ,\n",
              "        -0.47624 ],\n",
              "       [ 1.      ,  1.      ,  1.      , ...,  1.      ,  1.      ,\n",
              "         1.      ],\n",
              "       [ 0.27202 ,  0.15571 ,  0.015991, ..., -0.38315 ,  0.24421 ,\n",
              "         0.087124],\n",
              "       ...,\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
              "         0.      ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aYzyfi50GZ3H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "6a5aea73-4187-4959-a155-7f70c8a0c676"
      },
      "source": [
        "# Evaluation\n",
        "y_pred_dev = model.predict(X_dev)\n",
        "y_pred_dev = np.argmax(y_pred_dev, axis=-1)\n",
        "y_dev_true = np.argmax(y_dev, -1)\n",
        "\n",
        "# Convert the index to tag\n",
        "y_pred_dev = [[idx2tag[i] for i in row] for row in y_pred_dev]\n",
        "y_dev_true = [[idx2tag[i] for i in row] for row in y_dev_true]\n",
        "\n",
        "print(\"F1-score is : {:.1%}\".format(flat_f1_score(y_dev_true, y_pred_dev, average = 'micro', labels = labels)))\n",
        "\n",
        "report_dev_glob = flat_classification_report(y_true = y_dev_true, y_pred = y_pred_dev, labels = labels)\n",
        "print(report_dev_glob)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score is : 75.3%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-MOR       0.82      0.75      0.79      1616\n",
            "       I-MOR       0.81      0.59      0.68      2546\n",
            "       E-MOR       0.72      0.65      0.69      1613\n",
            "       S-MOR       0.92      0.85      0.88      1667\n",
            "       V-MOR       0.00      0.00      0.00        18\n",
            "\n",
            "   micro avg       0.82      0.70      0.75      7460\n",
            "   macro avg       0.65      0.57      0.61      7460\n",
            "weighted avg       0.82      0.70      0.75      7460\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2Dj_l4euo7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "995dd437-ce0a-4882-c824-12c0fdf35fd1"
      },
      "source": [
        "print(token_dev_cc[0][0])\n",
        "print(start_char_dev_cc[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Anamnesis' '\\n' 'Varón' 'de' '74' 'años' ',' 'exfumador' 'desde' 'hace'\n",
            " '15' 'años' ',' 'con' 'único' 'antecedente' 'de' 'hipertensión' ','\n",
            " 'dislipemia' 'y' 'apendicectomizado' ';' 'se' 'diagnostica' 'en' 'marzo'\n",
            " 'de' '2013' 'de' 'carcinoma' 'de' 'células' 'transicionales' 'de'\n",
            " 'vejiga' 'E-IV' '(' 'pulmonares' 'y' 'óseas' ')' '.' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD']\n",
            "[  0   9  10  16  19  22  26  28  38  44  49  52  56  58  62  68  80  83\n",
            "  95  97 108 110 127 129 132 144 147 153 156 161 164 174 177 185 200 203\n",
            " 210 215 216 227 229 234 235  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Owoj74INpvB",
        "colab_type": "text"
      },
      "source": [
        "**Predictions over the development set by clinical case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7An_YPtNnfw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, true_labels_cc, new_start_pos_cc = [], [], [], []\n",
        "new_tokens_all, new_labels_all, true_labels_all, new_start_pos_all = [], [], [], []\n",
        "\n",
        "label_indices_sent_cc = []\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(X_dev_cc)):\n",
        "  y_pred_dev = model.predict(X_dev_cc[cc])\n",
        "  y_pred_dev = np.argmax(y_pred_dev, axis=-1)\n",
        "  y_dev_true = np.argmax(y_dev_cc[cc], -1)\n",
        "\n",
        "  # Convert the index to tag\n",
        "  y_pred_dev = [[idx2tag[i] for i in row] for row in y_pred_dev]\n",
        "  y_dev_true = [[idx2tag[i] for i in row] for row in y_dev_true]\n",
        "\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = [], [], [], []\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = tokens_to_words(token_dev_cc[cc], \n",
        "                                        y_pred_dev, y_dev_true, start_char_dev_cc[cc])\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  true_labels_cc.append(true_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  true_labels_all.extend(true_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w9ABQU0ydu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "4a6487b5-c96d-4c5a-a48b-3ed730736aa0"
      },
      "source": [
        "report_dev_glob = flat_classification_report(y_true = true_labels_all, y_pred = new_labels_all, labels = labels)\n",
        "print(report_dev_glob)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-MOR       0.83      0.75      0.79      1631\n",
            "       I-MOR       0.81      0.59      0.68      2565\n",
            "       E-MOR       0.73      0.65      0.69      1621\n",
            "       S-MOR       0.92      0.85      0.88      1666\n",
            "       V-MOR       0.00      0.00      0.00        18\n",
            "\n",
            "   micro avg       0.82      0.70      0.75      7501\n",
            "   macro avg       0.66      0.57      0.61      7501\n",
            "weighted avg       0.82      0.70      0.75      7501\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dkizQFT1vXst"
      },
      "source": [
        "##### **Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NIobu_9gvXsu",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(actual, predicted):\n",
        "    #classes       = np.unique(np.concatenate((actual,predicted)))\n",
        "    classes = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR', 'O']\n",
        "    confusion_mtx = np.empty((len(classes),len(classes)),dtype=np.int)\n",
        "    for i,a in enumerate(classes):\n",
        "        for j,p in enumerate(classes):\n",
        "            value = sum([sum([np.where((actual[sent][word]==a)*(predicted[sent][word]==p))[0].shape[0] \n",
        "                              for word in range(len(actual[sent]))]) for sent in range(len(actual))])\n",
        "            #confusion_mtx[i,j] = sum([np.where((actual[sent]==a)*(predicted[sent]==p))[0].shape[0] for sent in range(len(actual))])\n",
        "            confusion_mtx[i,j] = value\n",
        "    return confusion_mtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ezj-lpn4vXsw",
        "colab": {}
      },
      "source": [
        "# code to extract METRICS FOR ENTITY AND NON-ENTITY from the confusion matrix\n",
        "\n",
        "def metrics_from_cm(cm,labels):\n",
        "  TP = [[v for v in value[0:len(labels)]] for value in cm[0:len(labels)]]\n",
        "  print(TP)\n",
        "  TP = np.array(TP)\n",
        "  TP = sum(sum(TP))\n",
        "\n",
        "  FN = [value[-1] for value in cm[0:len(labels)]] # last column is O\n",
        "  print(FN)\n",
        "  FN = np.array(FN) \n",
        "  FN = sum(FN)\n",
        "\n",
        "  FP = cm[len(labels)][0:len(labels)] # last column is O\n",
        "  print(FP)\n",
        "  FP = sum(FP)\n",
        "\n",
        "  TN = cm[len(labels)][len(labels)] # last column is O\n",
        "  print(TN)\n",
        "\n",
        "  return TP, FN, FP, TN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uLoybs7xvXsy"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JOeSddAOvXsy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0a1f7035-0203-408f-8497-1427be4fa4b5"
      },
      "source": [
        "# not done by clinical case\n",
        "actual    = np.array(y_valid_true)\n",
        "predicted = np.array(y_pred)\n",
        "confusion_matrix_train = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(confusion_matrix_train, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>481</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>5</td>\n",
              "      <td>688</td>\n",
              "      <td>39</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>424</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>25</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>567</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>64</td>\n",
              "      <td>191</td>\n",
              "      <td>92</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>86940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR      O\n",
              "B-MOR    481      9      1     22      0    114\n",
              "I-MOR      5    688     39      4      0    288\n",
              "E-MOR      1     36    424     14      0    153\n",
              "S-MOR     25      3     15    567      0     43\n",
              "V-MOR      0      0      1      0      0      0\n",
              "O         64    191     92      5      0  86940"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fNRdc0H8vXs4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "56343b70-cf11-4fb2-b213-fcfd3a7d629c"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(confusion_matrix_train,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[481, 9, 1, 22, 0], [5, 688, 39, 4, 0], [1, 36, 424, 14, 0], [25, 3, 15, 567, 0], [0, 0, 1, 0, 0]]\n",
            "[114, 288, 153, 43, 0]\n",
            "[ 64 191  92   5   0]\n",
            "86940\n",
            "2335\n",
            "598\n",
            "352\n",
            "86940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XmSOm6gtvXs6"
      },
      "source": [
        "**Development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSs5wfKzvXs6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2f786c71-319c-4b9b-db2d-92bccc6dacc7"
      },
      "source": [
        "# Example of confusion matrix over first clinical case\n",
        "actual    = np.array(true_labels_cc[0])\n",
        "predicted = np.array(new_labels_cc[0])\n",
        "cm = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(cm, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df\n",
        "\n",
        "# columns: Predicted \n",
        "# rows: True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>518</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR    O\n",
              "B-MOR      6      0      0      0      0    2\n",
              "I-MOR      1      7      2      0      0   10\n",
              "E-MOR      0      0      5      0      0    3\n",
              "S-MOR      0      0      0      3      0    1\n",
              "V-MOR      0      0      0      0      0    0\n",
              "O          0      0      0      0      0  518"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YAYOGw4MvXs8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "14526166-4a08-4ea2-9f26-e2d340c5e17b"
      },
      "source": [
        "# confusion matrix computed over all the clinical cases \n",
        "\n",
        "actual    = np.array(true_labels_all)\n",
        "predicted = np.array(new_labels_all)\n",
        "cm = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(cm, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>1228</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>28</td>\n",
              "      <td>1518</td>\n",
              "      <td>141</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>2</td>\n",
              "      <td>82</td>\n",
              "      <td>1061</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>1422</td>\n",
              "      <td>0</td>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>171</td>\n",
              "      <td>251</td>\n",
              "      <td>222</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>215145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR       O\n",
              "B-MOR   1228     24      2     75      0     302\n",
              "I-MOR     28   1518    141      7      0     871\n",
              "E-MOR      2     82   1061     25      0     451\n",
              "S-MOR     57      1     29   1422      0     157\n",
              "V-MOR      2      6      3      3      0       4\n",
              "O        171    251    222     18      0  215145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G3HMMuJcvXs_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d09bf95e-44d3-42be-c334-97e5f57cebf5"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(cm,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[974, 23, 2, 63, 0], [18, 1421, 107, 2, 0], [1, 63, 857, 20, 0], [45, 5, 32, 1175, 0], [1, 0, 2, 0, 0]]\n",
            "[206, 624, 329, 110, 0]\n",
            "[103 239 145  56   0]\n",
            "175356\n",
            "4811\n",
            "1269\n",
            "543\n",
            "175356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aBbepVp9vXtB"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SV_9xgkaEUjv"
      },
      "source": [
        "#### **Final BiLSTM-CRF approach 2 model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So-cd9ei5Q56",
        "colab_type": "text"
      },
      "source": [
        "##### **Feature preparation** \n",
        "\n",
        "**Complete dataset (combines train and development datasets)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZevqSKkm5QE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting unique words and labels from data \n",
        "words = list(df_data_complete['Word'].unique())\n",
        "if (preprocess==True):\n",
        "  words = [preprocess_word(w) for w in words]\n",
        "  words = np.unique(words)\n",
        "  \n",
        "tags = list(df_data_complete['Tag'].unique())\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "#word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "#word_to_index[\"UNK\"] = 1\n",
        "#word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "# label is key and value is index.\n",
        "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
        "tag_to_index[\"PAD\"] = 0\n",
        "\n",
        "#idx2word = {i: w for w, i in word_to_index.items()}\n",
        "idx2tag = {i: w for w, i in tag_to_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWp7QcLy6hnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cda040c0-c3eb-4ff6-95fa-7f56fbbaf4ca"
      },
      "source": [
        "# Converting each sentence into list of vectors from list of tokens\n",
        "#X_train = [[model_word2vec[w[0]] for w in s] for s in sentences_train]\n",
        "X_train = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in sentences_complete]\n",
        "\n",
        "#X_train = [[model_word2vec.wv.most_similar_cosmul(w[0]) for w in s] for s in sentences_train]\n",
        "\n",
        "print(\"Vectorized sequence:\")\n",
        "print(type(X_train[0][0][0]))\n",
        "print(\"Sequence length of the first sentence: %d\" %len(X_train[0]))\n",
        "\n",
        "# Padding each sequence to have same length  of each word\n",
        "X_train = pad_sequences(maxlen = max_len, sequences = X_train, padding = \"post\", truncating=\"post\",  dtype= np.float32)\n",
        "\n",
        "print(\"\\nPadded sequence: \")\n",
        "print(type(X_train[0][0][0]))\n",
        "print(\"Sequence length of the first sentence: %d\" %len(X_train[0]))\n",
        "\n",
        "#Convert label to index\n",
        "y_train = [[tag_to_index[w[2]] for w in s] for s in sentences_complete]\n",
        "\n",
        "# padding\n",
        "y_train = pad_sequences(maxlen = max_len, sequences = y_train, padding = \"post\", truncating=\"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "num_tags = df_data_complete['Tag'].nunique()\n",
        "\n",
        "# One hot encoded labels\n",
        "y_train = [to_categorical(i, num_classes = num_tags + 1) for i in y_train]\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorized sequence:\n",
            "<class 'numpy.float32'>\n",
            "Sequence length of the first sentence: 24\n",
            "\n",
            "Padded sequence: \n",
            "<class 'numpy.float32'>\n",
            "Sequence length of the first sentence: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc3Nf32476ps",
        "colab_type": "text"
      },
      "source": [
        "##### **Feature preparation**\n",
        "\n",
        "**Test Set**\n",
        "\n",
        "There are over 5000 clinical cases in the test dataset. There is not enough RAM memory to store the word embedding of the test dataset. That is why, the word embedding is computed over 5 subsets of clinical cases separately. At each iteration, the predictions over a subset are obtained and stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjzSlTSO_S0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3bf90f0b-fda2-4f8f-8f9d-7f1c98a50470"
      },
      "source": [
        "sentences_test_by_cc[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Paciente', 'PROPN', 0),\n",
              " ('mujer', 'NOUN', 9),\n",
              " (',', 'PUNCT', 14),\n",
              " ('75', 'NUM', 16),\n",
              " ('años', 'NOUN', 19),\n",
              " ('consulta', 'VERB', 24),\n",
              " ('el', 'DET', 33),\n",
              " ('4-6-2003', 'NOUN', 36),\n",
              " (',', 'PUNCT', 44),\n",
              " ('refiriendo', 'VERB', 46),\n",
              " ('como', 'SCONJ', 57),\n",
              " ('antecedentes', 'NOUN', 62),\n",
              " ('personales', 'ADJ', 75),\n",
              " (':', 'PUNCT', 85),\n",
              " ('Alergia', 'PROPN', 87),\n",
              " ('a', 'ADP', 95),\n",
              " ('salicilatos', 'NOUN', 97),\n",
              " ('.', 'PUNCT', 108)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUhpTKx4eHE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_cc=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifWzircfRolj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_test_by_cc_subset = sentences_test_by_cc.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyy41u74R0cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_test_by_cc_subset = sentences_test_by_cc_subset[:1000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[1000:2000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[2000:3000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[3000:4000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[4000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRIiK_FfR6UM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3558f66c-4b0f-48ec-9621-e84ff00fd00a"
      },
      "source": [
        "len(sentences_test_by_cc_subset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf1SlAm8fcoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e8c698d8-733a-4dc3-c71b-69cc09cbd7b5"
      },
      "source": [
        "print(sentences_test_by_cc_subset[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Paciente', 'PROPN', 0), ('de', 'ADP', 9), ('89', 'NUM', 12), ('años', 'NOUN', 15), ('con', 'ADP', 20), ('clínica', 'NOUN', 24), ('de', 'ADP', 32), ('abdomen', 'NOUN', 35), ('agudo', 'ADJ', 43), ('.', 'PUNCT', 48)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymqAJGGIRoB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST SET (done by clinical case) BY SUBSETS\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_test_cc = []\n",
        "start_char_test_cc = []\n",
        "token_test_cc = []\n",
        "for cc in sentences_test_by_cc_subset:\n",
        "  x_i = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in cc]\n",
        "  start_i = [[w[2] for w in s] for s in cc]\n",
        "  token_i = [[w[0] for w in s] for s in cc]\n",
        "\n",
        "  # Padding each sequence to have same length  of each word\n",
        "  X_test_cc.append(pad_sequences(maxlen = max_len, sequences = x_i, padding = \"post\", truncating=\"post\", dtype= np.float32))\n",
        "  start_char_test_cc.append(pad_sequences(maxlen=max_len, sequences = start_i, value=-1, padding=\"post\", truncating=\"post\"))\n",
        "  token_test_cc.append(pad_sequences(maxlen=max_len, sequences = token_i, value=\"PAD\", padding=\"post\", truncating=\"post\",dtype= object))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksICZHtsK5M7",
        "colab_type": "text"
      },
      "source": [
        "##### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emumx9Ji7LbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f78597ba-b83b-4094-c553-b6ed3f6e0da9"
      },
      "source": [
        "df_data_complete"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Index</th>\n",
              "      <th>Sentence_Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>start</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ANAMNESIS</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n</td>\n",
              "      <td>SPACE</td>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mujer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>16</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>NUM</td>\n",
              "      <td>19</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862674</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>exitus</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5646</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862675</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5653</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862676</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>la</td>\n",
              "      <td>DET</td>\n",
              "      <td>5656</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862677</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>paciente</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5659</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862678</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>5667</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>862679 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        File_Index  Sentence_Index       Word    POS  start Tag\n",
              "0                1               1  ANAMNESIS   NOUN      0   O\n",
              "1                1               1         \\n  SPACE      9   O\n",
              "2                1               1      Mujer   NOUN     10   O\n",
              "3                1               1         de    ADP     16   O\n",
              "4                1               1         67    NUM     19   O\n",
              "...            ...             ...        ...    ...    ...  ..\n",
              "862674        1001           37730     exitus   NOUN   5646   O\n",
              "862675        1001           37730         de    ADP   5653   O\n",
              "862676        1001           37730         la    DET   5656   O\n",
              "862677        1001           37730   paciente   NOUN   5659   O\n",
              "862678        1001           37730          .  PUNCT   5667   O\n",
              "\n",
              "[862679 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g3Fw9SxvEUj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "43234191-5690-4f42-ac0e-b769dbd86511"
      },
      "source": [
        "num_tags = df_data_complete['Tag'].nunique()\n",
        "\n",
        "# Model architecture\n",
        "\n",
        "#inputs = keras.Input(shape=(None, None, 3)) # if it was an RGB image --> None states that the dimension can vary\n",
        "input = Input(shape = X_train[0].shape)\n",
        "\n",
        "# max_len defines the number of words that with be considered as input to the model (can be considered as a window)\n",
        "# Hyperparameter tuning?\n",
        "# 2D tensor with shape: (batch_size, input_length). --> <tf.Tensor 'input_5:0' shape=(None, 75) dtype=float32>\n",
        "\n",
        "# 1. Embedding layer - already done \n",
        "\n",
        "# 2. Bidirectional layer\n",
        "model = Bidirectional(layer = LSTM(units = 50, return_sequences=True, recurrent_dropout=0.1))(input)\n",
        "# output shape: (None, 75, 100) --> output_dim is 100 because its concatenating \n",
        "# the outputs of the forward LSTM and backward LSTM instead of combining them\n",
        "\n",
        "# 3. TimeDistributed layer\n",
        "model = TimeDistributed(Dense(50, activation=\"relu\"))(model) \n",
        "# combines the 100 outputs of the time stamps specified by index (75) into 50 values using ReLU function. \n",
        "\n",
        "# 4. CRF layer (the classifier)\n",
        "crf = CRF(num_tags+1)  \n",
        "out = crf(model)  # output\n",
        "\n",
        "# instantiate the model; out must be the final layer\n",
        "model = Model(input, out) \n",
        "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 75, 300)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 75, 100)           140400    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 75, 50)            5050      \n",
            "_________________________________________________________________\n",
            "crf_1 (CRF)                  (None, 75, 7)             420       \n",
            "=================================================================\n",
            "Total params: 145,870\n",
            "Trainable params: 145,870\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hL1KoXAtEUj3",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath = 'models_CRF_LSTM',\n",
        "                       verbose = 0,\n",
        "                       mode = 'auto',\n",
        "                       save_best_only = True,\n",
        "                       monitor='val_loss') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7KCGFDtDEUj6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "63341c3c-a289-45e1-d86f-fbc9caf96d85"
      },
      "source": [
        "random.seed(0)\n",
        "history = model.fit(X_train, np.array(y_train), batch_size = batch_size,\n",
        "                    epochs = epochs, validation_split = 0.1, callbacks = [checkpointer], )\n",
        "# checkpointer is saving the model at the end of each epoch\n",
        "\n",
        "history.history.keys() \n",
        "# history.history is a dictionary that contains per-epoch timeseries of metrics values "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 33957 samples, validate on 3773 samples\n",
            "Epoch 1/8\n",
            "33957/33957 [==============================] - 92s 3ms/step - loss: 0.0461 - crf_viterbi_accuracy: 0.9915 - val_loss: 0.0183 - val_crf_viterbi_accuracy: 0.9950\n",
            "Epoch 2/8\n",
            "33957/33957 [==============================] - 91s 3ms/step - loss: 0.0123 - crf_viterbi_accuracy: 0.9953 - val_loss: 0.0107 - val_crf_viterbi_accuracy: 0.9932\n",
            "Epoch 3/8\n",
            "33957/33957 [==============================] - 92s 3ms/step - loss: 0.0022 - crf_viterbi_accuracy: 0.9961 - val_loss: -0.0017 - val_crf_viterbi_accuracy: 0.9961\n",
            "Epoch 4/8\n",
            "33957/33957 [==============================] - 90s 3ms/step - loss: -0.0063 - crf_viterbi_accuracy: 0.9965 - val_loss: -0.0091 - val_crf_viterbi_accuracy: 0.9963\n",
            "Epoch 5/8\n",
            "33957/33957 [==============================] - 90s 3ms/step - loss: -0.0142 - crf_viterbi_accuracy: 0.9968 - val_loss: -0.0163 - val_crf_viterbi_accuracy: 0.9961\n",
            "Epoch 6/8\n",
            "33957/33957 [==============================] - 91s 3ms/step - loss: -0.0217 - crf_viterbi_accuracy: 0.9970 - val_loss: -0.0243 - val_crf_viterbi_accuracy: 0.9967\n",
            "Epoch 7/8\n",
            "33957/33957 [==============================] - 91s 3ms/step - loss: -0.0292 - crf_viterbi_accuracy: 0.9972 - val_loss: -0.0311 - val_crf_viterbi_accuracy: 0.9965\n",
            "Epoch 8/8\n",
            "33957/33957 [==============================] - 90s 3ms/step - loss: -0.0366 - crf_viterbi_accuracy: 0.9975 - val_loss: -0.0385 - val_crf_viterbi_accuracy: 0.9966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_crf_viterbi_accuracy', 'loss', 'crf_viterbi_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KncMRgVpK-LG",
        "colab_type": "text"
      },
      "source": [
        "##### **Test set predictions done by Subsets of 1000 clinical cases**\n",
        "\n",
        "This code must be executed for every subset of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2llhPnwNLEWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words_test(tokens_sent,label_sent, start_char_pos):\n",
        "  new_tok, new_lab, new_start_pos = [], [], []\n",
        "\n",
        "  for tokens, labels, start_chars in zip(tokens_sent, label_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, new_start_pos_aux = [], [], []\n",
        "    for token, label, start_char_i in zip(tokens, labels,start_chars):\n",
        "      if token != \"PAD\":\n",
        "        new_tok_aux.append(token)\n",
        "        new_lab_aux.append(label)\n",
        "        new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOEBcupKLOXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, true_labels_cc, new_start_pos_cc = [], [], [], []\n",
        "new_tokens_all, new_labels_all, true_labels_all, new_start_pos_all = [], [], [], []\n",
        "\n",
        "label_indices_sent_cc = []\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(X_test_cc)):\n",
        "  y_pred_test = model.predict(X_test_cc[cc])\n",
        "  y_pred_test = np.argmax(y_pred_test, axis=-1)\n",
        "\n",
        "  # Convert the index to tag\n",
        "  y_pred_test = [[idx2tag[i] for i in row] for row in y_pred_test]\n",
        "\n",
        "  new_tokens, new_labels, new_start_pos = [], [], []\n",
        "  new_tokens, new_labels, new_start_pos = tokens_to_words_test(token_test_cc[cc], \n",
        "                                        y_pred_test, start_char_test_cc[cc])\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLeV62BALT8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(path+'results_BILSTM_ap2/predictions/subset5/new_tokens_cc', 'wb') as file: \n",
        "  pkl.dump(new_tokens_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_BILSTM_ap2/predictions/subset5/new_labels_cc', 'wb') as file: \n",
        "  pkl.dump(new_labels_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_BILSTM_ap2/predictions/subset5/new_start_pos_cc', 'wb') as file: \n",
        "  pkl.dump(new_start_pos_cc, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ogsIWxlCEUj9"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fccwrzlzW7UA"
      },
      "source": [
        "### **Approach 3: BI-LSTM-CRF with pretrained word embedding and character embedding**\n",
        "\n",
        "https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/\n",
        "\n",
        "https://github.com/SuphanutN/Thai-NER-BiLSTMCRF-WordCharEmbedding/blob/master/Thai_NER_WordCharacterEmbedding_Train.ipynb\n",
        "\n",
        "https://medium.com/@NagisaZ/thai-named-entity-recognition-with-bilstm-crf-using-word-character-embedding-keras-6834717d4fdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gmO36sZqW7UF"
      },
      "source": [
        "#### **Preprocessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnb5oH9GXT5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "518b12de-665a-4384-8729-814324e8213c"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "scielo_wiki_model = KeyedVectors.load_word2vec_format(path+'Scielo+Wiki_skipgram_cased.vec') # this is in txt format"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoB4E1t3Qaz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
        "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t4OfmmCSW7UG",
        "colab": {}
      },
      "source": [
        "max_len = 75 # already declared above\n",
        "max_len_char = 10 # this is the sequence length established"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zi-tSEiSW7UQ",
        "colab": {}
      },
      "source": [
        "def word_to_chars(sentences, max_len, max_len_char, char_to_idx):\n",
        "  X_char = []\n",
        "  for sentence in sentences:\n",
        "    sent_seq = []\n",
        "    for i in range(max_len): # max sequence length: 75\n",
        "      word_seq = []\n",
        "      for j in range(max_len_char): # max word length 10\n",
        "        try:\n",
        "          word_seq.append(char_to_idx.get(sentence[i][0][j],1)) # if the character does not appear in the dictionary --> 1: [\"UNK\"]\n",
        "        except:\n",
        "          word_seq.append(char_to_idx.get(\"PAD\"))\n",
        "      sent_seq.append(word_seq)\n",
        "    X_char.append(np.array(sent_seq))\n",
        "  return X_char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdF0SuP7YjwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modif_get_vector(model,word):\n",
        "  try:\n",
        "    vector = model.get_vector(word)\n",
        "  except:\n",
        "    vector = np.ones(300,dtype = 'float32') # 300 is the size of the embedding \n",
        "  return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9__V3eI7YwKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_sentences(sentences): \n",
        "  sentences2 = []\n",
        "  for sent in sentences:\n",
        "    if len(sent) > 2.5*max_len:\n",
        "      sentences2.append(sent[:max_len])\n",
        "      sentences2.append(sent[max_len:2*max_len])\n",
        "      sentences2.append(sent[2*max_len:])\n",
        "\n",
        "    elif len(sent) > 1.5*max_len:\n",
        "      sentences2.append(sent[:max_len])\n",
        "      sentences2.append(sent[max_len:])\n",
        "\n",
        "    else:\n",
        "      sentences2.append(sent)\n",
        "  return sentences2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxYDbUUWRbkj",
        "colab_type": "text"
      },
      "source": [
        "#### **Train and evaluate the BiLSTM-CRF approach 3 model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4fxeZFiW7UK"
      },
      "source": [
        "##### **Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJHEAhZuYKOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting unique words and labels from data \n",
        "words = list(df_data_train2['Word'].unique())\n",
        "if (preprocess==True):\n",
        "  words = [preprocess_word(w) for w in words]\n",
        "  words = np.unique(words)\n",
        "  \n",
        "tags = list(df_data_train2['Tag'].unique())\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "#word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "#word_to_index[\"UNK\"] = 1\n",
        "#word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "# label is key and value is index.\n",
        "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
        "tag_to_index[\"PAD\"] = 0\n",
        "\n",
        "#idx2word = {i: w for w, i in word_to_index.items()}\n",
        "idx2tag = {i: w for w, i in tag_to_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P-9WufNYzpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentences_train = split_sentences(sentences_train)\n",
        "#sentences_dev = split_sentences(sentences_dev)\n",
        "\n",
        "#sentences_dev_by_cc = []\n",
        "#for cc in sentences_dev_by_cc:\n",
        "#  sentences_dev_by_cc.append(split_sentences(cc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anYYhnun4qJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5a69d887-05d4-4332-8788-9f6017133322"
      },
      "source": [
        "chars = set([w_i for w in scielo_wiki_model.vocab.keys() for w_i in w])\n",
        "n_chars = len(chars)\n",
        "print(\"Number of characters in the dictionary: %d\" %n_chars)\n",
        "print(\"\\nUnique characters: %s\" %chars)\n",
        "\n",
        "char_to_idx = {c: i + 2 for i, c in enumerate(chars)}\n",
        "char_to_idx[\"UNK\"] = 1\n",
        "char_to_idx[\"PAD\"] = 0\n",
        "print(\"\\nDictionary of characters: %s\" %char_to_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters in the dictionary: 1953\n",
            "\n",
            "Unique characters: {'植', 'ƒ', 'Č', 'ϒ', '印', '男', '狗', 'خ', '遵', '游', '断', 'ɬ', '限', '商', '造', '泉', '振', 'さ', '居', '爆', 'ồ', '県', '雪', 'ド', '束', '覇', 'Ê', '沈', '惠', '伊', 'г', '>', '々', '张', '腐', 'じ', '録', '何', '月', '海', 'ˆ', 'ケ', '孙', '虑', '万', 'ſ', '卜', '麗', '阿', '寺', '澤', 'v', 'Σ', 'ф', '美', '禽', '血', '牛', '站', '離', '楊', '各', 'ὗ', '演', 'ζ', '安', 'ា', '背', '.', '爱', '具', '盧', '引', '潤', 'â', '感', 'न', 'ろ', 'r', '世', '话', '微', 'ἑ', 'Ἰ', '親', 'ッ', '波', '很', '实', 'ά', '叉', '京', 'び', 'ṣ', '閭', '夏', '吴', '重', 'ἱ', 'ゃ', '綺', '率', '状', '先', 'ὺ', 'آ', '史', '字', 'ミ', '短', 'は', 'க', '味', '糸', '忠', 'せ', 'Á', 'ἃ', '珍', '却', 'ナ', '秦', '島', '御', '评', '今', '论', '祖', '倫', 'ज', '共', 'ご', 'て', '伽', '郎', '石', 'ś', '葉', '忍', '愛', '種', '木', 'ά', '霉', '真', '间', '荷', '會', '契', '机', 'ح', 'ὄ', '卦', 'ヨ', '針', '流', 'ψ', '变', '助', '异', '私', 'வ', 'מ', '鹿', 'ᾶ', '沖', 'ו', '试', 'ʒ', '組', '等', '料', '奈', 'っ', 'お', '牙', '莊', '蔵', 'ń', '轮', '激', '较', 'ظ', 'Υ', '張', '种', '雜', 'ח', '香', 't', 'Ð', '谷', 'ι', '意', '卍', '得', 'ἡ', '恵', '醉', 'Ə', '便', '詩', '尊', '探', '兵', '宗', '臨', 'ὦ', '池', '號', '随', '当', '达', '韓', '关', '市', 'ŷ', 'φ', 'Є', '样', '属', '策', 'σ', '吕', '伯', 'ˈ', 'η', '千', '伏', '森', 'ℕ', '昭', 'ĭ', '汇', '師', '台', 'ẻ', 'C', 'ɲ', 'ì', 'ர', '綠', '阳', '研', '甲', '横', 'R', '芸', 'ṇ', '順', '可', '条', '君', 'ן', '积', '笑', '义', '多', 'ᾧ', '昌', '知', '标', 'け', '观', 'g', '则', '甫', 'è', 'ת', '瀬', '同', '燕', '物', '地', '船', 'デ', '庭', '入', '剣', '桃', '风', '衛', 'ứ', '城', '趙', '橋', '׳', 'Ö', 'あ', 'よ', '郭', '茂', '民', '画', 'ə', '雨', '均', '程', 'プ', '春', 'ょ', 'ὅ', '房', '财', '合', 'ὲ', 'Α', '紅', '贫', '語', '化', '拉', '劳', '掛', '散', '廣', 'द', '騎', 'ம', '致', '円', '林', '员', '·', '陶', '验', 'Q', '宋', 'ḥ', 'り', '贸', 'ี', '坐', '药', '放', '教', '惑', 'ć', '保', '禁', 'ţ', 'Z', '斎', 'Ó', '說', '蒲', '鯉', '志', 'ὸ', 'ϑ', '銀', 'Δ', '油', 'þ', '貞', '局', '考', 'ゆ', 'ό', 'а', 'в', 'タ', 'ñ', '认', '鮨', '陵', '政', 'Ü', 'ط', 'ț', '仙', '九', '哀', '潮', '帯', '風', 'ο', '炎', 'ェ', '敬', '包', '邪', 'ạ', '完', '式', 'Ḥ', '店', 'え', 'Κ', '梦', '卵', 'V', '学', 'ǎ', '迷', 'ò', '河', '火', '牧', 'ῦ', '奴', '賀', 'த', 'í', '独', '位', '寿', '近', '指', '長', '究', '现', '庵', '邦', '然', '枝', 'ý', 'أ', 'χ', '菊', '鑑', '實', '新', 'Ά', '少', '以', '第', '慕', 'х', '语', '秋', '改', '桂', 'ế', '投', 'স', '礼', '欧', 'ấ', '英', 'ℜ', 'ẏ', '响', 'д', 'ô', '其', '儒', 'ǐ', 'ρ', '女', '飛', '班', '煙', 'ĉ', '音', 'ʔ', 'س', 'ả', '菜', 'ʎ', 'ᜅ', '徳', '智', '翁', '侯', '本', '廷', '数', 'ą', '济', '采', 'ク', '士', '変', 'ネ', '悟', '特', 'ད', 'Е', '例', 'z', '2', '务', '片', 'ヤ', '境', 'П', '票', '魚', '椿', '四', 'ϰ', 'ז', 'ラ', 'د', '漢', 'و', 'へ', 'Ț', 'і', '央', '平', 'ǘ', '冰', '题', 'ÿ', '爾', 'ॐ', '砲', '懿', '十', 'ស', '治', '座', '即', '麴', '臣', '前', '棍', '调', '转', '頭', '西', '章', 'М', '原', 'ɑ', '霜', '慎', 'ン', '事', '吾', 'Β', 'ἲ', 'λ', '魔', '肉', '蓝', '一', '綿', \"'\", '伸', 'ざ', '南', 'ӧ', '由', '单', '育', 'Ã', '靈', 'Ô', 'ノ', '姿', '些', '統', '后', 'Œ', '東', '個', '桑', '湘', '逃', '妃', '打', '抱', '狐', '起', '蔡', '升', 'च', '鳥', '書', '細', '孔', '淑', 'B', '企', 'Θ', '母', '玲', '专', '羽', '桜', '燈', 'Ψ', '渡', '紫', 'Ø', '楽', '若', '絶', '話', '扁', 'う', '最', '説', 'Ä', '也', '大', '豚', 'भ', '哲', 'チ', '電', '謝', '已', 'Ł', '无', '作', '预', '反', '奇', '未', 'ŭ', 'А', '漫', '著', 'ч', '供', '货', 'ω', '宫', 'ह', '他', '追', 'ς', '차', 'Τ', '二', '武', 'у', '凡', '永', '好', 'ὁ', '幸', '展', '下', '卑', 'з', '涼', '土', '葵', 'ň', '上', 'ة', 'क', '雲', 'ἶ', '黃', '傅', 'ή', '広', 'ῥ', '軍', '辞', 'ἀ', '青', '描', '红', '宮', '代', '義', '常', '焙', '芪', 'べ', '李', '资', 'p', 'ङ', '百', 'ப', '螂', 'ね', '雄', 'ί', '夜', '家', 'ض', '份', '度', '聞', 'Đ', '守', '序', '戦', '法', '权', 'ŋ', 'Ŷ', '賓', '步', '隋', '曲', 'ę', 'Ь', 'Χ', 'إ', '藝', '交', '全', 'ầ', 'ῆ', '乱', 'Ţ', 'ἠ', 'Ù', 'Я', '弟', 'Ρ', 'פ', 'ب', 'ג', 'к', '衰', '付', '洲', 'Λ', '約', 'F', 'X', 'Φ', '方', 'く', '残', '賢', '你', '历', '์', '芋', 'ɯ', '蠕', 'Ν', '退', '館', '尾', '精', 'ḍ', '号', '快', '建', 'に', '鲜', 'Π', '任', '收', '強', '为', 'ē', 'ﬂ', 'ﬁ', '迅', '族', 'র', '始', '松', 'ひ', '联', '夕', '株', '纪', 'ἴ', '日', '치', 'Б', '蓮', '拓', '基', 'ɨ', 'ἤ', '浪', 'q', '問', '之', 'ه', 'ν', 'ḫ', 'n', 'げ', '談', '3', 'е', '的', 'ス', 'غ', '狂', '破', 'З', '射', '盟', '铁', '和', '單', '幕', '晉', '乳', '象', 'µ', '利', 'น', '織', 'ᜃ', '溪', '估', '所', '都', 'я', 'प', '回', '丶', '泡', 'Ч', '对', 'ǔ', '早', 'し', '益', 'ы', 'Y', '喜', 'ɸ', '府', 'έ', 'ᜇ', 'ἰ', '葛', 'e', '型', '虹', 'ǽ', '略', 'イ', '死', 'Д', '博', '烈', '想', '4', '關', '效', '年', '相', 'ὑ', 'ẵ', 'ø', '拔', 'の', '解', '问', '心', 'S', '从', '州', 'ῷ', '管', '緑', '禅', '使', '姓', 'ü', '佐', '界', '犬', 'º', '田', 'ż', 'ष', '刀', '省', '秀', '結', '9', 'ポ', '適', '七', 'た', '加', 'ल', '据', '示', 'য', '齊', '争', '类', '議', '成', '取', '丁', 'γ', '盾', 'キ', 'м', '連', '古', '跤', '点', '勝', '袁', '白', '个', '津', 'Ζ', '霊', 'ξ', '亡', '墨', '了', 'ה', '寒', '竞', '友', '唐', '行', 'ם', '遷', '康', '周', 'オ', '韩', 'ण', 'コ', '皮', '車', 'Ō', 'ὀ', 'ɣ', 'ώ', '久', 'ẋ', 'Å', 'Γ', 'Ξ', '强', 'セ', '操', '桓', '命', 'ː', '首', 'ὖ', '擒', 'ロ', 'ম', '止', '粉', 'ớ', 'ů', '鉄', 'र', 'ὔ', 'с', '导', '矛', '阶', 'ὐ', '<', '累', '是', 'ú', '邑', '勒', 'ō', '読', '比', 'İ', '坂', '景', '柔', '丸', 'ج', 'ウ', '勲', '間', '虎', '當', '陽', '它', '妖', 'o', '毒', '殺', 'ἄ', '屋', 'Ẹ', '岩', '面', '内', '妙', '质', '良', '裕', '佳', '析', '無', 'O', 'ば', 'Š', 'ɪ', '离', '有', '该', 'ك', '撃', 'み', '純', '獄', '就', 'ἐ', '刺', 'ύ', 'ל', 'i', 'ė', '列', 'い', '雅', '慶', '許', 'δ', '通', '活', '架', '系', 'ă', '赤', 'Ϊ', '慈', 'm', '球', '开', '次', '正', '哥', '口', 'ḗ', '伴', '節', 'ץ', '需', '華', '连', 'ễ', '朝', '人', '翼', '律', '进', '业', 'С', '玄', '到', 'נ', 'Г', 'ç', '版', '曹', '记', '術', '东', 'b', '子', '笠', '魏', 'め', '堂', '协', '能', 'ר', '逸', '砖', 'β', '宣', '食', 'ز', '沙', '室', 'म', 'ğ', '配', '增', '輝', '兰', '危', 'ハ', '厳', '右', '渾', 'श', 'ャ', '宇', 'ף', '视', '星', '失', '普', 'ی', '越', '金', 'フ', 'ö', 'ុ', '立', '宾', 'ァ', 'ي', 'ὃ', '峰', 'ゴ', '且', '夫', '技', 'ס', '恐', 'ş', '典', '朱', 'Ǻ', '貨', 'ǒ', 'ἓ', '黄', '向', '显', '去', '量', '巴', 'Ż', '形', 'x', '别', '藤', 'ず', 'だ', '汗', '述', 'ゼ', '德', '岳', '固', '1', 'Ï', '/', 'ǚ', '令', 'л', '乙', '匈', 'ʻ', '戀', '侍', '傳', '井', '江', 'ぼ', '段', '潜', '场', '天', '股', '聖', '気', 'ὕ', 'Ω', '猫', 'š', 'ぶ', '极', 'É', 'ᾳ', '兴', 'ὶ', '獣', '焼', '8', '吐', '希', '边', '再', '樂', 'Ñ', '自', '論', 'E', '此', '嘉', 'ἷ', '終', '信', '巨', 'ὼ', 'ϛ', '园', '瞳', 'é', '竜', '沮', '我', 'Ç', '耀', '元', '廟', '用', '烟', '6', '曜', 'œ', 'ル', ',', '替', 'バ', 'έ', 'Ф', '孫', '介', '酒', 'ŵ', 'Ⓐ', 'ő', 'ᜋ', 'ύ', '級', '経', '看', '奥', '云', '洋', '辺', '案', '讨', '支', '賊', '延', '灭', '杜', '胡', '发', 'ィ', 'ベ', 'ར', 'c', '八', '至', '传', '怪', '仁', '应', 'ˇ', '衫', 'ệ', '川', '鋼', '經', '件', '杂', '于', '溝', 'ὴ', 'ἦ', '勃', '陳', 'ᜐ', '差', 'Ā', '場', '龙', '存', '亭', '稳', '錦', '但', '醸', '分', 'Ĥ', '洵', 'ɔ', '设', 'き', 'ï', '区', '觀', '要', 'ஸ', 'ź', '稽', '公', 'カ', '茶', 's', '馬', 'ἂ', '走', '两', '声', 'ἅ', '虫', '竹', 'ʿ', '项', 'ف', '统', '持', 'æ', '運', '在', '計', 'צ', '免', '幻', '遊', '名', 'Ş', '映', '深', '洱', 'つ', '着', '福', '司', '貴', '域', '受', '更', 'k', '战', '宏', '産', '皇', '伝', 'ิ', 'ش', '结', 'م', '服', 'ʼ', '接', 'ό', '譚', 'य', '絵', 'и', 'ä', 'ʁ', '孝', '初', 'Η', '伦', '劉', 'ш', 'õ', '棒', 'ɛ', '滅', 'Ú', '摔', '童', '曼', 'θ', '浦', '浅', 'स', '豊', '苏', '鐵', '米', '计', '框', 'リ', '制', '卷', '藥', 'ボ', '产', '湯', '贷', '鬼', '琴', '况', '这', '冥', '修', '果', 'ó', 'К', '拳', '身', '秘', '主', 'マ', '學', 'も', 'ص', '假', 'Н', '端', '衣', '明', '蒙', '編', 'ぬ', 'ῖ', '彦', 'Þ', '币', '劇', '異', '黒', 'ῶ', '豆', '丘', '贡', '緒', '摩', 'レ', 'Î', '崎', 'ム', '羌', '陰', '儀', '花', 'ɢ', '经', '顔', '訓', '窓', '桐', '或', 'Ś', 'こ', 'な', 'व', 'Ò', '陈', 'G', '敏', '扩', 'ı', '言', '零', '陀', '功', '5', '恩', '落', 'w', '戒', 'ま', '与', 'ర', '逆', '藏', '業', '巻', 'ũ', '郁', '衡', 'ℝ', '斗', '体', 'ん', 'ṃ', 'ἥ', '賦', '太', '马', '括', '才', '斯', '余', 'ー', '病', 'ϖ', 'ù', '骨', 'が', '里', '為', 'ग', 'M', '雷', 'У', '乃', '援', 'ř', 'そ', 'ὰ', '北', '干', '切', '町', 'р', '洞', '如', '那', '達', 'れ', '提', 'ト', '้', 'ž', 'ʝ', '尽', 'В', 'ら', '梨', 'T', 'û', 'ή', 'ώ', 'ħ', '创', '佛', '洪', '旅', 'K', 'f', '動', 'ち', '科', '村', '彼', '光', 'ุ', 'Ж', '見', '影', '樹', 'ʃ', '冒', '沢', '债', 'π', '萬', 'ð', '派', 'Ì', 'ł', '因', '吉', '弘', '遠', '蹤', '半', 'ἢ', '菌', '呂', '部', '空', 'ع', '父', '术', '腿', '银', '释', 'ἁ', '农', '亜', '锺', '消', 'п', '征', '查', '非', '來', '突', '兼', '優', '威', '凝', '复', 'ש', '献', '左', '舞', '帝', '掌', '书', '革', '降', '索', '思', '品', 'О', '季', 'ق', '臼', 'ぐ', 'ε', '算', '時', 'サ', '_', '许', '樽', 'わ', '渠', 'ק', '虚', '督', 'И', '老', '蘇', 'ड', 'î', 'N', 'ब', '构', 'כ', 'ἔ', '僕', '校', '弱', '出', 'ϵ', 'Ο', '後', '仮', '沫', 'ℓ', '野', '김', '紀', '麹', '態', '進', '烏', '山', 'រ', 'ё', 'U', 'ᜈ', '被', '澄', '批', '气', '憶', '决', '湖', '返', '夢', '手', '价', 'अ', '道', '会', '情', '叶', '超', 'υ', '华', '悠', '螳', 'Л', '羅', '尤', 'る', '工', 'È', '期', '仲', '銃', 'ブ', '生', 'j', '中', 'ϐ', '勢', 'ध', 'т', '临', '带', 'y', '斛', 'ℏ', '者', '目', 'J', 'о', '錢', '挑', '穴', 'డ', '根', 'ě', '開', '车', 'ى', '外', '氏', '敦', '模', '浮', 'ั', '垂', 'ন', 'ῇ', '党', 'ན', '洗', '领', '力', '機', 'τ', 'か', 'h', 'ș', '清', '角', '獅', '欠', 'Í', '隠', '而', '様', '素', 'ª', 'I', 'á', '文', '縣', 'й', '厥', '小', 'ד', '過', 'Р', 'ど', 'ṭ', '布', 'ី', '極', 'ь', 'A', '篇', '夷', 'ī', '理', '毛', '甘', 'ا', 'で', '孟', 'を', '祭', 'ע', '国', 'ā', '推', 'א', 'Ž', 'ĕ', '际', 'ū', '广', 'ت', '詠', 'н', '值', '呼', 'L', '长', '農', '対', '够', '戸', 'ぎ', 'u', '色', '耳', 'ί', 'μ', 'Ω', 'Ι', 'd', '姫', 'ϕ', 'a', '直', '休', '念', '顯', 'त', '盛', '三', '恋', 'ச', '草', '必', '拿', 'Э', '来', '沼', '楚', '歌', '施', '熊', '悪', '鶴', '鎖', 'ء', '易', ':', '暗', '塚', '蔣', 'と', 'ë', '倉', 'Æ', '性', 'à', 'ã', '霍', '神', '富', '別', 'ر', '们', 'ɾ', '院', '昆', 'ê', '六', '时', 'י', 'å', '嘛', '麻', '寧', 'l', '器', 'ĩ', '宝', 'đ', '露', '茅', '判', 'Μ', '氣', '及', 'ن', 'ĥ', '7', 'ŏ', '师', '杰', 'H', '高', '膨', 'α', '集', '內', '将', '社', '0', '屠', 'ぞ', '检', '園', '鮮', '筋', 'Ë', '馆', 'モ', '不', 'ل', 'À', 'ἕ', '议', '國', '困', 'ὥ', 'ᜀ', '鮓', 'κ', '并', '板', '路', '脈', '过', '树', '菇', 'б', 'ט', '玉', '隊', 'D', '杉', '头', 'ơ', 'ẽ', '岡', '容', 'や', '្', '冲', '試', 'Â', '刘', '定', '表', '水', '墓', '源', '王', '密', 'Ἀ', '龍', '弓', '爪', '说', 'す', '記', 'ゅ', '관', 'Ю', '级', '折', '報', 'ὡ', 'Ε', '鳳', 'ב', 'W', '跡', '五', '善', '梅', '弥', '丹', '戲', 'Õ', '魂', 'č', '่', 'ủ', '融', 'ϱ', '飯', 'P', '門', '祝', 'ᜆ', '団', '门', '望', '実', '汽', '満', '殿', '俊', '动'}\n",
            "\n",
            "Dictionary of characters: {'植': 2, 'ƒ': 3, 'Č': 4, 'ϒ': 5, '印': 6, '男': 7, '狗': 8, 'خ': 9, '遵': 10, '游': 11, '断': 12, 'ɬ': 13, '限': 14, '商': 15, '造': 16, '泉': 17, '振': 18, 'さ': 19, '居': 20, '爆': 21, 'ồ': 22, '県': 23, '雪': 24, 'ド': 25, '束': 26, '覇': 27, 'Ê': 28, '沈': 29, '惠': 30, '伊': 31, 'г': 32, '>': 33, '々': 34, '张': 35, '腐': 36, 'じ': 37, '録': 38, '何': 39, '月': 40, '海': 41, 'ˆ': 42, 'ケ': 43, '孙': 44, '虑': 45, '万': 46, 'ſ': 47, '卜': 48, '麗': 49, '阿': 50, '寺': 51, '澤': 52, 'v': 53, 'Σ': 54, 'ф': 55, '美': 56, '禽': 57, '血': 58, '牛': 59, '站': 60, '離': 61, '楊': 62, '各': 63, 'ὗ': 64, '演': 65, 'ζ': 66, '安': 67, 'ា': 68, '背': 69, '.': 70, '爱': 71, '具': 72, '盧': 73, '引': 74, '潤': 75, 'â': 76, '感': 77, 'न': 78, 'ろ': 79, 'r': 80, '世': 81, '话': 82, '微': 83, 'ἑ': 84, 'Ἰ': 85, '親': 86, 'ッ': 87, '波': 88, '很': 89, '实': 90, 'ά': 91, '叉': 92, '京': 93, 'び': 94, 'ṣ': 95, '閭': 96, '夏': 97, '吴': 98, '重': 99, 'ἱ': 100, 'ゃ': 101, '綺': 102, '率': 103, '状': 104, '先': 105, 'ὺ': 106, 'آ': 107, '史': 108, '字': 109, 'ミ': 110, '短': 111, 'は': 112, 'க': 113, '味': 114, '糸': 115, '忠': 116, 'せ': 117, 'Á': 118, 'ἃ': 119, '珍': 120, '却': 121, 'ナ': 122, '秦': 123, '島': 124, '御': 125, '评': 126, '今': 127, '论': 128, '祖': 129, '倫': 130, 'ज': 131, '共': 132, 'ご': 133, 'て': 134, '伽': 135, '郎': 136, '石': 137, 'ś': 138, '葉': 139, '忍': 140, '愛': 141, '種': 142, '木': 143, 'ά': 144, '霉': 145, '真': 146, '间': 147, '荷': 148, '會': 149, '契': 150, '机': 151, 'ح': 152, 'ὄ': 153, '卦': 154, 'ヨ': 155, '針': 156, '流': 157, 'ψ': 158, '变': 159, '助': 160, '异': 161, '私': 162, 'வ': 163, 'מ': 164, '鹿': 165, 'ᾶ': 166, '沖': 167, 'ו': 168, '试': 169, 'ʒ': 170, '組': 171, '等': 172, '料': 173, '奈': 174, 'っ': 175, 'お': 176, '牙': 177, '莊': 178, '蔵': 179, 'ń': 180, '轮': 181, '激': 182, '较': 183, 'ظ': 184, 'Υ': 185, '張': 186, '种': 187, '雜': 188, 'ח': 189, '香': 190, 't': 191, 'Ð': 192, '谷': 193, 'ι': 194, '意': 195, '卍': 196, '得': 197, 'ἡ': 198, '恵': 199, '醉': 200, 'Ə': 201, '便': 202, '詩': 203, '尊': 204, '探': 205, '兵': 206, '宗': 207, '臨': 208, 'ὦ': 209, '池': 210, '號': 211, '随': 212, '当': 213, '达': 214, '韓': 215, '关': 216, '市': 217, 'ŷ': 218, 'φ': 219, 'Є': 220, '样': 221, '属': 222, '策': 223, 'σ': 224, '吕': 225, '伯': 226, 'ˈ': 227, 'η': 228, '千': 229, '伏': 230, '森': 231, 'ℕ': 232, '昭': 233, 'ĭ': 234, '汇': 235, '師': 236, '台': 237, 'ẻ': 238, 'C': 239, 'ɲ': 240, 'ì': 241, 'ர': 242, '綠': 243, '阳': 244, '研': 245, '甲': 246, '横': 247, 'R': 248, '芸': 249, 'ṇ': 250, '順': 251, '可': 252, '条': 253, '君': 254, 'ן': 255, '积': 256, '笑': 257, '义': 258, '多': 259, 'ᾧ': 260, '昌': 261, '知': 262, '标': 263, 'け': 264, '观': 265, 'g': 266, '则': 267, '甫': 268, 'è': 269, 'ת': 270, '瀬': 271, '同': 272, '燕': 273, '物': 274, '地': 275, '船': 276, 'デ': 277, '庭': 278, '入': 279, '剣': 280, '桃': 281, '风': 282, '衛': 283, 'ứ': 284, '城': 285, '趙': 286, '橋': 287, '׳': 288, 'Ö': 289, 'あ': 290, 'よ': 291, '郭': 292, '茂': 293, '民': 294, '画': 295, 'ə': 296, '雨': 297, '均': 298, '程': 299, 'プ': 300, '春': 301, 'ょ': 302, 'ὅ': 303, '房': 304, '财': 305, '合': 306, 'ὲ': 307, 'Α': 308, '紅': 309, '贫': 310, '語': 311, '化': 312, '拉': 313, '劳': 314, '掛': 315, '散': 316, '廣': 317, 'द': 318, '騎': 319, 'ம': 320, '致': 321, '円': 322, '林': 323, '员': 324, '·': 325, '陶': 326, '验': 327, 'Q': 328, '宋': 329, 'ḥ': 330, 'り': 331, '贸': 332, 'ี': 333, '坐': 334, '药': 335, '放': 336, '教': 337, '惑': 338, 'ć': 339, '保': 340, '禁': 341, 'ţ': 342, 'Z': 343, '斎': 344, 'Ó': 345, '說': 346, '蒲': 347, '鯉': 348, '志': 349, 'ὸ': 350, 'ϑ': 351, '銀': 352, 'Δ': 353, '油': 354, 'þ': 355, '貞': 356, '局': 357, '考': 358, 'ゆ': 359, 'ό': 360, 'а': 361, 'в': 362, 'タ': 363, 'ñ': 364, '认': 365, '鮨': 366, '陵': 367, '政': 368, 'Ü': 369, 'ط': 370, 'ț': 371, '仙': 372, '九': 373, '哀': 374, '潮': 375, '帯': 376, '風': 377, 'ο': 378, '炎': 379, 'ェ': 380, '敬': 381, '包': 382, '邪': 383, 'ạ': 384, '完': 385, '式': 386, 'Ḥ': 387, '店': 388, 'え': 389, 'Κ': 390, '梦': 391, '卵': 392, 'V': 393, '学': 394, 'ǎ': 395, '迷': 396, 'ò': 397, '河': 398, '火': 399, '牧': 400, 'ῦ': 401, '奴': 402, '賀': 403, 'த': 404, 'í': 405, '独': 406, '位': 407, '寿': 408, '近': 409, '指': 410, '長': 411, '究': 412, '现': 413, '庵': 414, '邦': 415, '然': 416, '枝': 417, 'ý': 418, 'أ': 419, 'χ': 420, '菊': 421, '鑑': 422, '實': 423, '新': 424, 'Ά': 425, '少': 426, '以': 427, '第': 428, '慕': 429, 'х': 430, '语': 431, '秋': 432, '改': 433, '桂': 434, 'ế': 435, '投': 436, 'স': 437, '礼': 438, '欧': 439, 'ấ': 440, '英': 441, 'ℜ': 442, 'ẏ': 443, '响': 444, 'д': 445, 'ô': 446, '其': 447, '儒': 448, 'ǐ': 449, 'ρ': 450, '女': 451, '飛': 452, '班': 453, '煙': 454, 'ĉ': 455, '音': 456, 'ʔ': 457, 'س': 458, 'ả': 459, '菜': 460, 'ʎ': 461, 'ᜅ': 462, '徳': 463, '智': 464, '翁': 465, '侯': 466, '本': 467, '廷': 468, '数': 469, 'ą': 470, '济': 471, '采': 472, 'ク': 473, '士': 474, '変': 475, 'ネ': 476, '悟': 477, '特': 478, 'ད': 479, 'Е': 480, '例': 481, 'z': 482, '2': 483, '务': 484, '片': 485, 'ヤ': 486, '境': 487, 'П': 488, '票': 489, '魚': 490, '椿': 491, '四': 492, 'ϰ': 493, 'ז': 494, 'ラ': 495, 'د': 496, '漢': 497, 'و': 498, 'へ': 499, 'Ț': 500, 'і': 501, '央': 502, '平': 503, 'ǘ': 504, '冰': 505, '题': 506, 'ÿ': 507, '爾': 508, 'ॐ': 509, '砲': 510, '懿': 511, '十': 512, 'ស': 513, '治': 514, '座': 515, '即': 516, '麴': 517, '臣': 518, '前': 519, '棍': 520, '调': 521, '转': 522, '頭': 523, '西': 524, '章': 525, 'М': 526, '原': 527, 'ɑ': 528, '霜': 529, '慎': 530, 'ン': 531, '事': 532, '吾': 533, 'Β': 534, 'ἲ': 535, 'λ': 536, '魔': 537, '肉': 538, '蓝': 539, '一': 540, '綿': 541, \"'\": 542, '伸': 543, 'ざ': 544, '南': 545, 'ӧ': 546, '由': 547, '单': 548, '育': 549, 'Ã': 550, '靈': 551, 'Ô': 552, 'ノ': 553, '姿': 554, '些': 555, '統': 556, '后': 557, 'Œ': 558, '東': 559, '個': 560, '桑': 561, '湘': 562, '逃': 563, '妃': 564, '打': 565, '抱': 566, '狐': 567, '起': 568, '蔡': 569, '升': 570, 'च': 571, '鳥': 572, '書': 573, '細': 574, '孔': 575, '淑': 576, 'B': 577, '企': 578, 'Θ': 579, '母': 580, '玲': 581, '专': 582, '羽': 583, '桜': 584, '燈': 585, 'Ψ': 586, '渡': 587, '紫': 588, 'Ø': 589, '楽': 590, '若': 591, '絶': 592, '話': 593, '扁': 594, 'う': 595, '最': 596, '説': 597, 'Ä': 598, '也': 599, '大': 600, '豚': 601, 'भ': 602, '哲': 603, 'チ': 604, '電': 605, '謝': 606, '已': 607, 'Ł': 608, '无': 609, '作': 610, '预': 611, '反': 612, '奇': 613, '未': 614, 'ŭ': 615, 'А': 616, '漫': 617, '著': 618, 'ч': 619, '供': 620, '货': 621, 'ω': 622, '宫': 623, 'ह': 624, '他': 625, '追': 626, 'ς': 627, '차': 628, 'Τ': 629, '二': 630, '武': 631, 'у': 632, '凡': 633, '永': 634, '好': 635, 'ὁ': 636, '幸': 637, '展': 638, '下': 639, '卑': 640, 'з': 641, '涼': 642, '土': 643, '葵': 644, 'ň': 645, '上': 646, 'ة': 647, 'क': 648, '雲': 649, 'ἶ': 650, '黃': 651, '傅': 652, 'ή': 653, '広': 654, 'ῥ': 655, '軍': 656, '辞': 657, 'ἀ': 658, '青': 659, '描': 660, '红': 661, '宮': 662, '代': 663, '義': 664, '常': 665, '焙': 666, '芪': 667, 'べ': 668, '李': 669, '资': 670, 'p': 671, 'ङ': 672, '百': 673, 'ப': 674, '螂': 675, 'ね': 676, '雄': 677, 'ί': 678, '夜': 679, '家': 680, 'ض': 681, '份': 682, '度': 683, '聞': 684, 'Đ': 685, '守': 686, '序': 687, '戦': 688, '法': 689, '权': 690, 'ŋ': 691, 'Ŷ': 692, '賓': 693, '步': 694, '隋': 695, '曲': 696, 'ę': 697, 'Ь': 698, 'Χ': 699, 'إ': 700, '藝': 701, '交': 702, '全': 703, 'ầ': 704, 'ῆ': 705, '乱': 706, 'Ţ': 707, 'ἠ': 708, 'Ù': 709, 'Я': 710, '弟': 711, 'Ρ': 712, 'פ': 713, 'ب': 714, 'ג': 715, 'к': 716, '衰': 717, '付': 718, '洲': 719, 'Λ': 720, '約': 721, 'F': 722, 'X': 723, 'Φ': 724, '方': 725, 'く': 726, '残': 727, '賢': 728, '你': 729, '历': 730, '์': 731, '芋': 732, 'ɯ': 733, '蠕': 734, 'Ν': 735, '退': 736, '館': 737, '尾': 738, '精': 739, 'ḍ': 740, '号': 741, '快': 742, '建': 743, 'に': 744, '鲜': 745, 'Π': 746, '任': 747, '收': 748, '強': 749, '为': 750, 'ē': 751, 'ﬂ': 752, 'ﬁ': 753, '迅': 754, '族': 755, 'র': 756, '始': 757, '松': 758, 'ひ': 759, '联': 760, '夕': 761, '株': 762, '纪': 763, 'ἴ': 764, '日': 765, '치': 766, 'Б': 767, '蓮': 768, '拓': 769, '基': 770, 'ɨ': 771, 'ἤ': 772, '浪': 773, 'q': 774, '問': 775, '之': 776, 'ه': 777, 'ν': 778, 'ḫ': 779, 'n': 780, 'げ': 781, '談': 782, '3': 783, 'е': 784, '的': 785, 'ス': 786, 'غ': 787, '狂': 788, '破': 789, 'З': 790, '射': 791, '盟': 792, '铁': 793, '和': 794, '單': 795, '幕': 796, '晉': 797, '乳': 798, '象': 799, 'µ': 800, '利': 801, 'น': 802, '織': 803, 'ᜃ': 804, '溪': 805, '估': 806, '所': 807, '都': 808, 'я': 809, 'प': 810, '回': 811, '丶': 812, '泡': 813, 'Ч': 814, '对': 815, 'ǔ': 816, '早': 817, 'し': 818, '益': 819, 'ы': 820, 'Y': 821, '喜': 822, 'ɸ': 823, '府': 824, 'έ': 825, 'ᜇ': 826, 'ἰ': 827, '葛': 828, 'e': 829, '型': 830, '虹': 831, 'ǽ': 832, '略': 833, 'イ': 834, '死': 835, 'Д': 836, '博': 837, '烈': 838, '想': 839, '4': 840, '關': 841, '效': 842, '年': 843, '相': 844, 'ὑ': 845, 'ẵ': 846, 'ø': 847, '拔': 848, 'の': 849, '解': 850, '问': 851, '心': 852, 'S': 853, '从': 854, '州': 855, 'ῷ': 856, '管': 857, '緑': 858, '禅': 859, '使': 860, '姓': 861, 'ü': 862, '佐': 863, '界': 864, '犬': 865, 'º': 866, '田': 867, 'ż': 868, 'ष': 869, '刀': 870, '省': 871, '秀': 872, '結': 873, '9': 874, 'ポ': 875, '適': 876, '七': 877, 'た': 878, '加': 879, 'ल': 880, '据': 881, '示': 882, 'য': 883, '齊': 884, '争': 885, '类': 886, '議': 887, '成': 888, '取': 889, '丁': 890, 'γ': 891, '盾': 892, 'キ': 893, 'м': 894, '連': 895, '古': 896, '跤': 897, '点': 898, '勝': 899, '袁': 900, '白': 901, '个': 902, '津': 903, 'Ζ': 904, '霊': 905, 'ξ': 906, '亡': 907, '墨': 908, '了': 909, 'ה': 910, '寒': 911, '竞': 912, '友': 913, '唐': 914, '行': 915, 'ם': 916, '遷': 917, '康': 918, '周': 919, 'オ': 920, '韩': 921, 'ण': 922, 'コ': 923, '皮': 924, '車': 925, 'Ō': 926, 'ὀ': 927, 'ɣ': 928, 'ώ': 929, '久': 930, 'ẋ': 931, 'Å': 932, 'Γ': 933, 'Ξ': 934, '强': 935, 'セ': 936, '操': 937, '桓': 938, '命': 939, 'ː': 940, '首': 941, 'ὖ': 942, '擒': 943, 'ロ': 944, 'ম': 945, '止': 946, '粉': 947, 'ớ': 948, 'ů': 949, '鉄': 950, 'र': 951, 'ὔ': 952, 'с': 953, '导': 954, '矛': 955, '阶': 956, 'ὐ': 957, '<': 958, '累': 959, '是': 960, 'ú': 961, '邑': 962, '勒': 963, 'ō': 964, '読': 965, '比': 966, 'İ': 967, '坂': 968, '景': 969, '柔': 970, '丸': 971, 'ج': 972, 'ウ': 973, '勲': 974, '間': 975, '虎': 976, '當': 977, '陽': 978, '它': 979, '妖': 980, 'o': 981, '毒': 982, '殺': 983, 'ἄ': 984, '屋': 985, 'Ẹ': 986, '岩': 987, '面': 988, '内': 989, '妙': 990, '质': 991, '良': 992, '裕': 993, '佳': 994, '析': 995, '無': 996, 'O': 997, 'ば': 998, 'Š': 999, 'ɪ': 1000, '离': 1001, '有': 1002, '该': 1003, 'ك': 1004, '撃': 1005, 'み': 1006, '純': 1007, '獄': 1008, '就': 1009, 'ἐ': 1010, '刺': 1011, 'ύ': 1012, 'ל': 1013, 'i': 1014, 'ė': 1015, '列': 1016, 'い': 1017, '雅': 1018, '慶': 1019, '許': 1020, 'δ': 1021, '通': 1022, '活': 1023, '架': 1024, '系': 1025, 'ă': 1026, '赤': 1027, 'Ϊ': 1028, '慈': 1029, 'm': 1030, '球': 1031, '开': 1032, '次': 1033, '正': 1034, '哥': 1035, '口': 1036, 'ḗ': 1037, '伴': 1038, '節': 1039, 'ץ': 1040, '需': 1041, '華': 1042, '连': 1043, 'ễ': 1044, '朝': 1045, '人': 1046, '翼': 1047, '律': 1048, '进': 1049, '业': 1050, 'С': 1051, '玄': 1052, '到': 1053, 'נ': 1054, 'Г': 1055, 'ç': 1056, '版': 1057, '曹': 1058, '记': 1059, '術': 1060, '东': 1061, 'b': 1062, '子': 1063, '笠': 1064, '魏': 1065, 'め': 1066, '堂': 1067, '协': 1068, '能': 1069, 'ר': 1070, '逸': 1071, '砖': 1072, 'β': 1073, '宣': 1074, '食': 1075, 'ز': 1076, '沙': 1077, '室': 1078, 'म': 1079, 'ğ': 1080, '配': 1081, '增': 1082, '輝': 1083, '兰': 1084, '危': 1085, 'ハ': 1086, '厳': 1087, '右': 1088, '渾': 1089, 'श': 1090, 'ャ': 1091, '宇': 1092, 'ף': 1093, '视': 1094, '星': 1095, '失': 1096, '普': 1097, 'ی': 1098, '越': 1099, '金': 1100, 'フ': 1101, 'ö': 1102, 'ុ': 1103, '立': 1104, '宾': 1105, 'ァ': 1106, 'ي': 1107, 'ὃ': 1108, '峰': 1109, 'ゴ': 1110, '且': 1111, '夫': 1112, '技': 1113, 'ס': 1114, '恐': 1115, 'ş': 1116, '典': 1117, '朱': 1118, 'Ǻ': 1119, '貨': 1120, 'ǒ': 1121, 'ἓ': 1122, '黄': 1123, '向': 1124, '显': 1125, '去': 1126, '量': 1127, '巴': 1128, 'Ż': 1129, '形': 1130, 'x': 1131, '别': 1132, '藤': 1133, 'ず': 1134, 'だ': 1135, '汗': 1136, '述': 1137, 'ゼ': 1138, '德': 1139, '岳': 1140, '固': 1141, '1': 1142, 'Ï': 1143, '/': 1144, 'ǚ': 1145, '令': 1146, 'л': 1147, '乙': 1148, '匈': 1149, 'ʻ': 1150, '戀': 1151, '侍': 1152, '傳': 1153, '井': 1154, '江': 1155, 'ぼ': 1156, '段': 1157, '潜': 1158, '场': 1159, '天': 1160, '股': 1161, '聖': 1162, '気': 1163, 'ὕ': 1164, 'Ω': 1165, '猫': 1166, 'š': 1167, 'ぶ': 1168, '极': 1169, 'É': 1170, 'ᾳ': 1171, '兴': 1172, 'ὶ': 1173, '獣': 1174, '焼': 1175, '8': 1176, '吐': 1177, '希': 1178, '边': 1179, '再': 1180, '樂': 1181, 'Ñ': 1182, '自': 1183, '論': 1184, 'E': 1185, '此': 1186, '嘉': 1187, 'ἷ': 1188, '終': 1189, '信': 1190, '巨': 1191, 'ὼ': 1192, 'ϛ': 1193, '园': 1194, '瞳': 1195, 'é': 1196, '竜': 1197, '沮': 1198, '我': 1199, 'Ç': 1200, '耀': 1201, '元': 1202, '廟': 1203, '用': 1204, '烟': 1205, '6': 1206, '曜': 1207, 'œ': 1208, 'ル': 1209, ',': 1210, '替': 1211, 'バ': 1212, 'έ': 1213, 'Ф': 1214, '孫': 1215, '介': 1216, '酒': 1217, 'ŵ': 1218, 'Ⓐ': 1219, 'ő': 1220, 'ᜋ': 1221, 'ύ': 1222, '級': 1223, '経': 1224, '看': 1225, '奥': 1226, '云': 1227, '洋': 1228, '辺': 1229, '案': 1230, '讨': 1231, '支': 1232, '賊': 1233, '延': 1234, '灭': 1235, '杜': 1236, '胡': 1237, '发': 1238, 'ィ': 1239, 'ベ': 1240, 'ར': 1241, 'c': 1242, '八': 1243, '至': 1244, '传': 1245, '怪': 1246, '仁': 1247, '应': 1248, 'ˇ': 1249, '衫': 1250, 'ệ': 1251, '川': 1252, '鋼': 1253, '經': 1254, '件': 1255, '杂': 1256, '于': 1257, '溝': 1258, 'ὴ': 1259, 'ἦ': 1260, '勃': 1261, '陳': 1262, 'ᜐ': 1263, '差': 1264, 'Ā': 1265, '場': 1266, '龙': 1267, '存': 1268, '亭': 1269, '稳': 1270, '錦': 1271, '但': 1272, '醸': 1273, '分': 1274, 'Ĥ': 1275, '洵': 1276, 'ɔ': 1277, '设': 1278, 'き': 1279, 'ï': 1280, '区': 1281, '觀': 1282, '要': 1283, 'ஸ': 1284, 'ź': 1285, '稽': 1286, '公': 1287, 'カ': 1288, '茶': 1289, 's': 1290, '馬': 1291, 'ἂ': 1292, '走': 1293, '两': 1294, '声': 1295, 'ἅ': 1296, '虫': 1297, '竹': 1298, 'ʿ': 1299, '项': 1300, 'ف': 1301, '统': 1302, '持': 1303, 'æ': 1304, '運': 1305, '在': 1306, '計': 1307, 'צ': 1308, '免': 1309, '幻': 1310, '遊': 1311, '名': 1312, 'Ş': 1313, '映': 1314, '深': 1315, '洱': 1316, 'つ': 1317, '着': 1318, '福': 1319, '司': 1320, '貴': 1321, '域': 1322, '受': 1323, '更': 1324, 'k': 1325, '战': 1326, '宏': 1327, '産': 1328, '皇': 1329, '伝': 1330, 'ิ': 1331, 'ش': 1332, '结': 1333, 'م': 1334, '服': 1335, 'ʼ': 1336, '接': 1337, 'ό': 1338, '譚': 1339, 'य': 1340, '絵': 1341, 'и': 1342, 'ä': 1343, 'ʁ': 1344, '孝': 1345, '初': 1346, 'Η': 1347, '伦': 1348, '劉': 1349, 'ш': 1350, 'õ': 1351, '棒': 1352, 'ɛ': 1353, '滅': 1354, 'Ú': 1355, '摔': 1356, '童': 1357, '曼': 1358, 'θ': 1359, '浦': 1360, '浅': 1361, 'स': 1362, '豊': 1363, '苏': 1364, '鐵': 1365, '米': 1366, '计': 1367, '框': 1368, 'リ': 1369, '制': 1370, '卷': 1371, '藥': 1372, 'ボ': 1373, '产': 1374, '湯': 1375, '贷': 1376, '鬼': 1377, '琴': 1378, '况': 1379, '这': 1380, '冥': 1381, '修': 1382, '果': 1383, 'ó': 1384, 'К': 1385, '拳': 1386, '身': 1387, '秘': 1388, '主': 1389, 'マ': 1390, '學': 1391, 'も': 1392, 'ص': 1393, '假': 1394, 'Н': 1395, '端': 1396, '衣': 1397, '明': 1398, '蒙': 1399, '編': 1400, 'ぬ': 1401, 'ῖ': 1402, '彦': 1403, 'Þ': 1404, '币': 1405, '劇': 1406, '異': 1407, '黒': 1408, 'ῶ': 1409, '豆': 1410, '丘': 1411, '贡': 1412, '緒': 1413, '摩': 1414, 'レ': 1415, 'Î': 1416, '崎': 1417, 'ム': 1418, '羌': 1419, '陰': 1420, '儀': 1421, '花': 1422, 'ɢ': 1423, '经': 1424, '顔': 1425, '訓': 1426, '窓': 1427, '桐': 1428, '或': 1429, 'Ś': 1430, 'こ': 1431, 'な': 1432, 'व': 1433, 'Ò': 1434, '陈': 1435, 'G': 1436, '敏': 1437, '扩': 1438, 'ı': 1439, '言': 1440, '零': 1441, '陀': 1442, '功': 1443, '5': 1444, '恩': 1445, '落': 1446, 'w': 1447, '戒': 1448, 'ま': 1449, '与': 1450, 'ర': 1451, '逆': 1452, '藏': 1453, '業': 1454, '巻': 1455, 'ũ': 1456, '郁': 1457, '衡': 1458, 'ℝ': 1459, '斗': 1460, '体': 1461, 'ん': 1462, 'ṃ': 1463, 'ἥ': 1464, '賦': 1465, '太': 1466, '马': 1467, '括': 1468, '才': 1469, '斯': 1470, '余': 1471, 'ー': 1472, '病': 1473, 'ϖ': 1474, 'ù': 1475, '骨': 1476, 'が': 1477, '里': 1478, '為': 1479, 'ग': 1480, 'M': 1481, '雷': 1482, 'У': 1483, '乃': 1484, '援': 1485, 'ř': 1486, 'そ': 1487, 'ὰ': 1488, '北': 1489, '干': 1490, '切': 1491, '町': 1492, 'р': 1493, '洞': 1494, '如': 1495, '那': 1496, '達': 1497, 'れ': 1498, '提': 1499, 'ト': 1500, '้': 1501, 'ž': 1502, 'ʝ': 1503, '尽': 1504, 'В': 1505, 'ら': 1506, '梨': 1507, 'T': 1508, 'û': 1509, 'ή': 1510, 'ώ': 1511, 'ħ': 1512, '创': 1513, '佛': 1514, '洪': 1515, '旅': 1516, 'K': 1517, 'f': 1518, '動': 1519, 'ち': 1520, '科': 1521, '村': 1522, '彼': 1523, '光': 1524, 'ุ': 1525, 'Ж': 1526, '見': 1527, '影': 1528, '樹': 1529, 'ʃ': 1530, '冒': 1531, '沢': 1532, '债': 1533, 'π': 1534, '萬': 1535, 'ð': 1536, '派': 1537, 'Ì': 1538, 'ł': 1539, '因': 1540, '吉': 1541, '弘': 1542, '遠': 1543, '蹤': 1544, '半': 1545, 'ἢ': 1546, '菌': 1547, '呂': 1548, '部': 1549, '空': 1550, 'ع': 1551, '父': 1552, '术': 1553, '腿': 1554, '银': 1555, '释': 1556, 'ἁ': 1557, '农': 1558, '亜': 1559, '锺': 1560, '消': 1561, 'п': 1562, '征': 1563, '查': 1564, '非': 1565, '來': 1566, '突': 1567, '兼': 1568, '優': 1569, '威': 1570, '凝': 1571, '复': 1572, 'ש': 1573, '献': 1574, '左': 1575, '舞': 1576, '帝': 1577, '掌': 1578, '书': 1579, '革': 1580, '降': 1581, '索': 1582, '思': 1583, '品': 1584, 'О': 1585, '季': 1586, 'ق': 1587, '臼': 1588, 'ぐ': 1589, 'ε': 1590, '算': 1591, '時': 1592, 'サ': 1593, '_': 1594, '许': 1595, '樽': 1596, 'わ': 1597, '渠': 1598, 'ק': 1599, '虚': 1600, '督': 1601, 'И': 1602, '老': 1603, '蘇': 1604, 'ड': 1605, 'î': 1606, 'N': 1607, 'ब': 1608, '构': 1609, 'כ': 1610, 'ἔ': 1611, '僕': 1612, '校': 1613, '弱': 1614, '出': 1615, 'ϵ': 1616, 'Ο': 1617, '後': 1618, '仮': 1619, '沫': 1620, 'ℓ': 1621, '野': 1622, '김': 1623, '紀': 1624, '麹': 1625, '態': 1626, '進': 1627, '烏': 1628, '山': 1629, 'រ': 1630, 'ё': 1631, 'U': 1632, 'ᜈ': 1633, '被': 1634, '澄': 1635, '批': 1636, '气': 1637, '憶': 1638, '决': 1639, '湖': 1640, '返': 1641, '夢': 1642, '手': 1643, '价': 1644, 'अ': 1645, '道': 1646, '会': 1647, '情': 1648, '叶': 1649, '超': 1650, 'υ': 1651, '华': 1652, '悠': 1653, '螳': 1654, 'Л': 1655, '羅': 1656, '尤': 1657, 'る': 1658, '工': 1659, 'È': 1660, '期': 1661, '仲': 1662, '銃': 1663, 'ブ': 1664, '生': 1665, 'j': 1666, '中': 1667, 'ϐ': 1668, '勢': 1669, 'ध': 1670, 'т': 1671, '临': 1672, '带': 1673, 'y': 1674, '斛': 1675, 'ℏ': 1676, '者': 1677, '目': 1678, 'J': 1679, 'о': 1680, '錢': 1681, '挑': 1682, '穴': 1683, 'డ': 1684, '根': 1685, 'ě': 1686, '開': 1687, '车': 1688, 'ى': 1689, '外': 1690, '氏': 1691, '敦': 1692, '模': 1693, '浮': 1694, 'ั': 1695, '垂': 1696, 'ন': 1697, 'ῇ': 1698, '党': 1699, 'ན': 1700, '洗': 1701, '领': 1702, '力': 1703, '機': 1704, 'τ': 1705, 'か': 1706, 'h': 1707, 'ș': 1708, '清': 1709, '角': 1710, '獅': 1711, '欠': 1712, 'Í': 1713, '隠': 1714, '而': 1715, '様': 1716, '素': 1717, 'ª': 1718, 'I': 1719, 'á': 1720, '文': 1721, '縣': 1722, 'й': 1723, '厥': 1724, '小': 1725, 'ד': 1726, '過': 1727, 'Р': 1728, 'ど': 1729, 'ṭ': 1730, '布': 1731, 'ី': 1732, '極': 1733, 'ь': 1734, 'A': 1735, '篇': 1736, '夷': 1737, 'ī': 1738, '理': 1739, '毛': 1740, '甘': 1741, 'ا': 1742, 'で': 1743, '孟': 1744, 'を': 1745, '祭': 1746, 'ע': 1747, '国': 1748, 'ā': 1749, '推': 1750, 'א': 1751, 'Ž': 1752, 'ĕ': 1753, '际': 1754, 'ū': 1755, '广': 1756, 'ت': 1757, '詠': 1758, 'н': 1759, '值': 1760, '呼': 1761, 'L': 1762, '长': 1763, '農': 1764, '対': 1765, '够': 1766, '戸': 1767, 'ぎ': 1768, 'u': 1769, '色': 1770, '耳': 1771, 'ί': 1772, 'μ': 1773, 'Ω': 1774, 'Ι': 1775, 'd': 1776, '姫': 1777, 'ϕ': 1778, 'a': 1779, '直': 1780, '休': 1781, '念': 1782, '顯': 1783, 'त': 1784, '盛': 1785, '三': 1786, '恋': 1787, 'ச': 1788, '草': 1789, '必': 1790, '拿': 1791, 'Э': 1792, '来': 1793, '沼': 1794, '楚': 1795, '歌': 1796, '施': 1797, '熊': 1798, '悪': 1799, '鶴': 1800, '鎖': 1801, 'ء': 1802, '易': 1803, ':': 1804, '暗': 1805, '塚': 1806, '蔣': 1807, 'と': 1808, 'ë': 1809, '倉': 1810, 'Æ': 1811, '性': 1812, 'à': 1813, 'ã': 1814, '霍': 1815, '神': 1816, '富': 1817, '別': 1818, 'ر': 1819, '们': 1820, 'ɾ': 1821, '院': 1822, '昆': 1823, 'ê': 1824, '六': 1825, '时': 1826, 'י': 1827, 'å': 1828, '嘛': 1829, '麻': 1830, '寧': 1831, 'l': 1832, '器': 1833, 'ĩ': 1834, '宝': 1835, 'đ': 1836, '露': 1837, '茅': 1838, '判': 1839, 'Μ': 1840, '氣': 1841, '及': 1842, 'ن': 1843, 'ĥ': 1844, '7': 1845, 'ŏ': 1846, '师': 1847, '杰': 1848, 'H': 1849, '高': 1850, '膨': 1851, 'α': 1852, '集': 1853, '內': 1854, '将': 1855, '社': 1856, '0': 1857, '屠': 1858, 'ぞ': 1859, '检': 1860, '園': 1861, '鮮': 1862, '筋': 1863, 'Ë': 1864, '馆': 1865, 'モ': 1866, '不': 1867, 'ل': 1868, 'À': 1869, 'ἕ': 1870, '议': 1871, '國': 1872, '困': 1873, 'ὥ': 1874, 'ᜀ': 1875, '鮓': 1876, 'κ': 1877, '并': 1878, '板': 1879, '路': 1880, '脈': 1881, '过': 1882, '树': 1883, '菇': 1884, 'б': 1885, 'ט': 1886, '玉': 1887, '隊': 1888, 'D': 1889, '杉': 1890, '头': 1891, 'ơ': 1892, 'ẽ': 1893, '岡': 1894, '容': 1895, 'や': 1896, '្': 1897, '冲': 1898, '試': 1899, 'Â': 1900, '刘': 1901, '定': 1902, '表': 1903, '水': 1904, '墓': 1905, '源': 1906, '王': 1907, '密': 1908, 'Ἀ': 1909, '龍': 1910, '弓': 1911, '爪': 1912, '说': 1913, 'す': 1914, '記': 1915, 'ゅ': 1916, '관': 1917, 'Ю': 1918, '级': 1919, '折': 1920, '報': 1921, 'ὡ': 1922, 'Ε': 1923, '鳳': 1924, 'ב': 1925, 'W': 1926, '跡': 1927, '五': 1928, '善': 1929, '梅': 1930, '弥': 1931, '丹': 1932, '戲': 1933, 'Õ': 1934, '魂': 1935, 'č': 1936, '่': 1937, 'ủ': 1938, '融': 1939, 'ϱ': 1940, '飯': 1941, 'P': 1942, '門': 1943, '祝': 1944, 'ᜆ': 1945, '団': 1946, '门': 1947, '望': 1948, '実': 1949, '汽': 1950, '満': 1951, '殿': 1952, '俊': 1953, '动': 1954, 'UNK': 1, 'PAD': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Nj_DupXW7UO",
        "colab": {}
      },
      "source": [
        "num_tags = df_data_train2['Tag'].nunique()\n",
        "\n",
        "# Sentences into Words\n",
        "X_word_train = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in sentences_train]\n",
        "\n",
        "X_word_train = pad_sequences(maxlen = max_len, sequences = X_word_train, padding = \"post\", dtype= np.float32)\n",
        "\n",
        "\n",
        "# Words into Chars\n",
        "X_char_train = []\n",
        "for sentence in sentences_train:\n",
        "    sent_seq = []\n",
        "    for i in range(max_len): # max sequence length: 75\n",
        "        word_seq = []\n",
        "        for j in range(max_len_char): # max word length 10\n",
        "          try:\n",
        "              word_seq.append(char_to_idx.get(sentence[i][0][j],1))\n",
        "          except:\n",
        "              word_seq.append(char_to_idx.get(\"PAD\"))\n",
        "        sent_seq.append(word_seq)\n",
        "    X_char_train.append(np.array(sent_seq))\n",
        "\n",
        "# Tags\n",
        "y_train = [[tag_to_index[w[2]] for w in s] for s in sentences_train]\n",
        "y_train = pad_sequences(maxlen = max_len, sequences = y_train, padding = \"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "\n",
        "# One hot encoded labels\n",
        "y_train = [to_categorical(i, num_classes = num_tags + 1) for i in y_train]\n",
        "\n",
        "# Split train set into train and validation\n",
        "X_word_train2, X_word_valid, y_train2, y_valid = train_test_split(X_word_train, y_train, test_size=0.2, random_state=0)\n",
        "X_char_train2, X_char_valid, _, _ = train_test_split(X_char_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WrAwc2QwW7UQ"
      },
      "source": [
        "##### **Development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j2QcL_s8W7US",
        "colab": {}
      },
      "source": [
        "# DEVELOPMENT SET (without considering clinical cases independently)\n",
        "\n",
        "# Sentences into Words\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_word_dev = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in sentences_dev] \n",
        "X_word_dev = pad_sequences(maxlen = max_len, sequences = X_word_dev, padding = \"post\", dtype= np.float32)\n",
        "\n",
        "# if the word is not in the vocabulary, is set to 1, which is the label \"UNK\" (unknown)\n",
        "\n",
        "# Words into Chars\n",
        "X_char_dev = word_to_chars(sentences_dev, max_len, max_len_char, char_to_idx)\n",
        "  \n",
        "# Tags\n",
        "y_dev = [[tag_to_index[w[2]] for w in s] for s in sentences_dev]\n",
        "y_dev = pad_sequences(maxlen = max_len, sequences = y_dev, padding = \"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "# One hot encoded labels\n",
        "y_dev = [to_categorical(i, num_classes = num_tags + 1) for i in y_dev]\n",
        "\n",
        "# ------------\n",
        "\n",
        "# DEVELOPMENT SET (done by clinical case)\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_word_dev_cc = []\n",
        "X_char_dev_cc = []\n",
        "y_dev_cc = []\n",
        "start_char_dev_cc = []\n",
        "token_dev_cc = []\n",
        "\n",
        "for cc in sentences_dev_by_cc:\n",
        "  # Sentences into Words\n",
        "  x_i = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in cc]\n",
        "  y_i = [[tag_to_index[w[2]] for w in s] for s in cc] #Convert label to index\n",
        "\n",
        "  start_i = [[w[3] for w in s] for s in cc]\n",
        "  token_i = [[w[0] for w in s] for s in cc]\n",
        "\n",
        "  # Padding each sequence to have same length  of each word\n",
        "  X_word_dev_cc.append(pad_sequences(maxlen = max_len, sequences = x_i, padding = \"post\", dtype= np.float32))\n",
        "  y_dev_cc.append(pad_sequences(maxlen = max_len, sequences = y_i, padding = \"post\", value = tag_to_index[\"PAD\"]))\n",
        "\n",
        "  # Words into Chars\n",
        "  x_char_i = word_to_chars(cc, max_len, max_len_char, char_to_idx)\n",
        "  X_char_dev_cc.append(x_char_i)\n",
        "\n",
        "  start_char_dev_cc.append(pad_sequences(maxlen=max_len, sequences = start_i, value=-1, padding=\"post\", truncating=\"post\"))\n",
        "  token_dev_cc.append(pad_sequences(maxlen=max_len, sequences = token_i, value=\"PAD\", padding=\"post\", truncating=\"post\",dtype= object))\n",
        "\n",
        "# One hot encoded labels\n",
        "y_dev_cc = [to_categorical(i, num_classes = num_tags + 1) for i in y_dev_cc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNwYxarSW7Ua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b51605de-4c75-4f7c-ce72-f661d21c7fa1"
      },
      "source": [
        "len(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zPPzDwMcW7Uf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9734958a-9efa-4bb0-84d8-2bb568831ebc"
      },
      "source": [
        "print(\"First word in train set:\")\n",
        "print(sentences_train[0][0]) \n",
        "print(X_char_train[0][0])\n",
        "\n",
        "print(\"\\nFirst word in development set:\")\n",
        "print(sentences_dev[0][0]) \n",
        "print(X_char_dev[0][0])\n",
        "\n",
        "print(\"\\nFirst word in development set per clinical case:\")\n",
        "print(sentences_dev_by_cc[0][0][0]) \n",
        "print(X_char_dev_cc[0][0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in train set:\n",
            "('ANAMNESIS', 'NOUN', 'O')\n",
            "[ 87   4  87  96   4  23 108 109 108   0]\n",
            "\n",
            "First word in development set:\n",
            "('Anamnesis', 'NOUN', 'O')\n",
            "[ 87  61   8  53  61  30  68 122  68   0]\n",
            "\n",
            "First word in development set per clinical case:\n",
            "('Anamnesis', 'NOUN', 'O')\n",
            "[ 87  61   8  53  61  30  68 122  68   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnZEYvOVW7Uh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46562414-3aa7-44e7-b639-671b633ba6bb"
      },
      "source": [
        "print(tag_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'O': 1, 'B-MOR': 2, 'E-MOR': 3, 'S-MOR': 4, 'I-MOR': 5, 'V-MOR': 6, 'PAD': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d1-KebHvRSCL"
      },
      "source": [
        "##### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vkKBazYyRSCO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "a693bb56-61ce-4bd3-8496-380e145b6f24"
      },
      "source": [
        "num_tags = df_data_train2['Tag'].nunique()\n",
        "\n",
        "# Model architecture\n",
        "\n",
        "# Inputs: (for word and character embedding)\n",
        "#word_in = Input(shape=(max_len,))\n",
        "word_in = Input(shape = X_word_train2[0].shape)\n",
        "char_in = Input(shape=(max_len, max_len_char,))\n",
        "# max_len defines the number of words that with be considered as input to the model (can be considered as a window)\n",
        "# 2D tensor with shape: (batch_size, input_length). --> <tf.Tensor 'input_5:0' shape=(None, 75) dtype=float32>\n",
        "\n",
        "# 1. Embedding layer for words\n",
        "emb_word = word_in\n",
        "# output_shape: (batch_size, input_length, output_dim) --> (None, 75, 40)\n",
        "\n",
        "# 2. Embedding and Time distributed layers for characters # output_dim=32\n",
        "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=32, input_length=max_len_char, mask_zero=False))(char_in)\n",
        "\n",
        "# 3. Character LSTM to get word encodings by characters UNIDIRECTIONAL\n",
        "char_enc = TimeDistributed(LSTM(units=32, return_sequences=False, recurrent_dropout=0.5))(emb_char)\n",
        "\n",
        "\n",
        "# 4. Bidirecitonal LSTM\n",
        "# first, create feature map formed by word embeddings and character encodings\n",
        "features = concatenate([emb_word, char_enc])\n",
        "features = SpatialDropout1D(0.3)(features)\n",
        "\n",
        "bi_lstm = Bidirectional(LSTM(units=256, return_sequences=True, recurrent_dropout=0.6))(features)\n",
        "# output shape: (None, 75, 100) --> output_dim is 100 because its concatenating \n",
        "# the outputs of the forward LSTM and backward LSTM instead of combining them\n",
        "\n",
        "# 5. Time Distributed layer \n",
        "#out = TimeDistributed(Dense(num_tags + 1, activation=\"sigmoid\"))(bi_lstm)\n",
        "#out = TimeDistributed(Dense(50, activation=\"sigmoid\"))(bi_lstm)\n",
        "out = TimeDistributed(Dense(50, activation=\"relu\"))(bi_lstm)\n",
        "\n",
        "# combines the 100 outputs of the time stamps specified by index (75) into 50 values using sigmoid function. \n",
        "# try: activation=\"relu\"\n",
        "\n",
        "## 6. CRF layer (the classifier) # PREVIOUS APPROACH\n",
        "crf = CRF(num_tags+1)  \n",
        "out = crf(out)  # output\n",
        "\n",
        "# 6. Instantiate the model; out must be the final layer\n",
        "model = Model([word_in, char_in], out)\n",
        "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy]) # PREVIOUS APPROACH\n",
        "#model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy]) # PREVIOUS APPROACH\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 75, 10)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 75, 10, 32)   62560       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 75, 300)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 75, 32)       8320        time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 75, 332)      0           input_2[0][0]                    \n",
            "                                                                 time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 75, 332)      0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 75, 512)      1206272     spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 75, 50)       25650       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "crf_2 (CRF)                     (None, 75, 7)        420         time_distributed_4[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,303,222\n",
            "Trainable params: 1,303,222\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qu2_ygeeRSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "9e17c8d1-8074-477e-88af-bd9475430000"
      },
      "source": [
        "random.seed(0)\n",
        "history = model.fit([X_word_train2,\n",
        "                     np.array(X_char_train2).reshape((len(X_char_train2), max_len, max_len_char))],\n",
        "                    np.array(y_train2),\n",
        "                    batch_size=64, epochs=8, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 33957 samples, validate on 3773 samples\n",
            "Epoch 1/8\n",
            "33957/33957 [==============================] - 853s 25ms/step - loss: 0.0369 - crf_viterbi_accuracy: 0.9908 - val_loss: 0.0136 - val_crf_viterbi_accuracy: 0.9948\n",
            "Epoch 2/8\n",
            "33957/33957 [==============================] - 861s 25ms/step - loss: 0.0090 - crf_viterbi_accuracy: 0.9951 - val_loss: 0.0017 - val_crf_viterbi_accuracy: 0.9962\n",
            "Epoch 3/8\n",
            "33957/33957 [==============================] - 862s 25ms/step - loss: -0.0021 - crf_viterbi_accuracy: 0.9958 - val_loss: -0.0058 - val_crf_viterbi_accuracy: 0.9951\n",
            "Epoch 4/8\n",
            "33957/33957 [==============================] - 854s 25ms/step - loss: -0.0113 - crf_viterbi_accuracy: 0.9963 - val_loss: -0.0149 - val_crf_viterbi_accuracy: 0.9963\n",
            "Epoch 5/8\n",
            "33957/33957 [==============================] - 856s 25ms/step - loss: -0.0194 - crf_viterbi_accuracy: 0.9966 - val_loss: -0.0218 - val_crf_viterbi_accuracy: 0.9950\n",
            "Epoch 6/8\n",
            "33957/33957 [==============================] - 856s 25ms/step - loss: -0.0271 - crf_viterbi_accuracy: 0.9968 - val_loss: -0.0303 - val_crf_viterbi_accuracy: 0.9965\n",
            "Epoch 7/8\n",
            "33957/33957 [==============================] - 858s 25ms/step - loss: -0.0346 - crf_viterbi_accuracy: 0.9970 - val_loss: -0.0376 - val_crf_viterbi_accuracy: 0.9967\n",
            "Epoch 8/8\n",
            "33957/33957 [==============================] - 852s 25ms/step - loss: -0.0421 - crf_viterbi_accuracy: 0.9971 - val_loss: -0.0446 - val_crf_viterbi_accuracy: 0.9966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_nOu6lieRSCT",
        "colab": {}
      },
      "source": [
        "hist = pd.DataFrame(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GkaKUJlTRSCU",
        "colab": {}
      },
      "source": [
        "X_char_train = 0\n",
        "X_word_train = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UU5NAh93RSCW",
        "colab": {}
      },
      "source": [
        "y_train = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sm0OBCaiW7Ux"
      },
      "source": [
        "##### **Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6FBFd8pYW7Ux",
        "colab": {}
      },
      "source": [
        "# labels used in the classification report\n",
        "labels = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_1I4vs-W7U0"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dyUacYlVW7U0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "81c7a628-14d7-473b-c849-522e567ddaa0"
      },
      "source": [
        "# Evaluation over train set\n",
        "y_pred = model.predict([X_word_valid, np.array(X_char_valid).reshape((len(X_char_valid),\n",
        "                                                                max_len, max_len_char))])\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_valid_true = np.argmax(y_valid, -1) #\n",
        "\n",
        "# Convert the index to tag\n",
        "y_pred = [[idx2tag[i] for i in row] for row in y_pred]\n",
        "y_valid_true = [[idx2tag[i] for i in row] for row in y_valid_true]\n",
        "\n",
        "print(\"F1-score is : {:.1%}\".format(flat_f1_score(y_valid_true, y_pred, average = 'micro', labels = labels)))\n",
        "\n",
        "report = flat_classification_report(y_true = y_valid_true, y_pred = y_pred, labels = labels)\n",
        "\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score is : 72.9%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-MOR       0.90      0.62      0.73       628\n",
            "       I-MOR       0.85      0.55      0.67      1069\n",
            "       E-MOR       0.80      0.55      0.65       629\n",
            "       S-MOR       0.82      0.93      0.87       617\n",
            "       V-MOR       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.84      0.64      0.73      2944\n",
            "   macro avg       0.67      0.53      0.58      2944\n",
            "weighted avg       0.84      0.64      0.72      2944\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HC8RAR8_W7U4"
      },
      "source": [
        "**Development set**\n",
        "\n",
        "The predictions over the development set are obtained by clinical case. Therefore, loop over all clinical cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0NoJCOf2W7U4"
      },
      "source": [
        "**Global metrics over the development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8EHAV1pW7U4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "e605e33f-251e-4c4e-ba80-461fa012140c"
      },
      "source": [
        "# Evaluation over the development set\n",
        "y_pred_dev = model.predict([X_word_dev, np.array(X_char_dev).reshape((len(X_char_dev),\n",
        "                                                                max_len, max_len_char))])\n",
        "y_pred_dev = np.argmax(y_pred_dev, axis=-1)\n",
        "y_dev_true = np.argmax(y_dev, axis=-1)\n",
        "# Convert the index to tag\n",
        "y_pred_dev = [[idx2tag[i] for i in row] for row in y_pred_dev]\n",
        "y_dev_true = [[idx2tag[i] for i in row] for row in y_dev_true]\n",
        "\n",
        "print(\"F1-score is : {:.1%}\".format(flat_f1_score(y_dev_true, y_pred_dev, average = 'micro', labels = labels)))\n",
        "\n",
        "report_dev_glob = flat_classification_report(y_true = y_dev_true, y_pred = y_pred_dev, labels = labels)\n",
        "print(report_dev_glob)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score is : 69.8%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-MOR       0.88      0.61      0.72      1620\n",
            "       I-MOR       0.79      0.49      0.61      2555\n",
            "       E-MOR       0.74      0.50      0.60      1617\n",
            "       S-MOR       0.85      0.90      0.87      1672\n",
            "       V-MOR       0.00      0.00      0.00        18\n",
            "\n",
            "   micro avg       0.82      0.61      0.70      7482\n",
            "   macro avg       0.65      0.50      0.56      7482\n",
            "weighted avg       0.81      0.61      0.69      7482\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BsaRLbctW7U8"
      },
      "source": [
        "**Metrics per clinical case over the development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhsbcdfmXiJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words(tokens_sent,label_sent, true_label_sent, start_char_pos):\n",
        "  new_tok, new_lab, true_lab, new_start_pos = [], [], [], []\n",
        "\n",
        "  for tokens, labels, true_labels, start_chars in zip(tokens_sent, label_sent, true_label_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, true_lab_aux, new_start_pos_aux = [], [], [], []\n",
        "    for token, label, true_label, start_char_i in zip(tokens, labels,true_labels,start_chars):\n",
        "      if token != \"PAD\":\n",
        "        new_tok_aux.append(token)\n",
        "        new_lab_aux.append(label)\n",
        "        true_lab_aux.append(true_label)\n",
        "        new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    true_lab.append(true_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, true_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egv5MyvWUhA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, true_labels_cc, new_start_pos_cc = [], [], [], []\n",
        "new_tokens_all, new_labels_all, true_labels_all, new_start_pos_all = [], [], [], []\n",
        "\n",
        "label_indices_sent_cc = []\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(X_dev_cc)):\n",
        "  y_pred_dev = model.predict([X_word_dev_cc[cc], np.array(X_char_dev_cc[cc]).reshape((len(X_char_dev_cc[cc]),\n",
        "                                                                max_len, max_len_char))])\n",
        "  y_pred_dev = np.argmax(y_pred_dev, axis=-1)\n",
        "  y_dev_true = np.argmax(y_dev_cc[cc], -1)\n",
        "\n",
        "  # Convert the index to tag\n",
        "  y_pred_dev = [[idx2tag[i] for i in row] for row in y_pred_dev]\n",
        "  y_dev_true = [[idx2tag[i] for i in row] for row in y_dev_true]\n",
        "\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = [], [], [], []\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = tokens_to_words(token_dev_cc[cc], \n",
        "                                        y_pred_dev, y_dev_true, start_char_dev_cc[cc])\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  true_labels_cc.append(true_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  true_labels_all.extend(true_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)\n",
        "\n",
        "report_dev_glob = flat_classification_report(y_true = true_labels_all, y_pred = new_labels_all, labels = labels)\n",
        "print(report_dev_glob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1dqwHKViW7VB"
      },
      "source": [
        "##### **Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yhIlh4AeW7VB",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(actual, predicted):\n",
        "    #classes       = np.unique(np.concatenate((actual,predicted)))\n",
        "    classes = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR', 'O']\n",
        "    confusion_mtx = np.empty((len(classes),len(classes)),dtype=np.int)\n",
        "    for i,a in enumerate(classes):\n",
        "        for j,p in enumerate(classes):\n",
        "            value = sum([sum([np.where((actual[sent][word]==a)*(predicted[sent][word]==p))[0].shape[0] \n",
        "                              for word in range(len(actual[sent]))]) for sent in range(len(actual))])\n",
        "            #confusion_mtx[i,j] = sum([np.where((actual[sent]==a)*(predicted[sent]==p))[0].shape[0] for sent in range(len(actual))])\n",
        "            confusion_mtx[i,j] = value\n",
        "    return confusion_mtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vTlgYU0KW7VE",
        "colab": {}
      },
      "source": [
        "# code to extract METRICS FOR ENTITY AND NON-ENTITY from the confusion matrix\n",
        "\n",
        "def metrics_from_cm(cm,labels):\n",
        "  TP = [[v for v in value[0:len(labels)]] for value in cm[0:len(labels)]]\n",
        "  print(TP)\n",
        "  TP = np.array(TP)\n",
        "  TP = sum(sum(TP))\n",
        "\n",
        "  FN = [value[-1] for value in cm[0:len(labels)]] # last column is O\n",
        "  print(FN)\n",
        "  FN = np.array(FN) \n",
        "  FN = sum(FN)\n",
        "\n",
        "  FP = cm[len(labels)][0:len(labels)] # last column is O\n",
        "  print(FP)\n",
        "  FP = sum(FP)\n",
        "\n",
        "  TN = cm[len(labels)][len(labels)] # last column is O\n",
        "  print(TN)\n",
        "\n",
        "  return TP, FN, FP, TN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lzVz0Xb_W7VG"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yBIPnT7VW7VG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4bb3b073-b91e-4c03-83a1-21f0ac6b51e7"
      },
      "source": [
        "# not done by clinical case\n",
        "actual    = np.array(y_valid_true)\n",
        "predicted = np.array(y_pred)\n",
        "confusion_matrix_train = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(confusion_matrix_train, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>387</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>7</td>\n",
              "      <td>590</td>\n",
              "      <td>41</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>344</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>575</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>29</td>\n",
              "      <td>67</td>\n",
              "      <td>40</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>88372</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR      O\n",
              "B-MOR    387     10      0     59      0    172\n",
              "I-MOR      7    590     41      3      0    428\n",
              "E-MOR      0     23    344     41      0    221\n",
              "S-MOR      8      1      4    575      0     29\n",
              "V-MOR      0      0      0      1      0      0\n",
              "O         29     67     40     26      0  88372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yq5S4LRxW7VI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "877caf9e-08cd-4238-d0db-9c751d359a97"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(confusion_matrix_train,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[387, 10, 0, 59, 0], [7, 590, 41, 3, 0], [0, 23, 344, 41, 0], [8, 1, 4, 575, 0], [0, 0, 0, 1, 0]]\n",
            "[172, 428, 221, 29, 0]\n",
            "[29 67 40 26  0]\n",
            "88372\n",
            "2094\n",
            "850\n",
            "162\n",
            "88372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i-oNDT-1W7VJ"
      },
      "source": [
        "**Development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bhTePJoXW7VL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "67f697fc-2d69-44a7-c6f3-7ca7b9a58c72"
      },
      "source": [
        "# confusion matrix computed over all the clinical cases \n",
        "\n",
        "actual    = np.array(true_labels_all)\n",
        "predicted = np.array(new_labels_all)\n",
        "cm = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(cm, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>989</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "      <td>476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>17</td>\n",
              "      <td>1264</td>\n",
              "      <td>141</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "      <td>814</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>32</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>1497</td>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>81</td>\n",
              "      <td>205</td>\n",
              "      <td>115</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>216026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR       O\n",
              "B-MOR    989     33      6    116      0     476\n",
              "I-MOR     17   1264    141     13      0    1120\n",
              "E-MOR      1     87    814     70      0     645\n",
              "S-MOR     32      5     23   1497      0     115\n",
              "V-MOR      3      6      3      4      0       2\n",
              "O         81    205    115     66      0  216026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EaJ_IxQTW7VN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6ad6ea90-9c98-4013-b498-1e8200f2bf22"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(cm,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[989, 33, 6, 116, 0], [17, 1264, 141, 13, 0], [1, 87, 814, 70, 0], [32, 5, 23, 1497, 0], [3, 6, 3, 4, 0]]\n",
            "[476, 1120, 645, 115, 2]\n",
            "[ 81 205 115  66   0]\n",
            "216026\n",
            "5124\n",
            "2358\n",
            "467\n",
            "216026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RovHCpu_W7VV"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-SO6iuhOW7Ui"
      },
      "source": [
        "#### **Final BiLSTM-CRF approach 3 model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91fOSmdg2Cp0",
        "colab_type": "text"
      },
      "source": [
        "##### **Feature preparation**\n",
        "**Complete dataset**\n",
        "\n",
        "combinaiton of train and development sets\n",
        "\n",
        "- df_data_complete\n",
        "\n",
        "- sentences_complete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TG8JwhZ2h1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting unique words and labels from data \n",
        "words = list(df_data_complete['Word'].unique())\n",
        "if (preprocess==True):\n",
        "  words = [preprocess_word(w) for w in words]\n",
        "  words = np.unique(words)\n",
        "  \n",
        "tags = list(df_data_complete['Tag'].unique())\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "# label is key and value is index.\n",
        "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
        "tag_to_index[\"PAD\"] = 0\n",
        "\n",
        "#idx2word = {i: w for w, i in word_to_index.items()}\n",
        "idx2tag = {i: w for w, i in tag_to_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En1E0j1L2tQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9b2827b4-56e0-4976-d4d2-0721afb212b9"
      },
      "source": [
        "chars = set([w_i for w in scielo_wiki_model.vocab.keys() for w_i in w])\n",
        "n_chars = len(chars)\n",
        "print(\"Number of characters in the dictionary: %d\" %n_chars)\n",
        "print(\"\\nUnique characters: %s\" %chars)\n",
        "\n",
        "char_to_idx = {c: i + 2 for i, c in enumerate(chars)}\n",
        "char_to_idx[\"UNK\"] = 1\n",
        "char_to_idx[\"PAD\"] = 0\n",
        "print(\"\\nDictionary of characters: %s\" %char_to_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters in the dictionary: 1953\n",
            "\n",
            "Unique characters: {'析', '義', 'Τ', 'ッ', '品', '電', '农', '出', '数', '残', '龙', '现', 'ἑ', '韓', '广', '過', '舞', 'ר', '动', '.', '魔', 'c', '島', 'т', 'ぞ', 'ש', '徳', '以', '川', 'Ζ', '夜', '之', '关', '御', '站', '展', 'М', '也', '帯', '銃', '右', '經', 'Υ', '主', '机', '太', 'ž', 'ö', '羽', '风', 'إ', 'Ḥ', '浮', '口', '読', '仙', 'ш', '禅', '变', '漫', '虫', '党', '丘', '怪', 'º', 'ó', 'T', '手', '效', '曼', '桃', '経', 'ब', 'ầ', '火', '提', 'え', '大', '寒', 'ủ', 'Г', '母', '议', '竹', '菜', '終', 'Ν', '基', 'ℓ', '様', '醉', '多', '笠', '稳', 'ĉ', '合', '妙', '秦', 'Y', 'á', '居', '雪', 'm', 'ζ', 'ţ', 'ض', '奥', '反', '玄', '粉', 'ŭ', 'ẻ', '家', '暗', 'û', 'ф', 'प', '干', '较', '深', '憶', '尽', '翁', '坂', '秘', '張', '懿', '病', 'Ù', '術', 'ᜃ', '擒', '游', '呼', 'В', 'ὗ', '向', '種', '裕', 'ἶ', '書', '汗', '靈', '入', '運', '华', '譚', '画', 'ご', '斗', 'ϐ', '卜', 'ɲ', 'ἓ', '雅', '会', '步', '煙', 'Ρ', 'Б', '开', '如', '井', '受', 'ひ', 'ʃ', '支', '债', '墨', 'ו', '衛', 'α', '対', 'φ', '歌', '要', 'Þ', '僕', '退', '桑', '空', '班', 'Ó', '韩', 'x', 'Ю', '义', 'ம', '次', '説', '鶴', '砖', '虚', '撃', '浅', '倫', '哲', 'ρ', '楚', 'ą', '摔', '零', 'Ε', '祭', '无', '味', '貨', '茂', '穴', '藥', 'Α', '界', 'ὥ', '墓', 'k', '廣', '件', 'ʼ', '弘', '标', '烟', 'ἴ', '教', '迅', '先', \"'\", 'Ā', 'S', '程', '甘', '狗', '期', '覇', '逆', 'べ', '说', '岡', '馆', 'ヤ', 'ĥ', '別', '卑', 'Ö', '复', '巨', 'β', '変', 'ད', '順', 'ء', '拳', '昆', '燈', '呂', 'L', '根', '治', '特', '葵', 'ར', '汇', '腐', 'ゴ', 'ć', 'ή', 'য', 'е', 'ǎ', 'ż', '七', '紀', '幻', 'Æ', 'ي', 'Ï', '滅', '沼', 'О', '不', '伝', 'Ж', '花', '书', '种', '江', '德', '雷', 'ь', '潤', '孟', 'स', '無', '冒', '落', '算', 'ぎ', 'プ', 'ع', 'у', '况', '牛', '夕', '詩', '毒', '康', 'ψ', 'ན', '银', '渡', 'ね', 'ע', '5', '志', '共', 'Ñ', 'カ', '仲', '投', '溪', '顯', '者', '>', '了', '牙', 'צ', '季', '玉', '泉', '慶', '場', '区', '下', '雜', 'ñ', 'ῶ', '澤', '宣', '衰', 's', 'з', '約', '状', '体', 'ϖ', 'ò', 'ὕ', 'ー', '契', '潜', '針', '恐', '式', '雨', 'Є', '辞', 'ち', 'å', 'Я', '春', 'Ş', 'क', 'ÿ', '君', 'ﬂ', '例', '任', 'ξ', '相', '富', '冲', 'Ō', '爾', '郎', 'ナ', 'ẋ', 'স', '樂', '師', 'し', '枝', '戦', 'こ', 'Õ', '遷', '稽', 'Φ', '沙', '果', '拉', '鋼', 'π', '试', '隠', '弟', '模', '能', '骨', 't', '管', 'σ', 'れ', 'ṭ', 'þ', 'м', '傅', '耳', '福', 'ĭ', 'Â', '酒', '蒙', '洱', '樽', '談', 'ṃ', '保', 'ἂ', '真', 'r', 'א', 'ﬁ', '鲜', '理', '遊', '農', '宏', '筋', '松', 'Ś', '老', '統', '離', '洋', 'ạ', 'ャ', 'ネ', '記', '姫', '波', '犬', '但', 'ج', 'ड', 'η', 'を', 'ἦ', '勝', 'つ', 'រ', '孔', '奈', '剣', '戀', '原', '那', '正', '螳', '1', '博', 'ì', '造', '援', 'ʝ', 'ù', 'バ', '殺', '焼', '米', '破', '竞', 'J', 'ℕ', '木', '并', '股', '敦', '迷', 'ℝ', '劳', 'Å', '架', '史', '释', 'अ', '鳥', '0', '积', 'ず', '仁', 'г', 'サ', 'セ', '態', 'ṣ', '马', '沈', 'க', '器', '久', '情', '4', '始', 'ı', '県', 'נ', '六', '用', '洞', '域', 'ग', '遠', '被', '传', 'ر', '男', '忍', 'д', '决', '周', 'ė', '広', '渠', '个', '灭', '會', 'Ì', '王', 'ῇ', 'る', '桂', '才', '些', '弓', 'ל', '陶', '别', '石', '视', 'Р', 'ា', '去', 'ル', '賀', 'ˈ', '片', 'N', '醸', 'κ', 'ώ', '晉', 'e', '東', 'コ', 'õ', 'ἁ', '湯', '验', '7', '李', '从', '累', '報', 'ベ', '兵', 'ῥ', '閭', '挑', 'ុ', '水', '勃', 'ェ', '伊', 'ざ', '/', 'î', 'ὄ', 'l', 'λ', '可', '3', '橋', '早', '休', 'வ', '济', 'Ú', '青', '藤', '企', '掌', 'ĕ', '奇', '贷', 'ύ', '爪', '及', ':', '湘', '然', 'ẏ', '很', '陀', 'פ', 'ز', '戒', 'ロ', '獄', '千', '且', 'с', '屋', 'ᜈ', 'ˇ', '调', '殿', '凝', 'ɯ', 'て', 'Ω', '話', '勢', '放', '寺', '园', 'モ', '一', '全', '略', '票', '业', 'ẽ', 'س', 'ῆ', '吴', 'ή', '推', '物', '判', 'é', 'Ê', 'ウ', '趙', '价', '得', 'च', '愛', '鐵', '郭', '演', '声', 'I', 'ק', '树', '半', '泡', '思', '茅', 'Ϊ', 'ŋ', '兼', 'み', 'ę', '助', '毛', '치', '侍', 'さ', '盛', '鮓', '重', 'С', '龍', '論', '度', '有', '丁', 'ἰ', '初', '必', 'ṇ', 'भ', '当', '杉', '最', '束', '再', '強', '沢', 'ल', '茶', '村', '許', 'ĩ', 'ボ', '着', '拔', '少', '折', 'に', '欧', '达', '色', '言', '山', 'ē', '宗', '豚', '聞', '雄', 'ण', '紫', 'C', '四', '设', '森', 'ィ', '探', '边', '鯉', '香', '組', '螂', '哥', 'd', 'Ξ', 'ش', '洲', '却', 'ω', 'ἢ', 'ク', '扩', 'せ', '弱', 'ɨ', '国', 'न', 'ế', 'ד', '感', 'ὔ', '業', 'ॐ', '鉄', 'ê', '论', '俊', '员', 'ä', '飛', '脈', '貴', '叉', '沮', '带', '弥', '随', '蓝', 'ט', '征', '曲', '陽', '車', '廟', 'К', '謝', '盧', '座', '國', 'ὖ', 'ồ', 'כ', '印', 'は', '號', 'đ', '凡', 'ε', 'ध', 'Ż', '赤', '货', '计', 'z', '走', '知', '显', 'ת', 'ី', '球', 'ิ', 'ἠ', '化', '争', 'ī', '漢', '鬼', '派', '牧', '尤', '技', '洪', '临', '辺', 'æ', 'œ', 'も', 'ħ', 'ș', '戲', 'ː', 'ர', '属', 'Ž', '的', 'Ë', '開', '孙', 'ุ', '岩', '事', '在', 'ð', '佐', 'ū', '跡', 'ώ', '鮮', 'Ã', '袁', '名', '百', '·', '妖', '门', 'ط', 'ɬ', 'ɸ', '說', 'Κ', '笑', '庵', '植', '于', '翼', '章', 'o', '通', '连', '使', 'ɣ', 'л', 'ḥ', 'ï', '芋', '星', '涼', '議', '氣', '顔', 'ন', '首', '沫', '杂', '形', '葉', 'Ə', '妃', 'ί', '直', '术', '梦', 'ὸ', 'í', 'ב', '轮', '澄', '回', '月', '芪', 'ज', '臼', 'Ä', 'f', '列', '越', '親', '利', 'す', '虹', '武', 'G', 'ば', 'ả', 'お', 'y', '比', 'Σ', '增', '坐', '麗', '來', '觀', 'У', 'ゃ', '献', '血', 'ḗ', '成', '産', '禁', 'А', '阶', '它', '吾', '瞳', 'ô', 'ú', '后', '即', 'Î', '麴', '寿', 'ᾧ', '房', '更', '喜', '縣', 'ſ', '司', 'ɔ', 'י', '白', '관', '實', 'И', '断', '差', '供', '央', 'マ', 'Ø', 'ص', '语', '実', '刺', 'ό', '扁', '嘛', 'ン', '面', 'à', '斛', '统', '日', 'ف', '类', '伴', '夷', '斯', '经', '精', 'ן', '和', '角', '棍', '侯', 'ἲ', '究', '樹', '蔵', 'ق', 'ɛ', '廷', 'ǒ', 'Ά', '葛', '端', '存', 'V', 'B', '録', '权', '乙', '典', 'い', 'ũ', '框', 'ễ', '抱', '将', 'Χ', 'ϕ', 'р', '馬', '命', '八', '常', '炎', '渾', '獅', '构', '危', '便', 'イ', '虑', '板', 'ぼ', '김', '劇', '園', '亭', '城', 'آ', 'Ò', '獣', '气', '认', '輝', '風', '何', 'ی', '純', '上', '館', '心', '詠', '林', 'Γ', '市', '芸', 'ष', '慕', '麹', 'ἷ', 'خ', '頭', 'υ', '内', '币', '豆', '异', '强', '料', '嘉', '亡', 'か', '北', '秋', '織', '鎖', 'ὐ', '取', '齊', 'き', '岳', '操', '案', '斎', '九', '校', '阳', '機', '鮨', '爱', '贸', '魚', '号', 'ч', 'レ', 'ὦ', 'ɢ', '背', '点', '惠', '為', '超', 'θ', '河', '务', 'ム', '到', 'ŵ', 'K', 'ا', 'ὼ', '致', '塚', '秀', '邑', '黒', '张', '儀', '溝', '円', 'Đ', '์', '猫', '冥', 'ङ', 'ケ', 'Œ', '路', '級', '亜', '样', '转', '改', '夏', '前', 'ה', '京', 'ἕ', 'ы', '是', '恵', '蠕', '查', '錦', '連', '浦', 'ん', '儒', 'Á', '为', '祝', '境', 'り', '限', '珍', '加', '各', '已', '佳', 'ト', '部', '恋', '念', '话', '曹', '未', '希', 'ˆ', '皇', '长', '表', '時', '阿', '蒲', '計', '平', '创', '慈', '悠', 'ह', '莊', 'น', '散', '冰', '庭', 'ο', '巴', 'ك', 'ŏ', 'Š', '西', 'ז', '力', 'ゅ', 'ァ', '夢', '結', 'ʔ', 'ø', '示', '包', '厥', '二', '里', '社', '想', '友', 'ʒ', 'タ', 'ń', '战', 'ǚ', '幕', 'ס', '個', '霜', '堂', 'X', '服', '洵', 'ẵ', '蘇', 'د', 'य', '田', '由', '気', 'ō', 'я', 'Ẹ', 'ὑ', 'н', '問', 'к', 'や', 'ף', '船', '等', 'ポ', '紅', '括', '邦', '领', '藏', 'Ι', '昌', '爆', 'ゆ', 'i', '細', 'デ', '極', 'غ', '糸', '池', '唐', '定', '2', 'Л', '分', '้', '政', 'ั', 'γ', 'げ', '贡', '異', '短', 'ἐ', 'È', '头', 'ὀ', 'ب', '賓', 'ة', '見', 'Ð', 'š', 'ḍ', 'P', '刀', 'ü', '楊', 'ă', 'μ', '桜', 'ὲ', '检', 'ν', '素', '法', '资', '昭', '旅', '父', '同', '激', '解', '遵', '惑', '实', '間', '密', '氏', '狂', '華', 'F', 'র', 'h', '乃', 'ř', '達', 'け', 'ğ', '皮', '假', '左', '藝', 'う', 'よ', '玲', '代', '霉', 'オ', '锺', '洗', 'श', 'ϵ', 'Ἀ', '据', '丶', '篇', '羌', '余', 'b', 'ド', '财', 'ま', '烏', 'र', 'っ', '窓', '町', 'で', 'が', '切', '制', '衣', '与', 'ç', '砲', '腿', '版', 'è', '离', '题', '逸', 'Η', 'め', 'M', '率', '礼', 'ῖ', '望', '掛', 'ἡ', '清', '吐', '丸', '协', '門', '质', 'त', '霍', 'W', '引', 'Е', '悪', 'Н', '9', 'ه', 'ǘ', '隋', '陈', '律', '他', '段', 'フ', 'ǽ', '安', 'ᜆ', 'ミ', '这', '垂', '好', '勒', 'ϒ', 'ứ', '野', 'Ț', '公', 'ś', '十', '车', 'ë', '自', '熊', 'ț', '策', '黃', '夫', '延', '軍', '你', '伏', '竜', '適', 'ớ', '神', 'ě', '吉', '新', '苏', '两', '流', 'в', 'ϰ', '三', '宫', '融', '土', 'ゼ', '拓', 'ن', '伯', '女', '估', '来', '量', '过', '絵', '结', '焙', 'ά', '<', '位', 'ʁ', '朱', '字', 'ὁ', 'ª', '帝', 'З', 'ź', '丹', '燕', 'ǐ', '柔', '纪', '小', '我', '衡', '均', '進', 'ϱ', '守', '節', '今', 'έ', 'ப', '人', '间', '霊', 'u', '单', '易', '孝', '杰', '工', '独', '映', '陳', '止', 'Π', '研', '株', 'о', '布', '非', '许', '际', '桓', '衫', '吕', '打', 'た', '油', '鹿', '伦', 'и', '年', '若', '永', 'б', 'п', '摩', 'ё', '卵', 'ϑ', '问', '逃', 'ت', 'ス', '普', '狐', '功', '具', '邪', '或', '外', 'w', '內', 'ə', 'ل', '铁', '麻', 'Β', 'ἥ', 'ϛ', '免', '学', '朝', '活', 'ラ', 'v', 'ם', 'Д', '峰', '意', 'Μ', '劉', '賢', '光', 'じ', '楽', 'Э', '益', 'Ô', 'ম', 'ろ', '所', '肉', '室', '高', '府', '东', '伸', '州', '孫', 'ἅ', '升', '元', '文', 'q', '近', '省', 'Ţ', 'χ', '革', '団', '露', 'ʿ', 'の', '食', '性', 'だ', '魂', '智', '動', '追', 'そ', '进', '魏', 'Ü', 'ἄ', '商', '豊', '采', 'ὴ', '地', '突', 'Ⓐ', '佛', 'ὡ', '振', '刘', 'ā', '场', 'х', 'ノ', '貞', '賦', '生', '海', '响', 'な', '尊', '就', '後', '对', '红', 'ら', 'リ', 'ῦ', 'ƒ', 'O', '考', '臣', '虎', 'द', 'ệ', '彦', '菌', '配', 'へ', '其', 'ᾳ', '伽', '蹤', 'â', 'n', '郁', '影', '看', 'ץ', '飯', '专', 'キ', '起', '倉', '需', '科', '騎', '讨', '乱', 'Ĥ', 'ʻ', '产', '子', '景', '綺', 'R', 'Í', '陵', '古', '固', 'チ', '鑑', '序', '׳', '五', 'Θ', 'Ŷ', '菇', '幸', 'ర', '蔣', 'ח', '贫', 'ὺ', '应', '谷', '祖', '草', 'ᾶ', 'ӧ', '修', 'ஸ', '蔡', 'ᜐ', 'ő', '禽', 'П', '中', '拿', '耀', '集', '条', 'τ', '士', '收', 'ό', 'a', '天', '6', 'ג', '萬', 'Ч', '此', 'ᜋ', '因', '容', '交', '崎', 'Ł', '威', '都', 'わ', '述', '緒', 'م', 'ɪ', '姓', 'ấ', '级', 'Ψ', '型', '្', 'Č', 'ί', '药', 'ň', '微', 'µ', '美', 'म', 'و', 'ã', ',', 'ʎ', 'δ', '่', 'p', '哀', '系', '鳳', '矛', '當', '烈', 'ὰ', '明', '尾', 'й', 'ச', 'ἤ', 'ἃ', 'ブ', '緑', '匈', '錢', '忠', '敏', '民', '隊', '导', '蓮', '私', 'ɾ', '云', '消', '院', '沖', 'ぬ', 'E', '卦', '良', 'ý', 'ς', '象', 'ぶ', '宇', '评', '乳', '梅', '_', '戸', '訓', '方', '淑', '持', '羅', '令', '万', 'ἱ', '店', '悟', '该', '善', '南', 'Ἰ', 'ύ', '々', '童', 'Q', 'ど', '预', 'מ', '源', '姿', '困', 'ى', '跤', '至', 'ş', '而', 'ᜀ', '死', '黄', '巻', 'ὶ', '局', '臨', 'і', '甫', '仮', 'ῷ', 'と', 'び', '建', '學', 'Ь', '兴', 'Ф', '語', '卷', '台', '菊', '完', '聖', 'ヨ', '銀', 'く', '杜', '本', 'č', '金', '浪', '慎', '育', '棒', '甲', 'ᜇ', '汽', '絶', 'Δ', '督', '替', '賊', '描', '历', '极', '차', '桐', 'İ', 'డ', '荷', '目', 'ŷ', '时', '施', '梨', '宾', '值', '记', 'j', 'व', 'ぐ', '失', '联', 'أ', '關', '津', '膨', 'ǔ', 'έ', '批', '瀬', 'ស', '長', '第', 'D', '勲', 'ά', '曜', 'ょ', '则', 'த', '叶', '立', 'ł', '傳', '宋', '道', 'ὃ', '信', 'ี', 'É', '綿', '身', '琴', '編', '作', '兰', '寧', 'ᜅ', 'ɑ', '射', '份', 'ơ', '綠', '試', '返', 'U', 'ι', '宝', 'ハ', '胡', '观', '椿', '行', 'ḫ', '單', 'ἀ', '横', '宮', '厳', '族', '盾', '索', 'ὅ', '奴', '優', 'ů', '満', 'g', 'ℏ', '们', '雲', 'Λ', 'À', 'Ω', '著', '卍', '音', '快', '敬', 'Ç', 'ἔ', 'H', '降', '介', '湖', '付', 'Ο', '潮', '接', 'Z', '陰', '欠', '恩', '屠', '盟', 'ح', 'あ', '项', 'Ǻ', '师', 'A', 'ℜ', 'ظ', '8', '彼', '指', '世', '英', 'а', '够', '发'}\n",
            "\n",
            "Dictionary of characters: {'析': 2, '義': 3, 'Τ': 4, 'ッ': 5, '品': 6, '電': 7, '农': 8, '出': 9, '数': 10, '残': 11, '龙': 12, '现': 13, 'ἑ': 14, '韓': 15, '广': 16, '過': 17, '舞': 18, 'ר': 19, '动': 20, '.': 21, '魔': 22, 'c': 23, '島': 24, 'т': 25, 'ぞ': 26, 'ש': 27, '徳': 28, '以': 29, '川': 30, 'Ζ': 31, '夜': 32, '之': 33, '关': 34, '御': 35, '站': 36, '展': 37, 'М': 38, '也': 39, '帯': 40, '銃': 41, '右': 42, '經': 43, 'Υ': 44, '主': 45, '机': 46, '太': 47, 'ž': 48, 'ö': 49, '羽': 50, '风': 51, 'إ': 52, 'Ḥ': 53, '浮': 54, '口': 55, '読': 56, '仙': 57, 'ш': 58, '禅': 59, '变': 60, '漫': 61, '虫': 62, '党': 63, '丘': 64, '怪': 65, 'º': 66, 'ó': 67, 'T': 68, '手': 69, '效': 70, '曼': 71, '桃': 72, '経': 73, 'ब': 74, 'ầ': 75, '火': 76, '提': 77, 'え': 78, '大': 79, '寒': 80, 'ủ': 81, 'Г': 82, '母': 83, '议': 84, '竹': 85, '菜': 86, '終': 87, 'Ν': 88, '基': 89, 'ℓ': 90, '様': 91, '醉': 92, '多': 93, '笠': 94, '稳': 95, 'ĉ': 96, '合': 97, '妙': 98, '秦': 99, 'Y': 100, 'á': 101, '居': 102, '雪': 103, 'm': 104, 'ζ': 105, 'ţ': 106, 'ض': 107, '奥': 108, '反': 109, '玄': 110, '粉': 111, 'ŭ': 112, 'ẻ': 113, '家': 114, '暗': 115, 'û': 116, 'ф': 117, 'प': 118, '干': 119, '较': 120, '深': 121, '憶': 122, '尽': 123, '翁': 124, '坂': 125, '秘': 126, '張': 127, '懿': 128, '病': 129, 'Ù': 130, '術': 131, 'ᜃ': 132, '擒': 133, '游': 134, '呼': 135, 'В': 136, 'ὗ': 137, '向': 138, '種': 139, '裕': 140, 'ἶ': 141, '書': 142, '汗': 143, '靈': 144, '入': 145, '運': 146, '华': 147, '譚': 148, '画': 149, 'ご': 150, '斗': 151, 'ϐ': 152, '卜': 153, 'ɲ': 154, 'ἓ': 155, '雅': 156, '会': 157, '步': 158, '煙': 159, 'Ρ': 160, 'Б': 161, '开': 162, '如': 163, '井': 164, '受': 165, 'ひ': 166, 'ʃ': 167, '支': 168, '债': 169, '墨': 170, 'ו': 171, '衛': 172, 'α': 173, '対': 174, 'φ': 175, '歌': 176, '要': 177, 'Þ': 178, '僕': 179, '退': 180, '桑': 181, '空': 182, '班': 183, 'Ó': 184, '韩': 185, 'x': 186, 'Ю': 187, '义': 188, 'ம': 189, '次': 190, '説': 191, '鶴': 192, '砖': 193, '虚': 194, '撃': 195, '浅': 196, '倫': 197, '哲': 198, 'ρ': 199, '楚': 200, 'ą': 201, '摔': 202, '零': 203, 'Ε': 204, '祭': 205, '无': 206, '味': 207, '貨': 208, '茂': 209, '穴': 210, '藥': 211, 'Α': 212, '界': 213, 'ὥ': 214, '墓': 215, 'k': 216, '廣': 217, '件': 218, 'ʼ': 219, '弘': 220, '标': 221, '烟': 222, 'ἴ': 223, '教': 224, '迅': 225, '先': 226, \"'\": 227, 'Ā': 228, 'S': 229, '程': 230, '甘': 231, '狗': 232, '期': 233, '覇': 234, '逆': 235, 'べ': 236, '说': 237, '岡': 238, '馆': 239, 'ヤ': 240, 'ĥ': 241, '別': 242, '卑': 243, 'Ö': 244, '复': 245, '巨': 246, 'β': 247, '変': 248, 'ད': 249, '順': 250, 'ء': 251, '拳': 252, '昆': 253, '燈': 254, '呂': 255, 'L': 256, '根': 257, '治': 258, '特': 259, '葵': 260, 'ར': 261, '汇': 262, '腐': 263, 'ゴ': 264, 'ć': 265, 'ή': 266, 'য': 267, 'е': 268, 'ǎ': 269, 'ż': 270, '七': 271, '紀': 272, '幻': 273, 'Æ': 274, 'ي': 275, 'Ï': 276, '滅': 277, '沼': 278, 'О': 279, '不': 280, '伝': 281, 'Ж': 282, '花': 283, '书': 284, '种': 285, '江': 286, '德': 287, '雷': 288, 'ь': 289, '潤': 290, '孟': 291, 'स': 292, '無': 293, '冒': 294, '落': 295, '算': 296, 'ぎ': 297, 'プ': 298, 'ع': 299, 'у': 300, '况': 301, '牛': 302, '夕': 303, '詩': 304, '毒': 305, '康': 306, 'ψ': 307, 'ན': 308, '银': 309, '渡': 310, 'ね': 311, 'ע': 312, '5': 313, '志': 314, '共': 315, 'Ñ': 316, 'カ': 317, '仲': 318, '投': 319, '溪': 320, '顯': 321, '者': 322, '>': 323, '了': 324, '牙': 325, 'צ': 326, '季': 327, '玉': 328, '泉': 329, '慶': 330, '場': 331, '区': 332, '下': 333, '雜': 334, 'ñ': 335, 'ῶ': 336, '澤': 337, '宣': 338, '衰': 339, 's': 340, 'з': 341, '約': 342, '状': 343, '体': 344, 'ϖ': 345, 'ò': 346, 'ὕ': 347, 'ー': 348, '契': 349, '潜': 350, '針': 351, '恐': 352, '式': 353, '雨': 354, 'Є': 355, '辞': 356, 'ち': 357, 'å': 358, 'Я': 359, '春': 360, 'Ş': 361, 'क': 362, 'ÿ': 363, '君': 364, 'ﬂ': 365, '例': 366, '任': 367, 'ξ': 368, '相': 369, '富': 370, '冲': 371, 'Ō': 372, '爾': 373, '郎': 374, 'ナ': 375, 'ẋ': 376, 'স': 377, '樂': 378, '師': 379, 'し': 380, '枝': 381, '戦': 382, 'こ': 383, 'Õ': 384, '遷': 385, '稽': 386, 'Φ': 387, '沙': 388, '果': 389, '拉': 390, '鋼': 391, 'π': 392, '试': 393, '隠': 394, '弟': 395, '模': 396, '能': 397, '骨': 398, 't': 399, '管': 400, 'σ': 401, 'れ': 402, 'ṭ': 403, 'þ': 404, 'м': 405, '傅': 406, '耳': 407, '福': 408, 'ĭ': 409, 'Â': 410, '酒': 411, '蒙': 412, '洱': 413, '樽': 414, '談': 415, 'ṃ': 416, '保': 417, 'ἂ': 418, '真': 419, 'r': 420, 'א': 421, 'ﬁ': 422, '鲜': 423, '理': 424, '遊': 425, '農': 426, '宏': 427, '筋': 428, '松': 429, 'Ś': 430, '老': 431, '統': 432, '離': 433, '洋': 434, 'ạ': 435, 'ャ': 436, 'ネ': 437, '記': 438, '姫': 439, '波': 440, '犬': 441, '但': 442, 'ج': 443, 'ड': 444, 'η': 445, 'を': 446, 'ἦ': 447, '勝': 448, 'つ': 449, 'រ': 450, '孔': 451, '奈': 452, '剣': 453, '戀': 454, '原': 455, '那': 456, '正': 457, '螳': 458, '1': 459, '博': 460, 'ì': 461, '造': 462, '援': 463, 'ʝ': 464, 'ù': 465, 'バ': 466, '殺': 467, '焼': 468, '米': 469, '破': 470, '竞': 471, 'J': 472, 'ℕ': 473, '木': 474, '并': 475, '股': 476, '敦': 477, '迷': 478, 'ℝ': 479, '劳': 480, 'Å': 481, '架': 482, '史': 483, '释': 484, 'अ': 485, '鳥': 486, '0': 487, '积': 488, 'ず': 489, '仁': 490, 'г': 491, 'サ': 492, 'セ': 493, '態': 494, 'ṣ': 495, '马': 496, '沈': 497, 'க': 498, '器': 499, '久': 500, '情': 501, '4': 502, '始': 503, 'ı': 504, '県': 505, 'נ': 506, '六': 507, '用': 508, '洞': 509, '域': 510, 'ग': 511, '遠': 512, '被': 513, '传': 514, 'ر': 515, '男': 516, '忍': 517, 'д': 518, '决': 519, '周': 520, 'ė': 521, '広': 522, '渠': 523, '个': 524, '灭': 525, '會': 526, 'Ì': 527, '王': 528, 'ῇ': 529, 'る': 530, '桂': 531, '才': 532, '些': 533, '弓': 534, 'ל': 535, '陶': 536, '别': 537, '石': 538, '视': 539, 'Р': 540, 'ា': 541, '去': 542, 'ル': 543, '賀': 544, 'ˈ': 545, '片': 546, 'N': 547, '醸': 548, 'κ': 549, 'ώ': 550, '晉': 551, 'e': 552, '東': 553, 'コ': 554, 'õ': 555, 'ἁ': 556, '湯': 557, '验': 558, '7': 559, '李': 560, '从': 561, '累': 562, '報': 563, 'ベ': 564, '兵': 565, 'ῥ': 566, '閭': 567, '挑': 568, 'ុ': 569, '水': 570, '勃': 571, 'ェ': 572, '伊': 573, 'ざ': 574, '/': 575, 'î': 576, 'ὄ': 577, 'l': 578, 'λ': 579, '可': 580, '3': 581, '橋': 582, '早': 583, '休': 584, 'வ': 585, '济': 586, 'Ú': 587, '青': 588, '藤': 589, '企': 590, '掌': 591, 'ĕ': 592, '奇': 593, '贷': 594, 'ύ': 595, '爪': 596, '及': 597, ':': 598, '湘': 599, '然': 600, 'ẏ': 601, '很': 602, '陀': 603, 'פ': 604, 'ز': 605, '戒': 606, 'ロ': 607, '獄': 608, '千': 609, '且': 610, 'с': 611, '屋': 612, 'ᜈ': 613, 'ˇ': 614, '调': 615, '殿': 616, '凝': 617, 'ɯ': 618, 'て': 619, 'Ω': 620, '話': 621, '勢': 622, '放': 623, '寺': 624, '园': 625, 'モ': 626, '一': 627, '全': 628, '略': 629, '票': 630, '业': 631, 'ẽ': 632, 'س': 633, 'ῆ': 634, '吴': 635, 'ή': 636, '推': 637, '物': 638, '判': 639, 'é': 640, 'Ê': 641, 'ウ': 642, '趙': 643, '价': 644, '得': 645, 'च': 646, '愛': 647, '鐵': 648, '郭': 649, '演': 650, '声': 651, 'I': 652, 'ק': 653, '树': 654, '半': 655, '泡': 656, '思': 657, '茅': 658, 'Ϊ': 659, 'ŋ': 660, '兼': 661, 'み': 662, 'ę': 663, '助': 664, '毛': 665, '치': 666, '侍': 667, 'さ': 668, '盛': 669, '鮓': 670, '重': 671, 'С': 672, '龍': 673, '論': 674, '度': 675, '有': 676, '丁': 677, 'ἰ': 678, '初': 679, '必': 680, 'ṇ': 681, 'भ': 682, '当': 683, '杉': 684, '最': 685, '束': 686, '再': 687, '強': 688, '沢': 689, 'ल': 690, '茶': 691, '村': 692, '許': 693, 'ĩ': 694, 'ボ': 695, '着': 696, '拔': 697, '少': 698, '折': 699, 'に': 700, '欧': 701, '达': 702, '色': 703, '言': 704, '山': 705, 'ē': 706, '宗': 707, '豚': 708, '聞': 709, '雄': 710, 'ण': 711, '紫': 712, 'C': 713, '四': 714, '设': 715, '森': 716, 'ィ': 717, '探': 718, '边': 719, '鯉': 720, '香': 721, '組': 722, '螂': 723, '哥': 724, 'd': 725, 'Ξ': 726, 'ش': 727, '洲': 728, '却': 729, 'ω': 730, 'ἢ': 731, 'ク': 732, '扩': 733, 'せ': 734, '弱': 735, 'ɨ': 736, '国': 737, 'न': 738, 'ế': 739, 'ד': 740, '感': 741, 'ὔ': 742, '業': 743, 'ॐ': 744, '鉄': 745, 'ê': 746, '论': 747, '俊': 748, '员': 749, 'ä': 750, '飛': 751, '脈': 752, '貴': 753, '叉': 754, '沮': 755, '带': 756, '弥': 757, '随': 758, '蓝': 759, 'ט': 760, '征': 761, '曲': 762, '陽': 763, '車': 764, '廟': 765, 'К': 766, '謝': 767, '盧': 768, '座': 769, '國': 770, 'ὖ': 771, 'ồ': 772, 'כ': 773, '印': 774, 'は': 775, '號': 776, 'đ': 777, '凡': 778, 'ε': 779, 'ध': 780, 'Ż': 781, '赤': 782, '货': 783, '计': 784, 'z': 785, '走': 786, '知': 787, '显': 788, 'ת': 789, 'ី': 790, '球': 791, 'ิ': 792, 'ἠ': 793, '化': 794, '争': 795, 'ī': 796, '漢': 797, '鬼': 798, '派': 799, '牧': 800, '尤': 801, '技': 802, '洪': 803, '临': 804, '辺': 805, 'æ': 806, 'œ': 807, 'も': 808, 'ħ': 809, 'ș': 810, '戲': 811, 'ː': 812, 'ர': 813, '属': 814, 'Ž': 815, '的': 816, 'Ë': 817, '開': 818, '孙': 819, 'ุ': 820, '岩': 821, '事': 822, '在': 823, 'ð': 824, '佐': 825, 'ū': 826, '跡': 827, 'ώ': 828, '鮮': 829, 'Ã': 830, '袁': 831, '名': 832, '百': 833, '·': 834, '妖': 835, '门': 836, 'ط': 837, 'ɬ': 838, 'ɸ': 839, '說': 840, 'Κ': 841, '笑': 842, '庵': 843, '植': 844, '于': 845, '翼': 846, '章': 847, 'o': 848, '通': 849, '连': 850, '使': 851, 'ɣ': 852, 'л': 853, 'ḥ': 854, 'ï': 855, '芋': 856, '星': 857, '涼': 858, '議': 859, '氣': 860, '顔': 861, 'ন': 862, '首': 863, '沫': 864, '杂': 865, '形': 866, '葉': 867, 'Ə': 868, '妃': 869, 'ί': 870, '直': 871, '术': 872, '梦': 873, 'ὸ': 874, 'í': 875, 'ב': 876, '轮': 877, '澄': 878, '回': 879, '月': 880, '芪': 881, 'ज': 882, '臼': 883, 'Ä': 884, 'f': 885, '列': 886, '越': 887, '親': 888, '利': 889, 'す': 890, '虹': 891, '武': 892, 'G': 893, 'ば': 894, 'ả': 895, 'お': 896, 'y': 897, '比': 898, 'Σ': 899, '增': 900, '坐': 901, '麗': 902, '來': 903, '觀': 904, 'У': 905, 'ゃ': 906, '献': 907, '血': 908, 'ḗ': 909, '成': 910, '産': 911, '禁': 912, 'А': 913, '阶': 914, '它': 915, '吾': 916, '瞳': 917, 'ô': 918, 'ú': 919, '后': 920, '即': 921, 'Î': 922, '麴': 923, '寿': 924, 'ᾧ': 925, '房': 926, '更': 927, '喜': 928, '縣': 929, 'ſ': 930, '司': 931, 'ɔ': 932, 'י': 933, '白': 934, '관': 935, '實': 936, 'И': 937, '断': 938, '差': 939, '供': 940, '央': 941, 'マ': 942, 'Ø': 943, 'ص': 944, '语': 945, '実': 946, '刺': 947, 'ό': 948, '扁': 949, '嘛': 950, 'ン': 951, '面': 952, 'à': 953, '斛': 954, '统': 955, '日': 956, 'ف': 957, '类': 958, '伴': 959, '夷': 960, '斯': 961, '经': 962, '精': 963, 'ן': 964, '和': 965, '角': 966, '棍': 967, '侯': 968, 'ἲ': 969, '究': 970, '樹': 971, '蔵': 972, 'ق': 973, 'ɛ': 974, '廷': 975, 'ǒ': 976, 'Ά': 977, '葛': 978, '端': 979, '存': 980, 'V': 981, 'B': 982, '録': 983, '权': 984, '乙': 985, '典': 986, 'い': 987, 'ũ': 988, '框': 989, 'ễ': 990, '抱': 991, '将': 992, 'Χ': 993, 'ϕ': 994, 'р': 995, '馬': 996, '命': 997, '八': 998, '常': 999, '炎': 1000, '渾': 1001, '獅': 1002, '构': 1003, '危': 1004, '便': 1005, 'イ': 1006, '虑': 1007, '板': 1008, 'ぼ': 1009, '김': 1010, '劇': 1011, '園': 1012, '亭': 1013, '城': 1014, 'آ': 1015, 'Ò': 1016, '獣': 1017, '气': 1018, '认': 1019, '輝': 1020, '風': 1021, '何': 1022, 'ی': 1023, '純': 1024, '上': 1025, '館': 1026, '心': 1027, '詠': 1028, '林': 1029, 'Γ': 1030, '市': 1031, '芸': 1032, 'ष': 1033, '慕': 1034, '麹': 1035, 'ἷ': 1036, 'خ': 1037, '頭': 1038, 'υ': 1039, '内': 1040, '币': 1041, '豆': 1042, '异': 1043, '强': 1044, '料': 1045, '嘉': 1046, '亡': 1047, 'か': 1048, '北': 1049, '秋': 1050, '織': 1051, '鎖': 1052, 'ὐ': 1053, '取': 1054, '齊': 1055, 'き': 1056, '岳': 1057, '操': 1058, '案': 1059, '斎': 1060, '九': 1061, '校': 1062, '阳': 1063, '機': 1064, '鮨': 1065, '爱': 1066, '贸': 1067, '魚': 1068, '号': 1069, 'ч': 1070, 'レ': 1071, 'ὦ': 1072, 'ɢ': 1073, '背': 1074, '点': 1075, '惠': 1076, '為': 1077, '超': 1078, 'θ': 1079, '河': 1080, '务': 1081, 'ム': 1082, '到': 1083, 'ŵ': 1084, 'K': 1085, 'ا': 1086, 'ὼ': 1087, '致': 1088, '塚': 1089, '秀': 1090, '邑': 1091, '黒': 1092, '张': 1093, '儀': 1094, '溝': 1095, '円': 1096, 'Đ': 1097, '์': 1098, '猫': 1099, '冥': 1100, 'ङ': 1101, 'ケ': 1102, 'Œ': 1103, '路': 1104, '級': 1105, '亜': 1106, '样': 1107, '转': 1108, '改': 1109, '夏': 1110, '前': 1111, 'ה': 1112, '京': 1113, 'ἕ': 1114, 'ы': 1115, '是': 1116, '恵': 1117, '蠕': 1118, '查': 1119, '錦': 1120, '連': 1121, '浦': 1122, 'ん': 1123, '儒': 1124, 'Á': 1125, '为': 1126, '祝': 1127, '境': 1128, 'り': 1129, '限': 1130, '珍': 1131, '加': 1132, '各': 1133, '已': 1134, '佳': 1135, 'ト': 1136, '部': 1137, '恋': 1138, '念': 1139, '话': 1140, '曹': 1141, '未': 1142, '希': 1143, 'ˆ': 1144, '皇': 1145, '长': 1146, '表': 1147, '時': 1148, '阿': 1149, '蒲': 1150, '計': 1151, '平': 1152, '创': 1153, '慈': 1154, '悠': 1155, 'ह': 1156, '莊': 1157, 'น': 1158, '散': 1159, '冰': 1160, '庭': 1161, 'ο': 1162, '巴': 1163, 'ك': 1164, 'ŏ': 1165, 'Š': 1166, '西': 1167, 'ז': 1168, '力': 1169, 'ゅ': 1170, 'ァ': 1171, '夢': 1172, '結': 1173, 'ʔ': 1174, 'ø': 1175, '示': 1176, '包': 1177, '厥': 1178, '二': 1179, '里': 1180, '社': 1181, '想': 1182, '友': 1183, 'ʒ': 1184, 'タ': 1185, 'ń': 1186, '战': 1187, 'ǚ': 1188, '幕': 1189, 'ס': 1190, '個': 1191, '霜': 1192, '堂': 1193, 'X': 1194, '服': 1195, '洵': 1196, 'ẵ': 1197, '蘇': 1198, 'د': 1199, 'य': 1200, '田': 1201, '由': 1202, '気': 1203, 'ō': 1204, 'я': 1205, 'Ẹ': 1206, 'ὑ': 1207, 'н': 1208, '問': 1209, 'к': 1210, 'や': 1211, 'ף': 1212, '船': 1213, '等': 1214, 'ポ': 1215, '紅': 1216, '括': 1217, '邦': 1218, '领': 1219, '藏': 1220, 'Ι': 1221, '昌': 1222, '爆': 1223, 'ゆ': 1224, 'i': 1225, '細': 1226, 'デ': 1227, '極': 1228, 'غ': 1229, '糸': 1230, '池': 1231, '唐': 1232, '定': 1233, '2': 1234, 'Л': 1235, '分': 1236, '้': 1237, '政': 1238, 'ั': 1239, 'γ': 1240, 'げ': 1241, '贡': 1242, '異': 1243, '短': 1244, 'ἐ': 1245, 'È': 1246, '头': 1247, 'ὀ': 1248, 'ب': 1249, '賓': 1250, 'ة': 1251, '見': 1252, 'Ð': 1253, 'š': 1254, 'ḍ': 1255, 'P': 1256, '刀': 1257, 'ü': 1258, '楊': 1259, 'ă': 1260, 'μ': 1261, '桜': 1262, 'ὲ': 1263, '检': 1264, 'ν': 1265, '素': 1266, '法': 1267, '资': 1268, '昭': 1269, '旅': 1270, '父': 1271, '同': 1272, '激': 1273, '解': 1274, '遵': 1275, '惑': 1276, '实': 1277, '間': 1278, '密': 1279, '氏': 1280, '狂': 1281, '華': 1282, 'F': 1283, 'র': 1284, 'h': 1285, '乃': 1286, 'ř': 1287, '達': 1288, 'け': 1289, 'ğ': 1290, '皮': 1291, '假': 1292, '左': 1293, '藝': 1294, 'う': 1295, 'よ': 1296, '玲': 1297, '代': 1298, '霉': 1299, 'オ': 1300, '锺': 1301, '洗': 1302, 'श': 1303, 'ϵ': 1304, 'Ἀ': 1305, '据': 1306, '丶': 1307, '篇': 1308, '羌': 1309, '余': 1310, 'b': 1311, 'ド': 1312, '财': 1313, 'ま': 1314, '烏': 1315, 'र': 1316, 'っ': 1317, '窓': 1318, '町': 1319, 'で': 1320, 'が': 1321, '切': 1322, '制': 1323, '衣': 1324, '与': 1325, 'ç': 1326, '砲': 1327, '腿': 1328, '版': 1329, 'è': 1330, '离': 1331, '题': 1332, '逸': 1333, 'Η': 1334, 'め': 1335, 'M': 1336, '率': 1337, '礼': 1338, 'ῖ': 1339, '望': 1340, '掛': 1341, 'ἡ': 1342, '清': 1343, '吐': 1344, '丸': 1345, '协': 1346, '門': 1347, '质': 1348, 'त': 1349, '霍': 1350, 'W': 1351, '引': 1352, 'Е': 1353, '悪': 1354, 'Н': 1355, '9': 1356, 'ه': 1357, 'ǘ': 1358, '隋': 1359, '陈': 1360, '律': 1361, '他': 1362, '段': 1363, 'フ': 1364, 'ǽ': 1365, '安': 1366, 'ᜆ': 1367, 'ミ': 1368, '这': 1369, '垂': 1370, '好': 1371, '勒': 1372, 'ϒ': 1373, 'ứ': 1374, '野': 1375, 'Ț': 1376, '公': 1377, 'ś': 1378, '十': 1379, '车': 1380, 'ë': 1381, '自': 1382, '熊': 1383, 'ț': 1384, '策': 1385, '黃': 1386, '夫': 1387, '延': 1388, '軍': 1389, '你': 1390, '伏': 1391, '竜': 1392, '適': 1393, 'ớ': 1394, '神': 1395, 'ě': 1396, '吉': 1397, '新': 1398, '苏': 1399, '两': 1400, '流': 1401, 'в': 1402, 'ϰ': 1403, '三': 1404, '宫': 1405, '融': 1406, '土': 1407, 'ゼ': 1408, '拓': 1409, 'ن': 1410, '伯': 1411, '女': 1412, '估': 1413, '来': 1414, '量': 1415, '过': 1416, '絵': 1417, '结': 1418, '焙': 1419, 'ά': 1420, '<': 1421, '位': 1422, 'ʁ': 1423, '朱': 1424, '字': 1425, 'ὁ': 1426, 'ª': 1427, '帝': 1428, 'З': 1429, 'ź': 1430, '丹': 1431, '燕': 1432, 'ǐ': 1433, '柔': 1434, '纪': 1435, '小': 1436, '我': 1437, '衡': 1438, '均': 1439, '進': 1440, 'ϱ': 1441, '守': 1442, '節': 1443, '今': 1444, 'έ': 1445, 'ப': 1446, '人': 1447, '间': 1448, '霊': 1449, 'u': 1450, '单': 1451, '易': 1452, '孝': 1453, '杰': 1454, '工': 1455, '独': 1456, '映': 1457, '陳': 1458, '止': 1459, 'Π': 1460, '研': 1461, '株': 1462, 'о': 1463, '布': 1464, '非': 1465, '许': 1466, '际': 1467, '桓': 1468, '衫': 1469, '吕': 1470, '打': 1471, 'た': 1472, '油': 1473, '鹿': 1474, '伦': 1475, 'и': 1476, '年': 1477, '若': 1478, '永': 1479, 'б': 1480, 'п': 1481, '摩': 1482, 'ё': 1483, '卵': 1484, 'ϑ': 1485, '问': 1486, '逃': 1487, 'ت': 1488, 'ス': 1489, '普': 1490, '狐': 1491, '功': 1492, '具': 1493, '邪': 1494, '或': 1495, '外': 1496, 'w': 1497, '內': 1498, 'ə': 1499, 'ل': 1500, '铁': 1501, '麻': 1502, 'Β': 1503, 'ἥ': 1504, 'ϛ': 1505, '免': 1506, '学': 1507, '朝': 1508, '活': 1509, 'ラ': 1510, 'v': 1511, 'ם': 1512, 'Д': 1513, '峰': 1514, '意': 1515, 'Μ': 1516, '劉': 1517, '賢': 1518, '光': 1519, 'じ': 1520, '楽': 1521, 'Э': 1522, '益': 1523, 'Ô': 1524, 'ম': 1525, 'ろ': 1526, '所': 1527, '肉': 1528, '室': 1529, '高': 1530, '府': 1531, '东': 1532, '伸': 1533, '州': 1534, '孫': 1535, 'ἅ': 1536, '升': 1537, '元': 1538, '文': 1539, 'q': 1540, '近': 1541, '省': 1542, 'Ţ': 1543, 'χ': 1544, '革': 1545, '団': 1546, '露': 1547, 'ʿ': 1548, 'の': 1549, '食': 1550, '性': 1551, 'だ': 1552, '魂': 1553, '智': 1554, '動': 1555, '追': 1556, 'そ': 1557, '进': 1558, '魏': 1559, 'Ü': 1560, 'ἄ': 1561, '商': 1562, '豊': 1563, '采': 1564, 'ὴ': 1565, '地': 1566, '突': 1567, 'Ⓐ': 1568, '佛': 1569, 'ὡ': 1570, '振': 1571, '刘': 1572, 'ā': 1573, '场': 1574, 'х': 1575, 'ノ': 1576, '貞': 1577, '賦': 1578, '生': 1579, '海': 1580, '响': 1581, 'な': 1582, '尊': 1583, '就': 1584, '後': 1585, '对': 1586, '红': 1587, 'ら': 1588, 'リ': 1589, 'ῦ': 1590, 'ƒ': 1591, 'O': 1592, '考': 1593, '臣': 1594, '虎': 1595, 'द': 1596, 'ệ': 1597, '彦': 1598, '菌': 1599, '配': 1600, 'へ': 1601, '其': 1602, 'ᾳ': 1603, '伽': 1604, '蹤': 1605, 'â': 1606, 'n': 1607, '郁': 1608, '影': 1609, '看': 1610, 'ץ': 1611, '飯': 1612, '专': 1613, 'キ': 1614, '起': 1615, '倉': 1616, '需': 1617, '科': 1618, '騎': 1619, '讨': 1620, '乱': 1621, 'Ĥ': 1622, 'ʻ': 1623, '产': 1624, '子': 1625, '景': 1626, '綺': 1627, 'R': 1628, 'Í': 1629, '陵': 1630, '古': 1631, '固': 1632, 'チ': 1633, '鑑': 1634, '序': 1635, '׳': 1636, '五': 1637, 'Θ': 1638, 'Ŷ': 1639, '菇': 1640, '幸': 1641, 'ర': 1642, '蔣': 1643, 'ח': 1644, '贫': 1645, 'ὺ': 1646, '应': 1647, '谷': 1648, '祖': 1649, '草': 1650, 'ᾶ': 1651, 'ӧ': 1652, '修': 1653, 'ஸ': 1654, '蔡': 1655, 'ᜐ': 1656, 'ő': 1657, '禽': 1658, 'П': 1659, '中': 1660, '拿': 1661, '耀': 1662, '集': 1663, '条': 1664, 'τ': 1665, '士': 1666, '收': 1667, 'ό': 1668, 'a': 1669, '天': 1670, '6': 1671, 'ג': 1672, '萬': 1673, 'Ч': 1674, '此': 1675, 'ᜋ': 1676, '因': 1677, '容': 1678, '交': 1679, '崎': 1680, 'Ł': 1681, '威': 1682, '都': 1683, 'わ': 1684, '述': 1685, '緒': 1686, 'م': 1687, 'ɪ': 1688, '姓': 1689, 'ấ': 1690, '级': 1691, 'Ψ': 1692, '型': 1693, '្': 1694, 'Č': 1695, 'ί': 1696, '药': 1697, 'ň': 1698, '微': 1699, 'µ': 1700, '美': 1701, 'म': 1702, 'و': 1703, 'ã': 1704, ',': 1705, 'ʎ': 1706, 'δ': 1707, '่': 1708, 'p': 1709, '哀': 1710, '系': 1711, '鳳': 1712, '矛': 1713, '當': 1714, '烈': 1715, 'ὰ': 1716, '明': 1717, '尾': 1718, 'й': 1719, 'ச': 1720, 'ἤ': 1721, 'ἃ': 1722, 'ブ': 1723, '緑': 1724, '匈': 1725, '錢': 1726, '忠': 1727, '敏': 1728, '民': 1729, '隊': 1730, '导': 1731, '蓮': 1732, '私': 1733, 'ɾ': 1734, '云': 1735, '消': 1736, '院': 1737, '沖': 1738, 'ぬ': 1739, 'E': 1740, '卦': 1741, '良': 1742, 'ý': 1743, 'ς': 1744, '象': 1745, 'ぶ': 1746, '宇': 1747, '评': 1748, '乳': 1749, '梅': 1750, '_': 1751, '戸': 1752, '訓': 1753, '方': 1754, '淑': 1755, '持': 1756, '羅': 1757, '令': 1758, '万': 1759, 'ἱ': 1760, '店': 1761, '悟': 1762, '该': 1763, '善': 1764, '南': 1765, 'Ἰ': 1766, 'ύ': 1767, '々': 1768, '童': 1769, 'Q': 1770, 'ど': 1771, '预': 1772, 'מ': 1773, '源': 1774, '姿': 1775, '困': 1776, 'ى': 1777, '跤': 1778, '至': 1779, 'ş': 1780, '而': 1781, 'ᜀ': 1782, '死': 1783, '黄': 1784, '巻': 1785, 'ὶ': 1786, '局': 1787, '臨': 1788, 'і': 1789, '甫': 1790, '仮': 1791, 'ῷ': 1792, 'と': 1793, 'び': 1794, '建': 1795, '學': 1796, 'Ь': 1797, '兴': 1798, 'Ф': 1799, '語': 1800, '卷': 1801, '台': 1802, '菊': 1803, '完': 1804, '聖': 1805, 'ヨ': 1806, '銀': 1807, 'く': 1808, '杜': 1809, '本': 1810, 'č': 1811, '金': 1812, '浪': 1813, '慎': 1814, '育': 1815, '棒': 1816, '甲': 1817, 'ᜇ': 1818, '汽': 1819, '絶': 1820, 'Δ': 1821, '督': 1822, '替': 1823, '賊': 1824, '描': 1825, '历': 1826, '极': 1827, '차': 1828, '桐': 1829, 'İ': 1830, 'డ': 1831, '荷': 1832, '目': 1833, 'ŷ': 1834, '时': 1835, '施': 1836, '梨': 1837, '宾': 1838, '值': 1839, '记': 1840, 'j': 1841, 'व': 1842, 'ぐ': 1843, '失': 1844, '联': 1845, 'أ': 1846, '關': 1847, '津': 1848, '膨': 1849, 'ǔ': 1850, 'έ': 1851, '批': 1852, '瀬': 1853, 'ស': 1854, '長': 1855, '第': 1856, 'D': 1857, '勲': 1858, 'ά': 1859, '曜': 1860, 'ょ': 1861, '则': 1862, 'த': 1863, '叶': 1864, '立': 1865, 'ł': 1866, '傳': 1867, '宋': 1868, '道': 1869, 'ὃ': 1870, '信': 1871, 'ี': 1872, 'É': 1873, '綿': 1874, '身': 1875, '琴': 1876, '編': 1877, '作': 1878, '兰': 1879, '寧': 1880, 'ᜅ': 1881, 'ɑ': 1882, '射': 1883, '份': 1884, 'ơ': 1885, '綠': 1886, '試': 1887, '返': 1888, 'U': 1889, 'ι': 1890, '宝': 1891, 'ハ': 1892, '胡': 1893, '观': 1894, '椿': 1895, '行': 1896, 'ḫ': 1897, '單': 1898, 'ἀ': 1899, '横': 1900, '宮': 1901, '厳': 1902, '族': 1903, '盾': 1904, '索': 1905, 'ὅ': 1906, '奴': 1907, '優': 1908, 'ů': 1909, '満': 1910, 'g': 1911, 'ℏ': 1912, '们': 1913, '雲': 1914, 'Λ': 1915, 'À': 1916, 'Ω': 1917, '著': 1918, '卍': 1919, '音': 1920, '快': 1921, '敬': 1922, 'Ç': 1923, 'ἔ': 1924, 'H': 1925, '降': 1926, '介': 1927, '湖': 1928, '付': 1929, 'Ο': 1930, '潮': 1931, '接': 1932, 'Z': 1933, '陰': 1934, '欠': 1935, '恩': 1936, '屠': 1937, '盟': 1938, 'ح': 1939, 'あ': 1940, '项': 1941, 'Ǻ': 1942, '师': 1943, 'A': 1944, 'ℜ': 1945, 'ظ': 1946, '8': 1947, '彼': 1948, '指': 1949, '世': 1950, '英': 1951, 'а': 1952, '够': 1953, '发': 1954, 'UNK': 1, 'PAD': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhqkuJWg3Qfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_tags = df_data_complete['Tag'].nunique()\n",
        "\n",
        "# Sentences into Words\n",
        "X_word_train = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in sentences_complete]\n",
        "\n",
        "X_word_train = pad_sequences(maxlen = max_len, sequences = X_word_train, padding = \"post\", dtype= np.float32)\n",
        "\n",
        "\n",
        "# Words into Chars\n",
        "X_char_train = []\n",
        "for sentence in sentences_complete:\n",
        "    sent_seq = []\n",
        "    for i in range(max_len): # max sequence length: 75\n",
        "        word_seq = []\n",
        "        for j in range(max_len_char): # max word length 10\n",
        "          try:\n",
        "              word_seq.append(char_to_idx.get(sentence[i][0][j],1))\n",
        "          except:\n",
        "              word_seq.append(char_to_idx.get(\"PAD\"))\n",
        "        sent_seq.append(word_seq)\n",
        "    X_char_train.append(np.array(sent_seq))\n",
        "\n",
        "# Tags\n",
        "y_train = [[tag_to_index[w[2]] for w in s] for s in sentences_complete]\n",
        "y_train = pad_sequences(maxlen = max_len, sequences = y_train, padding = \"post\", value = tag_to_index[\"PAD\"])\n",
        "\n",
        "\n",
        "# One hot encoded labels\n",
        "y_train = [to_categorical(i, num_classes = num_tags + 1) for i in y_train]\n",
        "\n",
        "# Split train set into train and validation\n",
        "#X_word_train2, X_word_valid, y_train2, y_valid = train_test_split(X_word_train, y_train, test_size=0.2, random_state=0)\n",
        "#X_char_train2, X_char_valid, _, _ = train_test_split(X_char_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQCZ1HM12KC",
        "colab_type": "text"
      },
      "source": [
        "##### **Feature preparation**\n",
        "**Test Set**\n",
        "\n",
        "Similarly as in approach 2, there is not enough RAM memory to store the word embeddings of the 5232 clinical cases. That is why, this procedure is repeated for every subset of 1000 clinical cases and the predictions obtained for eveyr subset is stored in different foulders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o21287nHY2sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_word_train = 0\n",
        "X_char_train = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JGg4GtD4lX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_word_test_cc = 0\n",
        "X_char_test_cc = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWeSr8xe4nfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2b56a0e4-119f-4d92-a081-86260de3c7d0"
      },
      "source": [
        "sentences_test_by_cc_subset = sentences_test_by_cc.copy()\n",
        "sentences_test_by_cc_subset = sentences_test_by_cc_subset[:1000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[1000:2000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[2000:3000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[3000:4000]\n",
        "#sentences_test_by_cc_subset = sentences_test_by_cc_subset[4000:]\n",
        "print(len(sentences_test_by_cc_subset))\n",
        "print(sentences_test_by_cc_subset[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1232\n",
            "[('Paciente', 'PROPN', 0), ('de', 'ADP', 9), ('89', 'NUM', 12), ('años', 'NOUN', 15), ('con', 'ADP', 20), ('clínica', 'NOUN', 24), ('de', 'ADP', 32), ('abdomen', 'NOUN', 35), ('agudo', 'ADJ', 43), ('.', 'PUNCT', 48)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFOzXmsM3z8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST SET (done by clinical case)\n",
        "\n",
        "# Converting each sentence into list of index from list of tokens\n",
        "X_word_test_cc = []\n",
        "X_char_test_cc = []\n",
        "start_char_test_cc = []\n",
        "token_test_cc = []\n",
        "\n",
        "for cc in sentences_test_by_cc_subset:\n",
        "  # Sentences into Words\n",
        "  x_i = [[modif_get_vector(scielo_wiki_model,w[0]) for w in s] for s in cc]\n",
        "  start_i = [[w[2] for w in s] for s in cc]\n",
        "  token_i = [[w[0] for w in s] for s in cc]\n",
        "\n",
        "  # Padding each sequence to have same length  of each word\n",
        "  X_word_test_cc.append(pad_sequences(maxlen = max_len, sequences = x_i, padding = \"post\", dtype= np.float32))\n",
        "\n",
        "  # Words into Chars\n",
        "  x_char_i = word_to_chars(cc, max_len, max_len_char, char_to_idx)\n",
        "  X_char_test_cc.append(x_char_i)\n",
        "\n",
        "  start_char_test_cc.append(pad_sequences(maxlen=max_len, sequences = start_i, value=-1, padding=\"post\", truncating=\"post\"))\n",
        "  token_test_cc.append(pad_sequences(maxlen=max_len, sequences = token_i, value=\"PAD\", padding=\"post\", truncating=\"post\",dtype= object))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoCoiWQRWFoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d62fae3-2891-4f94-f42c-969df2b6691f"
      },
      "source": [
        "len(X_word_test_cc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSR5xn3ORxPn",
        "colab_type": "text"
      },
      "source": [
        "##### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NnSzBjGW7Up",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "a693bb56-61ce-4bd3-8496-380e145b6f24"
      },
      "source": [
        "num_tags = df_data_complete['Tag'].nunique()\n",
        "\n",
        "# Model architecture\n",
        "\n",
        "# Inputs: (for word and character embedding)\n",
        "#word_in = Input(shape=(max_len,))\n",
        "word_in = Input(shape = X_word_train[0].shape)\n",
        "char_in = Input(shape=(max_len, max_len_char,))\n",
        "# max_len defines the number of words that with be considered as input to the model (can be considered as a window)\n",
        "# 2D tensor with shape: (batch_size, input_length). --> <tf.Tensor 'input_5:0' shape=(None, 75) dtype=float32>\n",
        "\n",
        "# 1. Embedding layer for words\n",
        "emb_word = word_in\n",
        "# output_shape: (batch_size, input_length, output_dim) --> (None, 75, 40)\n",
        "\n",
        "# 2. Embedding and Time distributed layers for characters # output_dim=32\n",
        "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=32, input_length=max_len_char, mask_zero=False))(char_in)\n",
        "\n",
        "# 3. Character LSTM to get word encodings by characters UNIDIRECTIONAL\n",
        "char_enc = TimeDistributed(LSTM(units=32, return_sequences=False, recurrent_dropout=0.5))(emb_char)\n",
        "\n",
        "\n",
        "# 4. Bidirecitonal LSTM\n",
        "# first, create feature map formed by word embeddings and character encodings\n",
        "features = concatenate([emb_word, char_enc])\n",
        "features = SpatialDropout1D(0.3)(features)\n",
        "\n",
        "bi_lstm = Bidirectional(LSTM(units=256, return_sequences=True, recurrent_dropout=0.6))(features)\n",
        "# output shape: (None, 75, 100) --> output_dim is 100 because its concatenating \n",
        "# the outputs of the forward LSTM and backward LSTM instead of combining them\n",
        "\n",
        "# 5. Time Distributed layer \n",
        "#out = TimeDistributed(Dense(num_tags + 1, activation=\"sigmoid\"))(bi_lstm)\n",
        "#out = TimeDistributed(Dense(50, activation=\"sigmoid\"))(bi_lstm)\n",
        "out = TimeDistributed(Dense(50, activation=\"relu\"))(bi_lstm)\n",
        "\n",
        "# combines the 100 outputs of the time stamps specified by index (75) into 50 values using sigmoid function. \n",
        "# try: activation=\"relu\"\n",
        "\n",
        "## 6. CRF layer (the classifier) # PREVIOUS APPROACH\n",
        "crf = CRF(num_tags+1)  \n",
        "out = crf(out)  # output\n",
        "\n",
        "# 6. Instantiate the model; out must be the final layer\n",
        "model = Model([word_in, char_in], out)\n",
        "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy]) # PREVIOUS APPROACH\n",
        "#model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy]) # PREVIOUS APPROACH\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 75, 10)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 75, 10, 32)   62560       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 75, 300)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 75, 32)       8320        time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 75, 332)      0           input_2[0][0]                    \n",
            "                                                                 time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 75, 332)      0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 75, 512)      1206272     spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 75, 50)       25650       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "crf_2 (CRF)                     (None, 75, 7)        420         time_distributed_4[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,303,222\n",
            "Trainable params: 1,303,222\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_uguEsfdW7Ut",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "9e17c8d1-8074-477e-88af-bd9475430000"
      },
      "source": [
        "random.seed(0)\n",
        "history = model.fit([X_word_train,\n",
        "                     np.array(X_char_train).reshape((len(X_char_train), max_len, max_len_char))],\n",
        "                    np.array(y_train),\n",
        "                    batch_size=64, epochs=8, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 33957 samples, validate on 3773 samples\n",
            "Epoch 1/8\n",
            "33957/33957 [==============================] - 853s 25ms/step - loss: 0.0369 - crf_viterbi_accuracy: 0.9908 - val_loss: 0.0136 - val_crf_viterbi_accuracy: 0.9948\n",
            "Epoch 2/8\n",
            "33957/33957 [==============================] - 861s 25ms/step - loss: 0.0090 - crf_viterbi_accuracy: 0.9951 - val_loss: 0.0017 - val_crf_viterbi_accuracy: 0.9962\n",
            "Epoch 3/8\n",
            "33957/33957 [==============================] - 862s 25ms/step - loss: -0.0021 - crf_viterbi_accuracy: 0.9958 - val_loss: -0.0058 - val_crf_viterbi_accuracy: 0.9951\n",
            "Epoch 4/8\n",
            "33957/33957 [==============================] - 854s 25ms/step - loss: -0.0113 - crf_viterbi_accuracy: 0.9963 - val_loss: -0.0149 - val_crf_viterbi_accuracy: 0.9963\n",
            "Epoch 5/8\n",
            "33957/33957 [==============================] - 856s 25ms/step - loss: -0.0194 - crf_viterbi_accuracy: 0.9966 - val_loss: -0.0218 - val_crf_viterbi_accuracy: 0.9950\n",
            "Epoch 6/8\n",
            "33957/33957 [==============================] - 856s 25ms/step - loss: -0.0271 - crf_viterbi_accuracy: 0.9968 - val_loss: -0.0303 - val_crf_viterbi_accuracy: 0.9965\n",
            "Epoch 7/8\n",
            "33957/33957 [==============================] - 858s 25ms/step - loss: -0.0346 - crf_viterbi_accuracy: 0.9970 - val_loss: -0.0376 - val_crf_viterbi_accuracy: 0.9967\n",
            "Epoch 8/8\n",
            "33957/33957 [==============================] - 852s 25ms/step - loss: -0.0421 - crf_viterbi_accuracy: 0.9971 - val_loss: -0.0446 - val_crf_viterbi_accuracy: 0.9966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YoxUdpwW7Uv",
        "colab": {}
      },
      "source": [
        "hist = pd.DataFrame(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64FLkP5iVDaa",
        "colab_type": "text"
      },
      "source": [
        "##### **Test set predictions**\n",
        "\n",
        "Done for every subset in the test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnlAbc7aVUYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words_test(tokens_sent,label_sent, start_char_pos):\n",
        "  new_tok, new_lab, new_start_pos = [], [], []\n",
        "\n",
        "  for tokens, labels, start_chars in zip(tokens_sent, label_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, new_start_pos_aux = [], [], []\n",
        "    for token, label, start_char_i in zip(tokens, labels,start_chars):\n",
        "      if token != \"PAD\":\n",
        "        new_tok_aux.append(token)\n",
        "        new_lab_aux.append(label)\n",
        "        new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sAvEp7_MW7U_",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, new_start_pos_cc = [], [], []\n",
        "new_tokens_all, new_labels_all, new_start_pos_all = [], [], []\n",
        "\n",
        "label_indices_sent_cc = []\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(X_word_test_cc)):\n",
        "  y_pred_test = model.predict([X_word_test_cc[cc], np.array(X_char_test_cc[cc]).reshape((len(X_char_test_cc[cc]),max_len, max_len_char))])\n",
        "  y_pred_test = np.argmax(y_pred_test, axis=-1)\n",
        "\n",
        "  # Convert the index to tag\n",
        "  y_pred_test = [[idx2tag[i] for i in row] for row in y_pred_test]\n",
        "\n",
        "  new_tokens, new_labels, new_start_pos = [], [], []\n",
        "  new_tokens, new_labels, new_start_pos = tokens_to_words_test(token_test_cc[cc], \n",
        "                                        y_pred_test, start_char_test_cc[cc])\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBLdE_VdVNQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(path+'results_BILSTM_ap3/predictions/subset5/new_tokens_cc', 'wb') as file: \n",
        "  pkl.dump(new_tokens_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_BILSTM_ap3/predictions/subset5/new_labels_cc', 'wb') as file: \n",
        "  pkl.dump(new_labels_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_BILSTM_ap3/predictions/subset5/new_start_pos_cc', 'wb') as file: \n",
        "  pkl.dump(new_start_pos_cc, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}