{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_by_BERT_Cantemist_Competicion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5Mt4hDPDvdSb",
        "rmzP6w1fm12P",
        "q3V94jR3E2le",
        "py9ypwdVq_WI",
        "ZL42AaeAH20L",
        "jVkEhyA6b_z7",
        "6iOBKwlkYZ_M",
        "U5A6Kl5nDZXz",
        "j2wXpeGzrUMA",
        "EY3sOtK6hf8N",
        "jBL5owMwhB17",
        "PfFk461tWvRU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e686aadbc574d51a554bc67aade45b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_30ada709ed804cf2a16983ea7427a95c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_73669580003a423faff028c1ec361774",
              "IPY_MODEL_e8b474c3f801424abbeea991151ec250"
            ]
          }
        },
        "30ada709ed804cf2a16983ea7427a95c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73669580003a423faff028c1ec361774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3651e775915a4ac8bbae292fe328a112",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_271a7a69c51145d1a652d2b4a9ab4d29"
          }
        },
        "e8b474c3f801424abbeea991151ec250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f78276dd9f3d4944b2d2157adc150726",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:01&lt;00:00, 950kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38a16333698040c280a2d6c194fdf766"
          }
        },
        "3651e775915a4ac8bbae292fe328a112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "271a7a69c51145d1a652d2b4a9ab4d29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f78276dd9f3d4944b2d2157adc150726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38a16333698040c280a2d6c194fdf766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6501d2d63566470c9f6d941b4556fe2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7c82f71d366b438daaec61bd26f8871d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_224f4bcfe37740dd913133f33b25a828",
              "IPY_MODEL_6f755c976ecd4430a6d115c6fa5a724b"
            ]
          }
        },
        "7c82f71d366b438daaec61bd26f8871d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "224f4bcfe37740dd913133f33b25a828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_504ffb750fd945c8a97283d24befb4ab",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08ea088dde694039a49fea6a5fd5d01f"
          }
        },
        "6f755c976ecd4430a6d115c6fa5a724b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51454878fb7a409cbfd59111f8b5e840",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [01:02&lt;00:00, 10.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01a63dc4b1924857be938cb48f8f15bb"
          }
        },
        "504ffb750fd945c8a97283d24befb4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08ea088dde694039a49fea6a5fd5d01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51454878fb7a409cbfd59111f8b5e840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01a63dc4b1924857be938cb48f8f15bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c24ce23cb8274e19b9a6d7b5bdc24a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3d77fb17d755489296c11d49ed537e04",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_34dc10e0f1b44f66a8711e99b4643870",
              "IPY_MODEL_f70a0abd2caf45d7aaa2ac6871baa2be"
            ]
          }
        },
        "3d77fb17d755489296c11d49ed537e04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34dc10e0f1b44f66a8711e99b4643870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8fc3719a2e9d493b9b9c49bc75c3e3b9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1be172aa77b439aa71b131a86e3141f"
          }
        },
        "f70a0abd2caf45d7aaa2ac6871baa2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0725e316073d4940b652d93ca6cc2666",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 714M/714M [01:00&lt;00:00, 11.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f037c9fc1bb4997a7258060032743fb"
          }
        },
        "8fc3719a2e9d493b9b9c49bc75c3e3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1be172aa77b439aa71b131a86e3141f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0725e316073d4940b652d93ca6cc2666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f037c9fc1bb4997a7258060032743fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18d49402014c4f5abaf21393b659cc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c9b34ea9d3bc4cebbf99fe9e42b96e6a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_548f2b2a4fb4414eb7a4afdb99186a7e",
              "IPY_MODEL_fbea8b2da498496fad36d4f71190e194"
            ]
          }
        },
        "c9b34ea9d3bc4cebbf99fe9e42b96e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "548f2b2a4fb4414eb7a4afdb99186a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aaf93b3abb124b999275d7f2fcc0f70e",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ad22626dea04f09acf932c3f2d6f2c6"
          }
        },
        "fbea8b2da498496fad36d4f71190e194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_413f241afef94b8f8bd02d9fd7dd7f5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [1:02:00&lt;00:00, 1240.25s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7253fdd42014bde92b3645168efe113"
          }
        },
        "aaf93b3abb124b999275d7f2fcc0f70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ad22626dea04f09acf932c3f2d6f2c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "413f241afef94b8f8bd02d9fd7dd7f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7253fdd42014bde92b3645168efe113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fju-V7DpAgm",
        "colab_type": "text"
      },
      "source": [
        "# **NER_by_BERT**\n",
        "\n",
        "## **Author:** Gema De Vargas Romero\n",
        "\n",
        "## **Master Thesis:** \"Development of a Named Entity Recognition System to automatically assign tumor morphology entity mentions to health-related documents in Spanish.\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05N_SwgGTL6t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "83f20d2c-c8f8-4eab-e99b-b0dc245c9959"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "path='drive/My Drive/Ejemplos NER - TFM/'\n",
        "!ls 'drive/My Drive/Ejemplos NER - TFM/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            " bert\n",
            " data\n",
            " dev_set\n",
            " dev_set2\n",
            "'Dictionary based NER (spacy).ipynb'\n",
            "'Ehealth_Dictionary based NER (spacy).ipynb'\n",
            " last_step_cantemist.ipynb\n",
            " last_step_cantemist_TEST.ipynb\n",
            " NER_by_BERT_Cantemist_BIOESV.ipynb\n",
            " NER_by_BERT_Cantemist_Competicion.ipynb\n",
            " NER_by_BERT_Cantemist.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist_BIOESV_2.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist_BIOESV.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist_Competicion.ipynb\n",
            " NER_by_BI_LSTM_CRF_Cantemist.ipynb\n",
            " NER_by_CRF_Cantemist_Competicion.ipynb\n",
            " NER_by_CRF_Cantemist.ipynb\n",
            " NER_by_CRF_Ehealth.ipynb\n",
            " NER_by_CRF.ipynb\n",
            " Preprocessing_NER_Cantemist.ipynb\n",
            " resources\n",
            " results_bert\n",
            " results_bert2\n",
            " results_BILSTM_ap1\n",
            " results_BILSTM_ap2\n",
            " results_BILSTM_ap3\n",
            " results_CRF\n",
            " sample_set\n",
            " Scielo+Wiki_skipgram_cased.bin\n",
            " Scielo+Wiki_skipgram_cased.vec\n",
            " test-background-set-to-publish\n",
            " train_set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mt4hDPDvdSb",
        "colab_type": "text"
      },
      "source": [
        "## **Loading libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scn6jrRSUoKx",
        "colab_type": "text"
      },
      "source": [
        "First, we must load the libraries that we will use in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtRt1ONNabh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "56831394-d2fc-4087-c916-ddf2766377bd"
      },
      "source": [
        "!pip install sklearn-crfsuite\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite.metrics import flat_f1_score\n",
        "from sklearn_crfsuite.metrics import flat_precision_score\n",
        "from sklearn_crfsuite.metrics import flat_recall_score\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "#import nltk\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzcWUWl2aWqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34079a67-e71a-47c2-a99c-4f3ed0b9c8bc"
      },
      "source": [
        "# Library spacy\n",
        "!pip install -U spacy \n",
        "#!python -m spacy validate\n",
        "!python -m spacy download es_core_news_lg\n",
        "\n",
        "import spacy\n",
        "\n",
        "# nlp = spacy.load(\"es\") # no longer works with updated version of spacy 2.3.1\n",
        "import es_core_news_lg\n",
        "nlp = es_core_news_lg.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Collecting thinc==7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.3.2 thinc-7.4.1\n",
            "Collecting es_core_news_lg==2.3.1\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-2.3.1/es_core_news_lg-2.3.1.tar.gz (573.1MB)\n",
            "\u001b[K     |████████████████████████████████| 573.1MB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from es_core_news_lg==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (49.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (0.7.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.1.0)\n",
            "Building wheels for collected packages: es-core-news-lg\n",
            "  Building wheel for es-core-news-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-lg: filename=es_core_news_lg-2.3.1-cp36-none-any.whl size=573139081 sha256=ede5718f5d0f5c53ccbcacdc5f445898c6e041a521919eb01c0e2638782fec8a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9e6puq8v/wheels/48/59/33/558e7f48e924c6cac0cbd3679ee7c84f5ae02964c335232e5a\n",
            "Successfully built es-core-news-lg\n",
            "Installing collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tagsZfDrd1lM"
      },
      "source": [
        "## **Read the files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FpxWth4pd1lN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "18d1a25e-0bc9-40a9-dd3f-5ecfc70a37a2"
      },
      "source": [
        "!ls 'drive/My Drive/Ejemplos NER - TFM/data'\n",
        "import pickle as pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_data_complete.csv  files_txt_dev\t   sentences_dev_by_cc2\n",
            "df_data_dev2_2.csv    files_txt_test\t   sentences_test\n",
            "df_data_dev2.csv      sentences_dev\t   sentences_test_by_cc\n",
            "df_data_test.csv      sentences_dev2\t   sentences_train\n",
            "df_data_train2.csv    sentences_dev_by_cc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj1_4U9IeC-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train dataset\n",
        "with open(path+'data/sentences_train', 'rb') as file: \n",
        "  sentences_train = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_train2 = pd.read_csv(path+'data/df_data_train2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_LAMsMgeIY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Development dataset 1\n",
        "with open(path+'data/sentences_dev', 'rb') as file: \n",
        "  sentences_dev = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'data/sentences_dev_by_cc', 'rb') as file: \n",
        "  sentences_dev_by_cc = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_dev2 = pd.read_csv(path+'data/df_data_dev2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ImCDQ44Bd1lP",
        "colab": {}
      },
      "source": [
        "# Development dataset 2\n",
        "with open(path+'data/sentences_dev2', 'rb') as file: \n",
        "  sentences_dev2 = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'data/sentences_dev_by_cc2', 'rb') as file: \n",
        "  sentences_dev_by_cc2 = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_dev2_2 = pd.read_csv(path+'data/df_data_dev2_2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANP2WPx49-S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST FILES:\n",
        "with open(path+'data/sentences_test', 'rb') as file: \n",
        "  sentences_test = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'data/sentences_test_by_cc', 'rb') as file: \n",
        "  sentences_test_by_cc = pkl.load(file)\n",
        "file.close()\n",
        "\n",
        "df_data_test = pd.read_csv(path+'data/df_data_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFUPmmADoRUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_train2 = df_data_train2.drop_duplicates(['File_Index','Sentence_Index','Word','start'], keep = 'first')\n",
        "df_data_dev2 = df_data_dev2.drop_duplicates(['File_Index','Sentence_Index','Word','start'], keep = 'first')\n",
        "df_data_dev2_2 = df_data_dev2_2.drop_duplicates(['File_Index','Sentence_Index','Word','start'], keep = 'first')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYFCocs9YC65",
        "colab_type": "text"
      },
      "source": [
        "## **Construct Complete dataset. Combine train and development sets 1 and 2**\n",
        "\n",
        "This will construct the full dataframe that will be employed to train the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF5nvuAbkts4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d1529e0e-8f61-4e5e-b56d-723bc43dbb5a"
      },
      "source": [
        "df_data_train2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Index</th>\n",
              "      <th>Sentence_Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>start</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ANAMNESIS</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n</td>\n",
              "      <td>SPACE</td>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mujer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>16</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>NUM</td>\n",
              "      <td>19</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454525</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>deshidrogenasa</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>7856</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454526</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>(</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>7871</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454527</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>SDHB</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>7872</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454528</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>)</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>7876</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454529</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>7877</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>454358 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        File_Index  Sentence_Index            Word    POS  start Tag\n",
              "0                1               1       ANAMNESIS   NOUN      0   O\n",
              "1                1               1              \\n  SPACE      9   O\n",
              "2                1               1           Mujer   NOUN     10   O\n",
              "3                1               1              de    ADP     16   O\n",
              "4                1               1              67    NUM     19   O\n",
              "...            ...             ...             ...    ...    ...  ..\n",
              "454525         501           19502  deshidrogenasa  PROPN   7856   O\n",
              "454526         501           19502               (  PUNCT   7871   O\n",
              "454527         501           19502            SDHB  PROPN   7872   O\n",
              "454528         501           19502               )  PUNCT   7876   O\n",
              "454529         501           19502               .  PUNCT   7877   O\n",
              "\n",
              "[454358 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXJEvgz8kVBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2996dd8f-a3d5-4cd0-a201-17ccd3350ec7"
      },
      "source": [
        "file_idx = df_data_train2['File_Index'].iloc[-1]\n",
        "sent_idx = df_data_train2['Sentence_Index'].iloc[-1]\n",
        "file_idx2 = df_data_dev2['File_Index'].iloc[-1]\n",
        "sent_idx2 = df_data_dev2['Sentence_Index'].iloc[-1]\n",
        "\n",
        "print(file_idx)\n",
        "print(sent_idx)\n",
        "print(file_idx2)\n",
        "print(sent_idx2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "501\n",
            "19502\n",
            "250\n",
            "9546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk5JY-jGlYCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_dev2_aux = df_data_dev2.copy()\n",
        "df_data_dev2_2_aux = df_data_dev2_2.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x-QhhrWlN4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_dev2_aux['File_Index'] = df_data_dev2['File_Index'] + file_idx\n",
        "df_data_dev2_aux['Sentence_Index'] = df_data_dev2['Sentence_Index'] + sent_idx\n",
        "\n",
        "df_data_dev2_2_aux['File_Index'] = df_data_dev2_2['File_Index'] + file_idx + file_idx2\n",
        "df_data_dev2_2_aux['Sentence_Index'] = df_data_dev2_2['Sentence_Index'] + sent_idx + sent_idx2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTLjBEfUnoEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "b9c71c98-bda2-4ecf-ed43-3c153191a094"
      },
      "source": [
        "df_data_dev2_2_aux"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Index</th>\n",
              "      <th>Sentence_Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>start</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>752</td>\n",
              "      <td>29049</td>\n",
              "      <td>Anamnesis</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>752</td>\n",
              "      <td>29049</td>\n",
              "      <td>\\n</td>\n",
              "      <td>SPACE</td>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>752</td>\n",
              "      <td>29049</td>\n",
              "      <td>Mujer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>752</td>\n",
              "      <td>29049</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>16</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>752</td>\n",
              "      <td>29049</td>\n",
              "      <td>60</td>\n",
              "      <td>NUM</td>\n",
              "      <td>19</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183180</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>exitus</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5646</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183181</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5653</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183182</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>la</td>\n",
              "      <td>DET</td>\n",
              "      <td>5656</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183183</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>paciente</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5659</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183184</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>5667</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>183056 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        File_Index  Sentence_Index       Word    POS  start Tag\n",
              "0              752           29049  Anamnesis   NOUN      0   O\n",
              "1              752           29049         \\n  SPACE      9   O\n",
              "2              752           29049      Mujer   NOUN     10   O\n",
              "3              752           29049         de    ADP     16   O\n",
              "4              752           29049         60    NUM     19   O\n",
              "...            ...             ...        ...    ...    ...  ..\n",
              "183180        1001           37730     exitus   NOUN   5646   O\n",
              "183181        1001           37730         de    ADP   5653   O\n",
              "183182        1001           37730         la    DET   5656   O\n",
              "183183        1001           37730   paciente   NOUN   5659   O\n",
              "183184        1001           37730          .  PUNCT   5667   O\n",
              "\n",
              "[183056 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v66WyIHuYUdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_complete = df_data_train2.append(df_data_dev2_aux, ignore_index=True)\n",
        "df_data_complete = df_data_complete.append(df_data_dev2_2_aux, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohpknFolYvdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "76537b7a-6e1d-482f-9956-d4bd210d54d8"
      },
      "source": [
        "df_data_complete"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Index</th>\n",
              "      <th>Sentence_Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>start</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ANAMNESIS</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n</td>\n",
              "      <td>SPACE</td>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mujer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>16</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>NUM</td>\n",
              "      <td>19</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862674</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>exitus</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5646</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862675</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5653</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862676</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>la</td>\n",
              "      <td>DET</td>\n",
              "      <td>5656</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862677</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>paciente</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5659</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862678</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>5667</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>862679 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        File_Index  Sentence_Index       Word    POS  start Tag\n",
              "0                1               1  ANAMNESIS   NOUN      0   O\n",
              "1                1               1         \\n  SPACE      9   O\n",
              "2                1               1      Mujer   NOUN     10   O\n",
              "3                1               1         de    ADP     16   O\n",
              "4                1               1         67    NUM     19   O\n",
              "...            ...             ...        ...    ...    ...  ..\n",
              "862674        1001           37730     exitus   NOUN   5646   O\n",
              "862675        1001           37730         de    ADP   5653   O\n",
              "862676        1001           37730         la    DET   5656   O\n",
              "862677        1001           37730   paciente   NOUN   5659   O\n",
              "862678        1001           37730          .  PUNCT   5667   O\n",
              "\n",
              "[862679 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiMpI_HXppaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_complete.to_csv(path+'data/df_data_complete.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxfGYD_vEOkL",
        "colab_type": "text"
      },
      "source": [
        "## **Model 3: BERT**\n",
        "\n",
        "https://medium.com/dair-ai/beto-spanish-bert-420e4860d2c6\n",
        "\n",
        "https://github.com/dccuchile/beto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmzP6w1fm12P",
        "colab_type": "text"
      },
      "source": [
        "#### **Loading libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq0_T1cFEtjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "0c77c3a9-6753-42c1-db53-5c133b1e8e21"
      },
      "source": [
        "!pip install transformers==2.6.0\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.__version__\n",
        "\n",
        "import transformers\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "\n",
        "transformers.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.14.30)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.6.20)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.30 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.17.30)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.30->boto3->transformers==2.6.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.30->boto3->transformers==2.6.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a556b6d0b903372693432d30bc55033fa3d13d577507dab574fdd17c633d66bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3V94jR3E2le",
        "colab_type": "text"
      },
      "source": [
        "#### **Model parameters**\n",
        "\n",
        "Recommended parameters by the paper:\n",
        "\n",
        "- batch size: 16, 32\n",
        "- Learning Rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "- Number of Epochs: 2, 3, 4\n",
        "\n",
        "Perform hyper-parameter tuning to find the best combination of these parameters. FUTURE WORK --> google cloud computing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JnwXf5RE1Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 75 # sequence length limited to 75 tokens \n",
        "batch_size = 32 # suggested 32\n",
        "# Bert supports sequences of up to 512 tokens."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwaOybr_E8cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c12ee546-5018-4031-93b9-e4eeb76dc2f2"
      },
      "source": [
        "# to work with GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25N92wRZoX4B",
        "colab_type": "text"
      },
      "source": [
        "#### **Preprocessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G47kZiLl9spL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class sentence(object):\n",
        "    def __init__(self, df):\n",
        "        self.n_sent = 1\n",
        "        self.df = df\n",
        "        self.empty = False\n",
        "        agg = lambda s : [(w, p, t, i) for w, p, t, i in zip(s['Word'].values.tolist(),\n",
        "                                                       s['POS'].values.tolist(),\n",
        "                                                       s['Tag'].values.tolist(),\n",
        "                                                       s['start'].values.tolist())]\n",
        "        self.grouped = self.df.groupby(\"Sentence_Index\").apply(agg)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "        \n",
        "    def get_text(self):\n",
        "        try:\n",
        "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
        "            self.n_sent +=1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMr4R-_79vyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a class to get sentence. The each sentence will be list of tuples with its tag and pos.\n",
        "class sentence_test(object):\n",
        "    def __init__(self, df):\n",
        "        self.n_sent = 1\n",
        "        self.df = df\n",
        "        self.empty = False\n",
        "        agg = lambda s : [(w, p, i) for w, p, i in zip(s['Word'].values.tolist(),\n",
        "                                                       s['POS'].values.tolist(),\n",
        "                                                      s['Start_Char_position'].values.tolist())]\n",
        "        self.grouped = self.df.groupby(\"Sentence_Index\").apply(agg)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "        \n",
        "    def get_text(self):\n",
        "        try:\n",
        "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
        "            self.n_sent +=1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TYykt6UE_fF",
        "colab_type": "text"
      },
      "source": [
        "**Prepare the sentences and labels**\n",
        "\n",
        "Before we can start fine-tuning the model, we have to prepare the data set for the use with pytorch and BERT.\n",
        "\n",
        "The sentences employed for BERT **sentences_bert** do not contain the POS and TAG values present in previous models because they will be tokenized by BERT. In fact, the corresponding TAGs of each sentence will be stores in **labels**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZUzUzmPZKG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "0b3f1793-619f-48c9-a0be-485630ded0ee"
      },
      "source": [
        "df_data_train2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Index</th>\n",
              "      <th>Sentence_Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>start</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ANAMNESIS</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n</td>\n",
              "      <td>SPACE</td>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mujer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>16</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>NUM</td>\n",
              "      <td>19</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454525</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>deshidrogenasa</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>7856</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454526</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>(</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>7871</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454527</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>SDHB</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>7872</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454528</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>)</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>7876</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454529</th>\n",
              "      <td>501</td>\n",
              "      <td>19502</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>7877</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>454358 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        File_Index  Sentence_Index            Word    POS  start Tag\n",
              "0                1               1       ANAMNESIS   NOUN      0   O\n",
              "1                1               1              \\n  SPACE      9   O\n",
              "2                1               1           Mujer   NOUN     10   O\n",
              "3                1               1              de    ADP     16   O\n",
              "4                1               1              67    NUM     19   O\n",
              "...            ...             ...             ...    ...    ...  ..\n",
              "454525         501           19502  deshidrogenasa  PROPN   7856   O\n",
              "454526         501           19502               (  PUNCT   7871   O\n",
              "454527         501           19502            SDHB  PROPN   7872   O\n",
              "454528         501           19502               )  PUNCT   7876   O\n",
              "454529         501           19502               .  PUNCT   7877   O\n",
              "\n",
              "[454358 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM_rjEbB9WuF",
        "colab_type": "text"
      },
      "source": [
        "##### **Complete dataset**\n",
        "\n",
        "combines train and development datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFZ_mEuP-8Ri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "7d6ec65c-97ee-4c7c-e58e-b2866c150405"
      },
      "source": [
        "df_data_complete"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Index</th>\n",
              "      <th>Sentence_Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>start</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ANAMNESIS</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n</td>\n",
              "      <td>SPACE</td>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mujer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>16</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>NUM</td>\n",
              "      <td>19</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862674</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>exitus</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5646</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862675</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5653</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862676</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>la</td>\n",
              "      <td>DET</td>\n",
              "      <td>5656</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862677</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>paciente</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>5659</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862678</th>\n",
              "      <td>1001</td>\n",
              "      <td>37730</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>5667</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>862679 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        File_Index  Sentence_Index       Word    POS  start Tag\n",
              "0                1               1  ANAMNESIS   NOUN      0   O\n",
              "1                1               1         \\n  SPACE      9   O\n",
              "2                1               1      Mujer   NOUN     10   O\n",
              "3                1               1         de    ADP     16   O\n",
              "4                1               1         67    NUM     19   O\n",
              "...            ...             ...        ...    ...    ...  ..\n",
              "862674        1001           37730     exitus   NOUN   5646   O\n",
              "862675        1001           37730         de    ADP   5653   O\n",
              "862676        1001           37730         la    DET   5656   O\n",
              "862677        1001           37730   paciente   NOUN   5659   O\n",
              "862678        1001           37730          .  PUNCT   5667   O\n",
              "\n",
              "[862679 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smwb3BsupUCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2b2a16c0-e9e9-4087-bda0-d112e099f3b7"
      },
      "source": [
        "# FULL DATASET (train and development sets combined)\n",
        "\n",
        "# sentences\n",
        "getter = sentence(df_data_complete)\n",
        "sentences_bert_complete = [[s[0] for s in sentence] for sentence in getter.sentences]\n",
        "print(sentences_bert_complete[0])\n",
        "\n",
        "# labels\n",
        "labels_bert_complete = [[s[2] for s in sentence] for sentence in getter.sentences]\n",
        "print(labels_bert_complete[0])\n",
        "\n",
        "# start char position\n",
        "start_bert_complete = [[s[3] for s in sentence] for sentence in getter.sentences]\n",
        "print(start_bert_complete[0])\n",
        "\n",
        "# Tags\n",
        "tag_values = list(set(df_data_complete[\"Tag\"].values))\n",
        "tag_values.append(\"PAD\")\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
        "print(tag_values)\n",
        "print(tag2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ANAMNESIS', '\\n', 'Mujer', 'de', '67', 'años', 'con', 'antecedentes', 'personales', 'de', 'hipotiroidismo', 'en', 'tratamiento', 'con', 'levotiroxina', 'y', 'fumadora', 'activa', 'de', '12.5', 'paquetes', '/', 'año', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 9, 10, 16, 19, 22, 27, 31, 44, 55, 58, 73, 76, 88, 92, 105, 107, 116, 123, 126, 131, 139, 140, 143]\n",
            "['O', 'S-MOR', 'B-MOR', 'E-MOR', 'V-MOR', 'I-MOR', 'PAD']\n",
            "{'O': 0, 'S-MOR': 1, 'B-MOR': 2, 'E-MOR': 3, 'V-MOR': 4, 'I-MOR': 5, 'PAD': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9jBTl5A9GUF",
        "colab_type": "text"
      },
      "source": [
        "##### **Train and development sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCBvrDmHFbcS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1db06bbc-c676-46ac-db90-7f91aef75d47"
      },
      "source": [
        "# TRAIN SET\n",
        "\n",
        "# sentences\n",
        "getter = sentence(df_data_train2)\n",
        "sentences_bert_train = [[s[0] for s in sentence] for sentence in getter.sentences]\n",
        "print(sentences_bert_train[0])\n",
        "\n",
        "# labels\n",
        "labels_bert_train = [[s[2] for s in sentence] for sentence in getter.sentences]\n",
        "print(labels_bert_train[0])\n",
        "\n",
        "# start char position\n",
        "start_bert_train = [[s[3] for s in sentence] for sentence in getter.sentences]\n",
        "print(start_bert_train[0])\n",
        "\n",
        "# Tags\n",
        "tag_values = list(set(df_data_train2[\"Tag\"].values))\n",
        "tag_values.append(\"PAD\")\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
        "print(tag_values)\n",
        "print(tag2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ANAMNESIS', '\\n', 'Mujer', 'de', '67', 'años', 'con', 'antecedentes', 'personales', 'de', 'hipotiroidismo', 'en', 'tratamiento', 'con', 'levotiroxina', 'y', 'fumadora', 'activa', 'de', '12.5', 'paquetes', '/', 'año', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 9, 10, 16, 19, 22, 27, 31, 44, 55, 58, 73, 76, 88, 92, 105, 107, 116, 123, 126, 131, 139, 140, 143]\n",
            "['O', 'V-MOR', 'B-MOR', 'E-MOR', 'S-MOR', 'I-MOR', 'PAD']\n",
            "{'O': 0, 'V-MOR': 1, 'B-MOR': 2, 'E-MOR': 3, 'S-MOR': 4, 'I-MOR': 5, 'PAD': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE2pol-nE-pW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b826b57e-ea41-445d-d6a3-2bfb1aa0148b"
      },
      "source": [
        "# DEVELOPMENT SET\n",
        "\n",
        "# *** without considering clinical cases independently ***\n",
        "\n",
        "# sentences\n",
        "getter_dev = sentence(df_data_dev2)\n",
        "sentences_bert_dev = [[s[0] for s in sentence] for sentence in getter_dev.sentences]\n",
        "print(sentences_bert_dev[0])\n",
        "\n",
        "# labels\n",
        "labels_bert_dev = [[s[2] for s in sentence] for sentence in getter_dev.sentences]\n",
        "print(labels_bert_dev[0])\n",
        "\n",
        "# start char position\n",
        "start_bert_dev = [[s[3] for s in sentence] for sentence in getter_dev.sentences]\n",
        "print(start_bert_dev[0])\n",
        "\n",
        "# ----------------\n",
        "\n",
        "# *** By clinical case ***\n",
        "getter_by_cc = df_data_dev2.groupby(\"File_Index\").apply(sentence)\n",
        "\n",
        "sentences_bert_dev_by_cc = []\n",
        "labels_bert_dev_by_cc = []\n",
        "start_bert_dev_by_cc = []\n",
        "\n",
        "for getter_i in getter_by_cc: # iterating over all the files\n",
        "  # sentences\n",
        "  sent_by_cc = [[s[0] for s in sentence] for sentence in getter_i.sentences]\n",
        "  sentences_bert_dev_by_cc.append(sent_by_cc)\n",
        "  # labels\n",
        "  lab_by_cc = [[s[2] for s in sentence] for sentence in getter_i.sentences]\n",
        "  labels_bert_dev_by_cc.append(lab_by_cc)\n",
        "  # start char position\n",
        "  start_by_cc = [[s[3] for s in sentence] for sentence in getter_i.sentences]\n",
        "  start_bert_dev_by_cc.append(start_by_cc)\n",
        "\n",
        "print()\n",
        "print(sentences_bert_dev_by_cc[0][0])\n",
        "print(labels_bert_dev_by_cc[0][0])\n",
        "print(start_bert_dev_by_cc[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Anamnesis', '\\n', 'Varón', 'de', '74', 'años', ',', 'exfumador', 'desde', 'hace', '15', 'años', ',', 'con', 'único', 'antecedente', 'de', 'hipertensión', ',', 'dislipemia', 'y', 'apendicectomizado', ';', 'se', 'diagnostica', 'en', 'marzo', 'de', '2013', 'de', 'carcinoma', 'de', 'células', 'transicionales', 'de', 'vejiga', 'E-IV', '(', 'pulmonares', 'y', 'óseas', ')', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'E-MOR', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 9, 10, 16, 19, 22, 26, 28, 38, 44, 49, 52, 56, 58, 62, 68, 80, 83, 95, 97, 108, 110, 127, 129, 132, 144, 147, 153, 156, 161, 164, 174, 177, 185, 200, 203, 210, 215, 216, 227, 229, 234, 235]\n",
            "\n",
            "['Anamnesis', '\\n', 'Varón', 'de', '74', 'años', ',', 'exfumador', 'desde', 'hace', '15', 'años', ',', 'con', 'único', 'antecedente', 'de', 'hipertensión', ',', 'dislipemia', 'y', 'apendicectomizado', ';', 'se', 'diagnostica', 'en', 'marzo', 'de', '2013', 'de', 'carcinoma', 'de', 'células', 'transicionales', 'de', 'vejiga', 'E-IV', '(', 'pulmonares', 'y', 'óseas', ')', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'E-MOR', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 9, 10, 16, 19, 22, 26, 28, 38, 44, 49, 52, 56, 58, 62, 68, 80, 83, 95, 97, 108, 110, 127, 129, 132, 144, 147, 153, 156, 161, 164, 174, 177, 185, 200, 203, 210, 215, 216, 227, 229, 234, 235]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suGoZEU5-abv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b01bad0b-861c-4f8c-d35a-35ce9c398951"
      },
      "source": [
        "# DEVELOPMENT SET 2\n",
        "\n",
        "# *** without considering clinical cases independently ***\n",
        "\n",
        "# sentences\n",
        "getter_dev2 = sentence(df_data_dev2_2)\n",
        "sentences_bert_dev2 = [[s[0] for s in sentence] for sentence in getter_dev2.sentences]\n",
        "print(sentences_bert_dev2[0])\n",
        "\n",
        "# labels\n",
        "labels_bert_dev2 = [[s[2] for s in sentence] for sentence in getter_dev2.sentences]\n",
        "print(labels_bert_dev2[0])\n",
        "\n",
        "# start char position\n",
        "start_bert_dev2 = [[s[3] for s in sentence] for sentence in getter_dev2.sentences]\n",
        "print(start_bert_dev2[0])\n",
        "# ----------------\n",
        "\n",
        "# *** By clinical case ***\n",
        "getter_by_cc2 = df_data_dev2_2.groupby(\"File_Index\").apply(sentence)\n",
        "\n",
        "sentences_bert_dev_by_cc2 = []\n",
        "labels_bert_dev_by_cc2 = []\n",
        "start_bert_dev_by_cc2 = []\n",
        "\n",
        "for getter_i in getter_by_cc2: # iterating over all the files\n",
        "  # sentences\n",
        "  sent_by_cc = [[s[0] for s in sentence] for sentence in getter_i.sentences]\n",
        "  sentences_bert_dev_by_cc2.append(sent_by_cc)\n",
        "  # labels\n",
        "  lab_by_cc = [[s[2] for s in sentence] for sentence in getter_i.sentences]\n",
        "  labels_bert_dev_by_cc2.append(lab_by_cc)\n",
        "  # start char position\n",
        "  start_by_cc = [[s[3] for s in sentence] for sentence in getter_i.sentences]\n",
        "  start_bert_dev_by_cc2.append(start_by_cc)\n",
        "\n",
        "print()\n",
        "print(sentences_bert_dev_by_cc2[0][0])\n",
        "print(labels_bert_dev_by_cc2[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Anamnesis', '\\n', 'Mujer', 'de', '60', 'años', 'diagnosticada', 'de', 'enfermedad', 'pulmonar', 'obstructiva', 'crónica', 'y', 'trastorno', 'bipolar', 'de', 'años', 'de', 'evolución', ',', 'en', 'tratamiento', 'con', 'carbamazepina', 'hasta', 'la', 'fecha', ',', 'sin', 'otros', 'antecedentes', 'personales', 'de', 'interés', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 9, 10, 16, 19, 22, 27, 41, 44, 55, 64, 76, 84, 86, 96, 104, 107, 112, 115, 124, 126, 129, 141, 145, 159, 165, 168, 173, 175, 179, 185, 198, 209, 212, 219]\n",
            "\n",
            "['Anamnesis', '\\n', 'Mujer', 'de', '60', 'años', 'diagnosticada', 'de', 'enfermedad', 'pulmonar', 'obstructiva', 'crónica', 'y', 'trastorno', 'bipolar', 'de', 'años', 'de', 'evolución', ',', 'en', 'tratamiento', 'con', 'carbamazepina', 'hasta', 'la', 'fecha', ',', 'sin', 'otros', 'antecedentes', 'personales', 'de', 'interés', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag5G1x_cS9DG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "496e4483-e042-4c49-c6bb-4e1d64017d5a"
      },
      "source": [
        "print(sentences_bert_dev_by_cc2[0][1])\n",
        "print(sentences_bert_dev2[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Exfumadora', 'desde', 'hace', '9', 'años', '.']\n",
            "['Exfumadora', 'desde', 'hace', '9', 'años', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhfXv49P9Mlf",
        "colab_type": "text"
      },
      "source": [
        "##### **Test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iyZFuMyIEER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d44480e2-23c8-413d-c684-529fec5f391e"
      },
      "source": [
        "# TEST SET\n",
        "\n",
        "# *** without considering clinical cases independently ***\n",
        "\n",
        "# sentences\n",
        "getter_test = sentence_test(df_data_test)\n",
        "sentences_bert_test = [[s[0] for s in sentence] for sentence in getter_test.sentences]\n",
        "print(sentences_bert_test[0])\n",
        "\n",
        "# start char position\n",
        "start_bert_test = [[s[2] for s in sentence] for sentence in getter_test.sentences]\n",
        "print(start_bert_test[0])\n",
        "\n",
        "# ----------------\n",
        "\n",
        "# *** By clinical case ***\n",
        "getter_by_cc_test = df_data_test.groupby(\"File_Index\").apply(sentence_test)\n",
        "\n",
        "sentences_bert_test_by_cc = []\n",
        "start_bert_test_by_cc = []\n",
        "\n",
        "for getter_i in getter_by_cc_test: # iterating over all the files\n",
        "  # sentences\n",
        "  sent_by_cc = [[s[0] for s in sentence] for sentence in getter_i.sentences]\n",
        "  sentences_bert_test_by_cc.append(sent_by_cc)\n",
        "  # start char position\n",
        "  start_by_cc = [[s[2] for s in sentence] for sentence in getter_i.sentences]\n",
        "  start_bert_test_by_cc.append(start_by_cc)\n",
        "\n",
        "print()\n",
        "print(sentences_bert_test_by_cc[0][0])\n",
        "print(start_bert_test_by_cc[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Paciente', 'mujer', ',', '75', 'años', 'consulta', 'el', '4-6-2003', ',', 'refiriendo', 'como', 'antecedentes', 'personales', ':', 'Alergia', 'a', 'salicilatos', '.']\n",
            "[0, 9, 14, 16, 19, 24, 33, 36, 44, 46, 57, 62, 75, 85, 87, 95, 97, 108]\n",
            "\n",
            "['Paciente', 'mujer', ',', '75', 'años', 'consulta', 'el', '4-6-2003', ',', 'refiriendo', 'como', 'antecedentes', 'personales', ':', 'Alergia', 'a', 'salicilatos', '.']\n",
            "[0, 9, 14, 16, 19, 24, 33, 36, 44, 46, 57, 62, 75, 85, 87, 95, 97, 108]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Gkea9rIrhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "983fa4a3-f0c6-47d6-eb0b-9d0700ca773a"
      },
      "source": [
        "print(sentences_bert_test_by_cc[0][1])\n",
        "print(sentences_bert_test[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'los', '59', 'años', 'fué', 'diagnosticada', 'de', 'fiebre', 'de', 'probable', 'etiología', 'específica', ',', 'tratada', 'con', 'tuberculostáticos', ',', 'según', 'pauta', 'habitual', '.']\n",
            "['A', 'los', '59', 'años', 'fué', 'diagnosticada', 'de', 'fiebre', 'de', 'probable', 'etiología', 'específica', ',', 'tratada', 'con', 'tuberculostáticos', ',', 'según', 'pauta', 'habitual', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpi0gM1zqZcb",
        "colab_type": "text"
      },
      "source": [
        "#### **Bert tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_oI03l8GdCr",
        "colab_type": "text"
      },
      "source": [
        "The following command constructs a **BERT tokenizer**. The Bert implementation comes with a pretrained tokenizer and a definsed vocabulary. We load the one related to the smallest pre-trained model bert-base-cased. We use the cased variate since it is well suited for NER.\n",
        "\n",
        "NOTE: we are not converting all to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP_BS5QZGcnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "2e686aadbc574d51a554bc67aade45b0",
            "30ada709ed804cf2a16983ea7427a95c",
            "73669580003a423faff028c1ec361774",
            "e8b474c3f801424abbeea991151ec250",
            "3651e775915a4ac8bbae292fe328a112",
            "271a7a69c51145d1a652d2b4a9ab4d29",
            "f78276dd9f3d4944b2d2157adc150726",
            "38a16333698040c280a2d6c194fdf766"
          ]
        },
        "outputId": "1eb984d0-09ce-49d8-cc69-11810dd2f2e0"
      },
      "source": [
        " # The Bert implementation comes with a pretrained tokenizer and a definied vocabulary\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False) # beto\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e686aadbc574d51a554bc67aade45b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5eOrDryqh_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, start_offset):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "    start_pos = []\n",
        "\n",
        "    for word, label, start in zip(sentence, text_labels, start_offset): \n",
        "      # start offset contains the position of the first character on every word\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        if str(word) == 'nan':\n",
        "          tokenized_word = 'UNK'\n",
        "          n_subwords = 1\n",
        "\n",
        "        else:\n",
        "          tokenized_word = tokenizer.tokenize(word)\n",
        "          n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "        # Add the same start character position\n",
        "        start_pos.extend([start] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels, start_pos\n",
        "\n",
        "# this code breaks up each word into several tokens for every sentence and preserves\n",
        "# the tag (\"O\", \"B-MOR\", \"I-MOR\", \"E-MOR\", \"S-MOR\", \"V-MOR\") while dong it.\n",
        "# This way, the sentences become longer (are formed by a higher number of tokens than before)\n",
        "# this is done to find words that could be present in different forms while having the same meaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdgfwfEXI86N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_test(sentence, start_offset):\n",
        "    tokenized_sentence = []\n",
        "    start_pos = []\n",
        "\n",
        "    for word, start in zip(sentence, start_offset):\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        if str(word) == 'nan':\n",
        "          tokenized_word = 'UNK'\n",
        "          n_subwords = 1\n",
        "        else:\n",
        "          tokenized_word = tokenizer.tokenize(word)\n",
        "          n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same start character position\n",
        "        start_pos.extend([start] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWBzDcaulx7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = \" \".join([\" \".join([str(s) for s in sent]) for sent in sentences_bert_test_by_cc[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-8L6ninl9uj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "80289e27-880e-4fe2-e28b-8541bfd4794b"
      },
      "source": [
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Paciente mujer , 75 años consulta el 4-6-2003 , refiriendo como antecedentes personales : Alergia a salicilatos . A los 59 años fué diagnosticada de fiebre de probable etiología específica , tratada con tuberculostáticos , según pauta habitual . En 1.998 , es diagnosticada de fiebre mediterránea familiar después de exclusión de otros procesos . Osteoporosis . Está en tratamiento con Colchicina ( 0,5 mg / día),Aines ( 50 mg / día ) , deflazacor ( 6 mg / día ) y Omeprazol . \\n Historia actual : La paciente tuvo una caída casual en octubre de 2.002 sobre flanco izquierdo presentando 10 días después un episodio de hematuria monosintomática . En mayo del 2.003 , tuvo fiebre y dolor lumbar izquierdo ingresando , en un centro hospitalario , donde se realiza un TAC abdominal que se informa como hipernefroma izquierdo con riñón derecho normal . Se le trata con quinolonas . No se realiza cultivo . En el momento de la consulta se encuentra asintomática , sin fiebre , ni dolor lumbar . \\n Exploración : no se palpan masas abdominales . \\n Analítica : Hemoglobina 12,3 . Hematocrito 37,1 . Leucocitos 11.200 ( Neutrofilos 76).Glucosa , urea , creatinina , BT , BD , GOT y GPT , son normales . Sedimento de orina , 10-15 leucocitos / campo . Urinocultivo : E. Coli . Ecografía renal : Masa renal izquierda sólido-quística de 12 cm compatible con hipernefroma . Riñón derecho normal . \\n TAC Abdominal : ( 10-06-03 ) Masa en la cara lateral del riñón izquierdo que produce un desplazamiento y una horizontalización del riñón que tiene un diámetro máximo de 12 cm presentando grandes áreas quísticas y otra central de componente sólido , pero incluso las áreas quísticas presentan una pared engrosada , por lo que debe tratarse de una tumoración sólida . Otra posibilidad es que se trate de un nefroma quístico , pero en cualquier caso la lesión es de aspecto tumoral y tiene un contorno mal definido , por lo que parece que existe infiltración de la grasa perirrenal e incluso en la parte posterior esta lesión está en contacto con la pared abdominal en la región lumbar con engrosamiento de la musculatura por encima de la cresta iliaca , sugestivo de infiltración de los músculos de la pared , a nivel del cuadrado lumbar . Riñón derecho normal . \\n Adenopatías en ilio hepático superiores a un cm . \\n La paciente se programa para estudio preoperatorio ambulatorio y nefrectomía . \\n El 25-06-03 , la paciente ingresó por presentar fiebre de 38ºC y dolor lumbar moderado sin afectación del estado general . \\n Exploración : masa lumbar izquierda . Puño percusión renal izquierda positiva . \\n Analítica : Hemoglobina 9,9 . Hematocrito 28,1 . Leucocitos 18.400 . Neutrofilos 87,6 . Cayados 5 . Plaquetas 328.000 . Urea , Creatinina y pruebas de función hepática , normales . Sedimento de orina , 100 leucocitos /campo . Urinocultivo : E. Coli . \\n Se realiza Ecografía abdominal que se informa de una gran masa sólida , heterogénea , polilobulada con zonas quísticas ocupando los dos tercios inferiores del riñón izquierdo y que se extiende hasta la pala iliaca y contacta con la pared abdominal posterior . \\n Un TAC abdominal se informa como masa compleja en relación con el polo inferior del riñón izquierdo Figuras 2 y 3).Esta masa tiene unos bordes irregulares , densidad mixta , con zonas de alta densidad y otras realmente quísticas . La masa crece fuera de la celda renal , invadiendo el espacio pararrenal posterior y destruyendo la pared abdominal , a nivel de la inserción de los músculos oblicuos y transversos en el borde superior del ala iliaca izquierda . El diámetro craneo caudal de la masa es de 12 cm . y muestra un diámetro similar . Se coloca un drenaje de las zonas quísticas saliendo abundante pus . El cultivo del pus drenado es E. Coli . Instaurando tratamiento con Ciprofloxacino IV 200 mgr /cada 12 horas . \\n El 4-07-03 , se realizó nefrectomía izquierda por lumbotomía sin incidencias . \\n El diagnóstico anatomopatológico pone de manifiesto unas células histiocitarias poligonales o fusocelulares eosinofílas con cuerpos de Michaelis-Gutmann y linfocitos acompañantes . \\n El postoperatorio cursó sin complicaciones . Se mantuvo el tratamiento con ciprofloxacino oral a dosis habituales durante un mes . \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9ypwdVq_WI",
        "colab_type": "text"
      },
      "source": [
        "##### **Tokenize Train and Development sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2jWPTFqhys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1fa7bd18-b25a-4b8a-ae76-bc1d87edae7c"
      },
      "source": [
        "print(\"Number of words in the first sentence: %d\" %len(sentences_bert_train[0]))\n",
        "\n",
        "# 1. Tokenize each sentence and keep text and labels\n",
        "tokenized_texts_and_labels_train = [tokenize_and_preserve_labels(sent, labs,start) for sent, labs, start \n",
        "                                    in zip(sentences_bert_train, labels_bert_train, start_bert_train)]\n",
        "print(\"Number of words in the first sentence after tokenizing: %d\" %len(tokenized_texts_and_labels_train[0][0]))\n",
        "print(tokenized_texts_and_labels_train[0][0]) # first sentence; text part\n",
        "print(tokenized_texts_and_labels_train[0][1]) # first sentence; label part (Contains both the sentences and the labels (tags))\n",
        "print(tokenized_texts_and_labels_train[0][2]) # first sentence; start position part (Contains both the sentences and the labels (tags))\n",
        "\n",
        "\n",
        "# 2. Split each tokenized sentence into text and labels\n",
        "tokenized_texts_train = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_train]\n",
        "labels_train = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_train]\n",
        "start_pos_train = [token_label_pair[2] for token_label_pair in tokenized_texts_and_labels_train]\n",
        "\n",
        "print()\n",
        "print(tokenized_texts_train[0])\n",
        "\n",
        "\n",
        "# 3. Cut and pad the token and label sequences to the desired length. (max_len: max sequence length = 75 tokens)\n",
        "input_ids_train = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_train],\n",
        "                          maxlen=max_len, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags_train = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_train],\n",
        "                     maxlen=max_len, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "# tokenizer.convert_tokens_to_ids(txt) is a built-in function that converts a token string \n",
        "# (or a sequence of tokens) into a single integer id (or a sequence of ids),\n",
        "# using the pre-defined vocabulary in tokenizer.\n",
        "print(\"\\nID representation of the first sentence: \\n %s\" %input_ids_train[0])\n",
        "\n",
        "\n",
        "# 4. Creation of attention masks to IGNORE certain words (padded elements)\n",
        "# If the ID of a word is 0, that word is \"PAD\", a mask must be created to IGNORE such word (to ignore padded elements)\n",
        "attention_masks_train = [[float(i != 0.0) for i in ii] for ii in input_ids_train] # it's an array of arrays\n",
        "print(\"\\nAttention mask of the first sentence: \\n %s\" %attention_masks_train[0])\n",
        "\n",
        "\n",
        "# 5. Split train dataset into train and validation\n",
        "train_inputs, valid_inputs, train_tags, valid_tags = train_test_split(input_ids_train, tags_train, random_state=0, test_size=0.2)\n",
        "train_masks, valid_masks, _, _ = train_test_split(attention_masks_train, input_ids_train, random_state=0, test_size=0.2)\n",
        "# _: used to not store tags of masked words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the first sentence: 24\n",
            "Number of words in the first sentence after tokenizing: 43\n",
            "['AN', '##AM', '##NE', '##SI', '##S', 'Mujer', 'de', '67', 'años', 'con', 'ante', '##cedent', '##es', 'personales', 'de', 'hip', '##oti', '##roi', '##dis', '##mo', 'en', 'tratamiento', 'con', 'le', '##vot', '##iro', '##xin', '##a', 'y', 'fu', '##mad', '##ora', 'activa', 'de', '12', '.', '5', 'pa', '##quet', '##es', '/', 'año', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 0, 0, 0, 0, 10, 16, 19, 22, 27, 31, 31, 31, 44, 55, 58, 58, 58, 58, 58, 73, 76, 88, 92, 92, 92, 92, 92, 105, 107, 107, 107, 116, 123, 126, 126, 126, 131, 131, 131, 139, 140, 143]\n",
            "\n",
            "['AN', '##AM', '##NE', '##SI', '##S', 'Mujer', 'de', '67', 'años', 'con', 'ante', '##cedent', '##es', 'personales', 'de', 'hip', '##oti', '##roi', '##dis', '##mo', 'en', 'tratamiento', 'con', 'le', '##vot', '##iro', '##xin', '##a', 'y', 'fu', '##mad', '##ora', 'activa', 'de', '12', '.', '5', 'pa', '##quet', '##es', '/', 'año', '.']\n",
            "\n",
            "ID representation of the first sentence: \n",
            " [ 50972  36535  93280  44802  10731  95391  10104  12316  11278  10173\n",
            "  15865 104101  10171  90770  10104  25377  23841  56859  17442  11033\n",
            "  10110  56708  10173  10141  63129  14213  76750  10113    193  11005\n",
            "  42998  14945  50796  10104  10186    119    126  10931  27579  10171\n",
            "    120  11734    119      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]\n",
            "\n",
            "Attention mask of the first sentence: \n",
            " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP2-umFBEMSq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9d9dcda7-a111-4038-96fd-5f488e82e970"
      },
      "source": [
        "print(sentences_bert_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ANAMNESIS', '\\n', 'Mujer', 'de', '67', 'años', 'con', 'antecedentes', 'personales', 'de', 'hipotiroidismo', 'en', 'tratamiento', 'con', 'levotiroxina', 'y', 'fumadora', 'activa', 'de', '12.5', 'paquetes', '/', 'año', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzzlrE-vES91",
        "colab_type": "text"
      },
      "source": [
        "['ANAMNESIS', '\\n', 'Mujer', 'de', '67', 'años', 'con', 'antecedentes', 'personales', 'de', 'hipotiroidismo', 'en', 'tratamiento', 'con', 'levotiroxina', 'y', 'fumadora', 'activa', 'de', '12.5', 'paquetes', '/', 'año', '.']\n",
        "\n",
        "\n",
        "['AN', '##AM', '##NE', '##SI', '##S', 'Mujer', 'de', '67', 'años', 'con', 'ante', '##cedent', '##es', 'personales', 'de', 'hip', '##oti', '##roi', '##dis', '##mo', 'en', 'tratamiento', 'con', 'le', '##vot', '##iro', '##xin', '##a', 'y', 'fu', '##mad', '##ora', 'activa', 'de', '12', '.', '5', 'pa', '##quet', '##es', '/', 'año', '.']\n",
        "\n",
        " [ 50972  36535  93280  44802  10731  95391  10104  12316  11278  10173\n",
        "  15865 104101  10171  90770  10104  25377  23841  56859  17442  11033\n",
        "  10110  56708  10173  10141  63129  14213  76750  10113    193  11005\n",
        "  42998  14945  50796  10104  10186    119    126  10931  27579  10171\n",
        "    120  11734    119      0      0      0      0      0      0      0\n",
        "      0      0      0      0      0      0      0      0      0      0\n",
        "      0      0      0      0      0      0      0      0      0      0\n",
        "      0      0      0      0      0]\n",
        "\n",
        "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHgZBKY4ynx8",
        "colab_type": "text"
      },
      "source": [
        "**Tokenize development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43QSa7BtyslY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "76da26cc-f84b-4fdd-ee1c-5086679fe148"
      },
      "source": [
        "# *** without considering clinical cases independently ***\n",
        "\n",
        "print(\"Number of words in the first sentence: %d\" %len(sentences_bert_dev[0]))\n",
        "\n",
        "# 1. Tokenize each sentence and keep text and labels\n",
        "tokenized_texts_and_labels_dev = [tokenize_and_preserve_labels(sent, labs,start) for sent, labs,start \n",
        "                                  in zip(sentences_bert_dev, labels_bert_dev,start_bert_dev)]\n",
        "print(\"Number of words in the first sentence after tokenizing: %d\" %len(tokenized_texts_and_labels_dev[0][0]))\n",
        "print(tokenized_texts_and_labels_dev[0][0]) # first sentence; text part\n",
        "print(tokenized_texts_and_labels_dev[0][1]) # first sentence; label part (Contains both the sentences and the labels (tags))\n",
        "print(tokenized_texts_and_labels_dev[0][2]) # first sentence; start char position part (Contains both the sentences and the labels (tags))\n",
        "\n",
        "\n",
        "# 2. Split each tokenized sentence into text and labels\n",
        "tokenized_texts_dev = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_dev]\n",
        "labels_dev = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_dev]\n",
        "start_pos_dev = [token_label_pair[2] for token_label_pair in tokenized_texts_and_labels_dev]\n",
        "\n",
        "print()\n",
        "print(tokenized_texts_dev[0])\n",
        "\n",
        "\n",
        "# 3. Cut and pad the token and label sequences to the desired length. (max_len: max sequence length = 75 tokens)\n",
        "input_ids_dev = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_dev],\n",
        "                          maxlen=max_len, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags_dev = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_dev],\n",
        "                     maxlen=max_len, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "print(\"\\nID representation of the first sentence: \\n %s\" %input_ids_dev[0])\n",
        "\n",
        "start_dev = pad_sequences(start_pos_dev,\n",
        "                     maxlen=max_len, value=-1, padding=\"post\", truncating=\"post\")\n",
        "print(\"\\nStart char positions the first sentence: \\n %s\" %start_dev[0])\n",
        "\n",
        "\n",
        "# 4. Creation of attention masks to IGNORE certain words (padded elements)\n",
        "attention_masks_dev = [[float(i != 0.0) for i in ii] for ii in input_ids_dev] # it's an array of arrays\n",
        "print(\"\\nAttention mask of the first sentence: \\n %s\" %attention_masks_dev[0])\n",
        "\n",
        "\n",
        "# 5. Split train dataset into train and validation\n",
        "dev_inputs = input_ids_dev\n",
        "dev_tags =  tags_dev\n",
        "dev_masks = attention_masks_dev\n",
        "# _: used to not store tags of masked words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the first sentence: 43\n",
            "Number of words in the first sentence after tokenizing: 70\n",
            "['Ana', '##mne', '##sis', 'Var', '##ón', 'de', '74', 'años', ',', 'ex', '##fum', '##ador', 'desde', 'hace', '15', 'años', ',', 'con', 'único', 'ante', '##cedent', '##e', 'de', 'hip', '##erten', '##sión', ',', 'dis', '##lip', '##emia', 'y', 'ap', '##endi', '##ce', '##cto', '##mi', '##zado', ';', 'se', 'diagnostic', '##a', 'en', 'marzo', 'de', '2013', 'de', 'car', '##cino', '##ma', 'de', 'células', 'trans', '##icion', '##ales', 'de', 'vej', '##iga', 'E', '-', 'IV', '(', 'pu', '##lm', '##onare', '##s', 'y', 'ós', '##eas', ')', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOR', 'B-MOR', 'B-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'I-MOR', 'E-MOR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 0, 0, 10, 10, 16, 19, 22, 26, 28, 28, 28, 38, 44, 49, 52, 56, 58, 62, 68, 68, 68, 80, 83, 83, 83, 95, 97, 97, 97, 108, 110, 110, 110, 110, 110, 110, 127, 129, 132, 132, 144, 147, 153, 156, 161, 164, 164, 164, 174, 177, 185, 185, 185, 200, 203, 203, 210, 210, 210, 215, 216, 216, 216, 216, 227, 229, 229, 234, 235]\n",
            "\n",
            "['Ana', '##mne', '##sis', 'Var', '##ón', 'de', '74', 'años', ',', 'ex', '##fum', '##ador', 'desde', 'hace', '15', 'años', ',', 'con', 'único', 'ante', '##cedent', '##e', 'de', 'hip', '##erten', '##sión', ',', 'dis', '##lip', '##emia', 'y', 'ap', '##endi', '##ce', '##cto', '##mi', '##zado', ';', 'se', 'diagnostic', '##a', 'en', 'marzo', 'de', '2013', 'de', 'car', '##cino', '##ma', 'de', 'células', 'trans', '##icion', '##ales', 'de', 'vej', '##iga', 'E', '-', 'IV', '(', 'pu', '##lm', '##onare', '##s', 'y', 'ós', '##eas', ')', '.']\n",
            "\n",
            "ID representation of the first sentence: \n",
            " [ 16377  43685  13109  48725  11482  10104  12535  11278    117  11419\n",
            " 101851  18368  11392  18414  10208  11278    117  10173  18680  15865\n",
            " 104101  10112  10104  25377  26645  39200    117  27920  69553  44070\n",
            "    193  26219  77089  10419  31981  10500  20092    132  10126 100527\n",
            "  10113  10110  12401  10104  10207  10104  13000  38333  10369  10104\n",
            "  28958  37241  67645  18174  10104 102525  15342    142    118  11094\n",
            "    113  34597  55183  97095  10107    193  77949  42658    114    119\n",
            "      0      0      0      0      0]\n",
            "\n",
            "Start char positions the first sentence: \n",
            " [  0   0   0  10  10  16  19  22  26  28  28  28  38  44  49  52  56  58\n",
            "  62  68  68  68  80  83  83  83  95  97  97  97 108 110 110 110 110 110\n",
            " 110 127 129 132 132 144 147 153 156 161 164 164 164 174 177 185 185 185\n",
            " 200 203 203 210 210 210 215 216 216 216 216 227 229 229 234 235  -1  -1\n",
            "  -1  -1  -1]\n",
            "\n",
            "Attention mask of the first sentence: \n",
            " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWYu3x--1sZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d220f53-dd81-47bb-b878-1040d5d9a6bc"
      },
      "source": [
        "num_files_dev = len(sentences_bert_dev_by_cc) \n",
        "print(\"Number of files in the development set: %d\" %num_files_dev)\n",
        "len_sent_by_cc = [len(cc) for cc in sentences_bert_dev_by_cc] # number of sentences in each file\n",
        "sentences_bert_dev_by_cc2 = []\n",
        "offset = 0\n",
        "\n",
        "for length_file in len_sent_by_cc:\n",
        "  sent_cc = sentences_bert_dev[offset: offset + length_file]\n",
        "  sentences_bert_dev_by_cc2.append(sent_cc)\n",
        "  offset = offset + length_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of files in the development set: 250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJBKjX_v0hYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *** By clinical case ***\n",
        "\n",
        "len_sent_by_cc = [len(cc) for cc in sentences_bert_dev_by_cc] # number of sentences in each file\n",
        "\n",
        "dev_inputs_by_cc = []\n",
        "dev_tags_by_cc = []\n",
        "dev_masks_by_cc = []\n",
        "start_dev_by_cc = []\n",
        "offset = 0\n",
        "\n",
        "for length_file in len_sent_by_cc:\n",
        "  dev_inputs_by_cc.append(dev_inputs[offset: offset + length_file])\n",
        "  dev_tags_by_cc.append(dev_tags[offset: offset + length_file])\n",
        "  dev_masks_by_cc.append(dev_masks[offset: offset + length_file])\n",
        "  start_dev_by_cc.append(start_dev[offset: offset + length_file])\n",
        "\n",
        "  offset = offset + length_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jobhnkEwUnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a795d7e-ae3a-4392-bc7d-ee0c715e3abf"
      },
      "source": [
        "print(tag2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'E-MOR': 0, 'O': 1, 'I-MOR': 2, 'B-MOR': 3, 'S-MOR': 4, 'V-MOR': 5, 'PAD': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q7qpboXYgYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "df8761f3-2050-4d6e-f282-bab2e9d8f708"
      },
      "source": [
        "print(dev_inputs[26])\n",
        "print()\n",
        "print(dev_inputs_by_cc[1][1])\n",
        "print(\"\\n-----------------------------------------------------------------------\\n\")\n",
        "print(dev_tags[26])\n",
        "print()\n",
        "print(dev_tags_by_cc[1][1])\n",
        "print(\"\\n-----------------------------------------------------------------------\\n\")\n",
        "print(dev_masks[26])\n",
        "print()\n",
        "print(dev_masks_by_cc[1][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 34387  11488  10567  10104  10671 100076  10107  48602  11272  10110\n",
            "  16814  10104  10185  10110  10109  86153  10104 103174  14865  10173\n",
            "  12596  76456  10165  81270  11389    119      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]\n",
            "\n",
            "[ 34387  11488  10567  10104  10671 100076  10107  48602  11272  10110\n",
            "  16814  10104  10185  10110  10109  86153  10104 103174  14865  10173\n",
            "  12596  76456  10165  81270  11389    119      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]\n",
            "\n",
            "-----------------------------------------------------------------------\n",
            "\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 6 6 6 6 6 6 6 6\n",
            " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            " 6]\n",
            "\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 6 6 6 6 6 6 6 6\n",
            " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            " 6]\n",
            "\n",
            "-----------------------------------------------------------------------\n",
            "\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99vqxbJhHZHo",
        "colab_type": "text"
      },
      "source": [
        "**Tokenize development set 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG7DbcG9HYbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "cac28967-b06a-45a3-dd30-b2c7a7f74a19"
      },
      "source": [
        "# *** without considering clinical cases independently ***\n",
        "\n",
        "print(\"Number of words in the first sentence: %d\" %len(sentences_bert_dev2[0]))\n",
        "\n",
        "# 1. Tokenize each sentence and keep text and labels\n",
        "tokenized_texts_and_labels_dev2 = [tokenize_and_preserve_labels(sent, labs, start) for sent, labs,start \n",
        "                                   in zip(sentences_bert_dev2, labels_bert_dev2,start_bert_dev2)]\n",
        "print(\"Number of words in the first sentence after tokenizing: %d\" %len(tokenized_texts_and_labels_dev2[0][0]))\n",
        "print(tokenized_texts_and_labels_dev2[0][0]) # first sentence; text part\n",
        "print(tokenized_texts_and_labels_dev2[0][1]) # first sentence; label part (Contains both the sentences and the labels (tags))\n",
        "print(tokenized_texts_and_labels_dev2[0][2]) # first sentence; start char position part (Contains both the sentences and the labels (tags))\n",
        "\n",
        "\n",
        "# 2. Split each tokenized sentence into text and labels\n",
        "tokenized_texts_dev2 = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_dev2]\n",
        "labels_dev2 = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_dev2]\n",
        "start_pos_dev2 = [token_label_pair[2] for token_label_pair in tokenized_texts_and_labels_dev2]\n",
        "\n",
        "print()\n",
        "print(tokenized_texts_dev2[0])\n",
        "\n",
        "\n",
        "# 3. Cut and pad the token and label sequences to the desired length. (max_len: max sequence length = 75 tokens)\n",
        "input_ids_dev2 = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_dev2],\n",
        "                          maxlen=max_len, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags_dev2 = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_dev2],\n",
        "                     maxlen=max_len, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "print(\"\\nID representation of the first sentence: \\n %s\" %input_ids_dev2[0])\n",
        "\n",
        "start_dev2 = pad_sequences(start_pos_dev2,\n",
        "                     maxlen=max_len, value=-1, padding=\"post\", truncating=\"post\")\n",
        "print(\"\\nStart char positions the first sentence: \\n %s\" %start_dev2[0])\n",
        "\n",
        "# 4. Creation of attention masks to IGNORE certain words (padded elements)\n",
        "attention_masks_dev2 = [[float(i != 0.0) for i in ii] for ii in input_ids_dev2] # it's an array of arrays\n",
        "print(\"\\nAttention mask of the first sentence: \\n %s\" %attention_masks_dev2[0])\n",
        "\n",
        "\n",
        "# 5. Split train dataset into train and validation\n",
        "dev_inputs2 = input_ids_dev2\n",
        "dev_tags2 =  tags_dev2\n",
        "dev_masks2 = attention_masks_dev2\n",
        "# _: used to not store tags of masked words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the first sentence: 35\n",
            "Number of words in the first sentence after tokenizing: 51\n",
            "['Ana', '##mne', '##sis', 'Mujer', 'de', '60', 'años', 'diagnostic', '##ada', 'de', 'enfermedad', 'pu', '##lm', '##onar', 'ob', '##stru', '##ctiva', 'c', '##rónica', 'y', 'tras', '##torno', 'bi', '##pola', '##r', 'de', 'años', 'de', 'evolución', ',', 'en', 'tratamiento', 'con', 'car', '##ba', '##maz', '##ep', '##ina', 'hasta', 'la', 'fecha', ',', 'sin', 'otros', 'ante', '##cedent', '##es', 'personales', 'de', 'interés', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 0, 0, 10, 16, 19, 22, 27, 27, 41, 44, 55, 55, 55, 64, 64, 64, 76, 76, 84, 86, 86, 96, 96, 96, 104, 107, 112, 115, 124, 126, 129, 141, 145, 145, 145, 145, 145, 159, 165, 168, 173, 175, 179, 185, 185, 185, 198, 209, 212, 219]\n",
            "\n",
            "['Ana', '##mne', '##sis', 'Mujer', 'de', '60', 'años', 'diagnostic', '##ada', 'de', 'enfermedad', 'pu', '##lm', '##onar', 'ob', '##stru', '##ctiva', 'c', '##rónica', 'y', 'tras', '##torno', 'bi', '##pola', '##r', 'de', 'años', 'de', 'evolución', ',', 'en', 'tratamiento', 'con', 'car', '##ba', '##maz', '##ep', '##ina', 'hasta', 'la', 'fecha', ',', 'sin', 'otros', 'ante', '##cedent', '##es', 'personales', 'de', 'interés', '.']\n",
            "\n",
            "ID representation of the first sentence: \n",
            " [ 16377  43685  13109  95391  10104  10709  11278 100527  11153  10104\n",
            "  51855  34597  55183  44320  17339  42461  98721    171  57288    193\n",
            "  14807 100921  11342 101123  10129  10104  11278  10104  44043    117\n",
            "  10110  56708  10173  13000  10537  39125  19986  11067  11410  10109\n",
            "  24283    117  10795  12525  15865 104101  10171  90770  10104  30644\n",
            "    119      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]\n",
            "\n",
            "Start char positions the first sentence: \n",
            " [  0   0   0  10  16  19  22  27  27  41  44  55  55  55  64  64  64  76\n",
            "  76  84  86  86  96  96  96 104 107 112 115 124 126 129 141 145 145 145\n",
            " 145 145 159 165 168 173 175 179 185 185 185 198 209 212 219  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1]\n",
            "\n",
            "Attention mask of the first sentence: \n",
            " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3mBihJQVdk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *** By clinical case ***\n",
        "\n",
        "len_sent_by_cc2 = [len(cc) for cc in sentences_bert_dev_by_cc2] # number of sentences in each file\n",
        "\n",
        "dev_inputs_by_cc2 = []\n",
        "dev_tags_by_cc2 = []\n",
        "dev_masks_by_cc2 = []\n",
        "start_dev_by_cc2 = []\n",
        "offset = 0\n",
        "\n",
        "for length_file in len_sent_by_cc2:\n",
        "  dev_inputs_by_cc2.append(dev_inputs2[offset: offset + length_file])\n",
        "  dev_tags_by_cc2.append(dev_tags2[offset: offset + length_file])\n",
        "  dev_masks_by_cc2.append(dev_masks2[offset: offset + length_file])\n",
        "  start_dev_by_cc2.append(start_dev2[offset: offset + length_file])\n",
        "\n",
        "\n",
        "  offset = offset + length_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL42AaeAH20L",
        "colab_type": "text"
      },
      "source": [
        "##### **Tokenize test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JgQGzO2H1UD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "f4b5ec5e-a1c5-4d2f-d04a-d2e51f27b9e7"
      },
      "source": [
        "# *** without considering clinical cases independently ***\n",
        "\n",
        "print(\"Number of words in the first sentence: %d\" %len(sentences_bert_test[0]))\n",
        "\n",
        "# 1. Tokenize each sentence \n",
        "tokenized_texts_and_labels_test = [tokenize_test(sent,start) for sent, start in zip(sentences_bert_test,start_bert_test)]\n",
        "print(\"Number of words in the first sentence after tokenizing: %d\" %len(tokenized_texts_and_labels_test[0][0]))\n",
        "print(tokenized_texts_and_labels_test[0][0]) # first sentence; text part\n",
        "print(tokenized_texts_and_labels_test[0][1]) # first sentence; start char position part (Contains both the sentences and the labels (tags))\n",
        "\n",
        "\n",
        "# 2. Split each tokenized sentence into text and start char position\n",
        "tokenized_texts_test = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_test]\n",
        "start_pos_test = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_test]\n",
        "print()\n",
        "print(tokenized_texts_test[0])\n",
        "\n",
        "# 3. Cut and pad the token and label sequences to the desired length. (max_len: max sequence length = 75 tokens)\n",
        "input_ids_test= pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
        "                          maxlen=max_len, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "print(\"\\nID representation of the first sentence: \\n %s\" %input_ids_test[0])\n",
        "\n",
        "start_test = pad_sequences(start_pos_test,\n",
        "                     maxlen=max_len, value=-1, padding=\"post\", truncating=\"post\")\n",
        "print(\"\\nStart char positions the first sentence: \\n %s\" %start_test[0])\n",
        "\n",
        "# 4. Creation of attention masks to IGNORE certain words (padded elements)\n",
        "attention_masks_test = [[float(i != 0.0) for i in ii] for ii in input_ids_test] # it's an array of arrays\n",
        "print(\"\\nAttention mask of the first sentence: \\n %s\" %attention_masks_test[0])\n",
        "\n",
        "\n",
        "# 5. Split train dataset into train and validation\n",
        "test_inputs = input_ids_test\n",
        "test_masks = attention_masks_test\n",
        "# _: used to not store tags of masked words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the first sentence: 18\n",
            "Number of words in the first sentence after tokenizing: 31\n",
            "['Pac', '##iente', 'mujer', ',', '75', 'años', 'consulta', 'el', '4', '-', '6', '-', '2003', ',', 'ref', '##iri', '##endo', 'como', 'ante', '##cedent', '##es', 'personales', ':', 'Ale', '##rgia', 'a', 'sal', '##ici', '##lato', '##s', '.']\n",
            "[0, 0, 9, 14, 16, 19, 24, 33, 36, 36, 36, 36, 36, 44, 46, 46, 46, 57, 62, 62, 62, 75, 85, 87, 87, 95, 97, 97, 97, 97, 108]\n",
            "\n",
            "['Pac', '##iente', 'mujer', ',', '75', 'años', 'consulta', 'el', '4', '-', '6', '-', '2003', ',', 'ref', '##iri', '##endo', 'como', 'ante', '##cedent', '##es', 'personales', ':', 'Ale', '##rgia', 'a', 'sal', '##ici', '##lato', '##s', '.']\n",
            "\n",
            "ID representation of the first sentence: \n",
            " [ 82376  22847  24033    117  11417  11278  86153  10125    125    118\n",
            "    127    118  10295    117  48056  19334  17560  10225  15865 104101\n",
            "  10171  90770    131  59340  85368    169  31119  13439  35304  10107\n",
            "    119      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]\n",
            "\n",
            "Start char positions the first sentence: \n",
            " [  0   0   9  14  16  19  24  33  36  36  36  36  36  44  46  46  46  57\n",
            "  62  62  62  75  85  87  87  95  97  97  97  97 108  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1]\n",
            "\n",
            "Attention mask of the first sentence: \n",
            " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls_SQsjlLA_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28b715cc-8ffd-4b23-c3f5-6ac07cad63ea"
      },
      "source": [
        "num_files_test = len(sentences_bert_test_by_cc) \n",
        "print(\"Number of files in the test set: %d\" %num_files_test)\n",
        "len_sent_by_cc = [len(cc) for cc in sentences_bert_test_by_cc] # number of sentences in each file\n",
        "sentences_bert_test_by_cc2 = []\n",
        "offset = 0\n",
        "\n",
        "for length_file in len_sent_by_cc:\n",
        "  sent_cc = sentences_bert_test[offset: offset + length_file]\n",
        "  sentences_bert_test_by_cc2.append(sent_cc)\n",
        "  offset = offset + length_file\n",
        "\n",
        "\n",
        "# *** By clinical case ***\n",
        "\n",
        "len_sent_by_cc = [len(cc) for cc in sentences_bert_test_by_cc] # number of sentences in each file\n",
        "\n",
        "test_inputs_by_cc = []\n",
        "test_masks_by_cc = []\n",
        "start_test_by_cc = []\n",
        "offset = 0\n",
        "\n",
        "for length_file in len_sent_by_cc:\n",
        "  test_inputs_by_cc.append(test_inputs[offset: offset + length_file])\n",
        "  test_masks_by_cc.append(test_masks[offset: offset + length_file])\n",
        "  start_test_by_cc.append(start_test[offset: offset + length_file])\n",
        "\n",
        "  offset = offset + length_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of files in the test set: 5232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1tJWBztLHkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "1e44d117-4b63-4aee-a5ff-89aa09a243db"
      },
      "source": [
        "print(test_inputs[48])\n",
        "print()\n",
        "print(test_inputs_by_cc[1][1])\n",
        "print(\"\\n-----------------------------------------------------------------------\\n\")\n",
        "print(test_masks[48])\n",
        "print()\n",
        "print(test_masks_by_cc[1][1])\n",
        "print(\"\\n-----------------------------------------------------------------------\\n\")\n",
        "print(start_test[48])\n",
        "print()\n",
        "print(start_test_by_cc[1][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10243 10182 52909   117 15882 10537 35076 10567 25183 10104 10109 93622\n",
            " 48832 39467 10280   113 29137   119   126   181 10350 53166   119   120\n",
            " 10233 10237   119     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "[10243 10182 52909   117 15882 10537 35076 10567 25183 10104 10109 93622\n",
            " 48832 39467 10280   113 29137   119   126   181 10350 53166   119   120\n",
            " 10233 10237   119     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "-----------------------------------------------------------------------\n",
            "\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "\n",
            "-----------------------------------------------------------------------\n",
            "\n",
            "[515 518 522 530 532 532 543 543 550 558 561 564 564 564 564 577 578 578\n",
            " 578 584 584 584 584 584 584 584 593  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1]\n",
            "\n",
            "[515 518 522 530 532 532 543 543 550 558 561 564 564 564 564 577 578 578\n",
            " 578 584 584 584 584 584 584 584 593  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n",
            "  -1  -1  -1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwXh8vq-ZeOg",
        "colab_type": "text"
      },
      "source": [
        "#### **Convert data into tensors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKKdfsrH91Xt",
        "colab_type": "text"
      },
      "source": [
        "##### **Train and development sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHgs-y1qZ3lg",
        "colab_type": "text"
      },
      "source": [
        "**Train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9veAqM5ZZiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to TENSORS (working with pytorch)\n",
        "\n",
        "# text\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "valid_inputs = torch.tensor(valid_inputs)\n",
        "\n",
        "# tags\n",
        "train_tags = torch.tensor(train_tags)\n",
        "valid_tags = torch.tensor(valid_tags)\n",
        "\n",
        "# masks\n",
        "train_masks = torch.tensor(train_masks)\n",
        "valid_masks = torch.tensor(valid_masks)\n",
        "\n",
        "\n",
        "# Define DATALOADERS\n",
        "# shuffle the data of train set with RandomSampler \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
        "train_sampler = RandomSampler(train_data, ) # it just shuffles wihtout selecting a subset since num_samples = None\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_data = TensorDataset(valid_inputs, valid_masks, valid_tags)\n",
        "valid_sampler = SequentialSampler(valid_data) # validation data is passed sequentially\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SaV0x8EZ6uR",
        "colab_type": "text"
      },
      "source": [
        "**Development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "72k_SeCmMeo2",
        "colab": {}
      },
      "source": [
        "# Convert to TENSORS (working with pytorch)\n",
        "\n",
        "# text\n",
        "dev_inputs = torch.tensor(dev_inputs)\n",
        "\n",
        "# tags\n",
        "dev_tags = torch.tensor(dev_tags)\n",
        "\n",
        "# masks\n",
        "dev_masks = torch.tensor(dev_masks)\n",
        "\n",
        "\n",
        "# Define DATALOADERS\n",
        "\n",
        "# *** without considering clinical cases individually ***\n",
        "dev_data = TensorDataset(dev_inputs, dev_masks, dev_tags)\n",
        "dev_sampler = SequentialSampler(dev_data) # validation data is passed sequentially\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
        "\n",
        "# *** by clinical case***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqdnXOLMMgIP",
        "colab_type": "text"
      },
      "source": [
        "**Development set 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e2A_MBaaN1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to TENSORS (working with pytorch)\n",
        "\n",
        "# text\n",
        "dev_inputs2 = torch.tensor(dev_inputs2)\n",
        "\n",
        "# tags\n",
        "dev_tags2 = torch.tensor(dev_tags2)\n",
        "\n",
        "# masks\n",
        "dev_masks2 = torch.tensor(dev_masks2)\n",
        "\n",
        "\n",
        "# Define DATALOADERS\n",
        "\n",
        "# *** without considering clinical cases individually ***\n",
        "dev_data2 = TensorDataset(dev_inputs2, dev_masks2, dev_tags2)\n",
        "dev_sampler2 = SequentialSampler(dev_data2) # validation data is passed sequentially\n",
        "dev_dataloader2 = DataLoader(dev_data2, sampler=dev_sampler2, batch_size=batch_size)\n",
        "\n",
        "# *** by clinical case***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK7fd0rnMtUO",
        "colab_type": "text"
      },
      "source": [
        "##### **Test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m-7KhTYyMdSG",
        "colab": {}
      },
      "source": [
        "# Convert to TENSORS (working with pytorch)\n",
        "\n",
        "# text\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "\n",
        "# masks\n",
        "test_masks = torch.tensor(test_masks)\n",
        "\n",
        "\n",
        "# Define DATALOADERS\n",
        "\n",
        "# *** without considering clinical cases individually ***\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data) # validation data is passed sequentially\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D7_VUA2bsr_",
        "colab_type": "text"
      },
      "source": [
        "#### **STAGE 1: BERT Classification pre-trained model**\n",
        "\n",
        "**BertForTokenClassification class** from transformers is used for token-level predictions. BertForTokenClassification is a fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. \n",
        "\n",
        "The token-level classifier is a linear layer that takes as input the last hidden state of the sequence. We load the pre-trained bert-base-cased model and provide the number of possible labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQyYpPKabxdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "6501d2d63566470c9f6d941b4556fe2f",
            "7c82f71d366b438daaec61bd26f8871d",
            "224f4bcfe37740dd913133f33b25a828",
            "6f755c976ecd4430a6d115c6fa5a724b",
            "504ffb750fd945c8a97283d24befb4ab",
            "08ea088dde694039a49fea6a5fd5d01f",
            "51454878fb7a409cbfd59111f8b5e840",
            "01a63dc4b1924857be938cb48f8f15bb",
            "c24ce23cb8274e19b9a6d7b5bdc24a7f",
            "3d77fb17d755489296c11d49ed537e04",
            "34dc10e0f1b44f66a8711e99b4643870",
            "f70a0abd2caf45d7aaa2ac6871baa2be",
            "8fc3719a2e9d493b9b9c49bc75c3e3b9",
            "d1be172aa77b439aa71b131a86e3141f",
            "0725e316073d4940b652d93ca6cc2666",
            "5f037c9fc1bb4997a7258060032743fb"
          ]
        },
        "outputId": "5aa3e627-cf8a-4c83-fd94-292fb8947ff0"
      },
      "source": [
        "# loading the pretrained model of BERT base \n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")\n",
        "\n",
        "# pass the model parameters to the GPU\n",
        "model.cuda();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6501d2d63566470c9f6d941b4556fe2f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c24ce23cb8274e19b9a6d7b5bdc24a7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVkEhyA6b_z7",
        "colab_type": "text"
      },
      "source": [
        "#### **STAGE 2: Fine-tuning**\n",
        "\n",
        "Prior to the fine-tuning process, the optimizer (**AdamW**, Implements Adam algorithm with weight decay fix) and parameters to update must be setup. In order to avoid overfitting, some regularization will be applied by including weight_decay to the main weight matrices. \n",
        "\n",
        "If you have limited resources, you can also try to just train the linear classifier on top of BERT and keep all other weights fixed. This will still give you a good performance.\n",
        "\n",
        "The **AdamW** optimizer has parameters:\n",
        "- lr (float) – learning rate. Default 1e-3.\n",
        "- betas (tuple of 2 floats) – Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n",
        "- eps (float) – Adams epsilon. Default: 1e-6\n",
        "- weight_decay (float) – Weight decay. Default: 0.0\n",
        "- correct_bias (bool) – can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68TJYGo1cEew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FULL_FINETUNING = True\n",
        "\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5, # learning rate\n",
        "    eps=1e-8\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcIm76NEcH7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 3\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler. Used to linearly reduce the learning rate throughout the epochs.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1EeZo_wcL52",
        "colab_type": "text"
      },
      "source": [
        "#### **STAGE 3: Fit BERT for named entity recognition**\n",
        "\n",
        "f1_score from the seqeval, accuracy is measured on token level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-aQlLsOcj2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "7d54ac2a-b38e-438c-8245-7802797dced0"
      },
      "source": [
        "!pip install seqeval\n",
        "import seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=10ff880c22ad9bbd48d14f88ed3745f1038cc710bdfc5319ae5071ae4ef1c817\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVj8qQaqgNZM",
        "colab_type": "text"
      },
      "source": [
        "#### **Train and evaluate the BERT model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfGDUmqLD62F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seqeval.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytTM0BnOcY8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seqeval.metrics import f1_score, accuracy_score\n",
        "from tqdm import tnrange as trange\n",
        "\n",
        "## Store the average loss after each epoch to plot them.\n",
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        model.zero_grad() # reset gradients before backward pass\n",
        "\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    # Put the model into evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Reset the validation loss for this epoch.\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients,\n",
        "        # saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have not provided labels.\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "            \n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Compute the accuracy for this batch of test sentences.\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "    valid_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3Aedwcyezb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1_score(pred_tags, valid_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iOBKwlkYZ_M",
        "colab_type": "text"
      },
      "source": [
        "#### **Function definitions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZftu3M0YRrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(actual, predicted):\n",
        "    #classes       = np.unique(np.concatenate((actual,predicted)))\n",
        "    classes = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR', 'O']\n",
        "    confusion_mtx = np.empty((len(classes),len(classes)),dtype=np.int)\n",
        "    for i,a in enumerate(classes):\n",
        "        for j,p in enumerate(classes):\n",
        "            value = sum([sum([np.where((actual[sent][word]==a)*(predicted[sent][word]==p))[0].shape[0] \n",
        "                              for word in range(len(actual[sent]))]) for sent in range(len(actual))])\n",
        "            #confusion_mtx[i,j] = sum([np.where((actual[sent]==a)*(predicted[sent]==p))[0].shape[0] for sent in range(len(actual))])\n",
        "            confusion_mtx[i,j] = value\n",
        "    return confusion_mtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOUrDc4oYwT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code to extract METRICS FOR ENTITY AND NON-ENTITY from the confusion matrix\n",
        "\n",
        "def metrics_from_cm(cm,labels):\n",
        "  TP = [[v for v in value[0:len(labels)]] for value in cm[0:len(labels)]]\n",
        "  print(TP)\n",
        "  TP = np.array(TP)\n",
        "  TP = sum(sum(TP))\n",
        "\n",
        "  FN = [value[-1] for value in cm[0:len(labels)]] # last column is O\n",
        "  print(FN)\n",
        "  FN = np.array(FN) \n",
        "  FN = sum(FN)\n",
        "\n",
        "  FP = cm[len(labels)][0:len(labels)] # last column is O\n",
        "  print(FP)\n",
        "  FP = sum(FP)\n",
        "\n",
        "  TN = cm[len(labels)][len(labels)] # last column is O\n",
        "  print(TN)\n",
        "\n",
        "  return TP, FN, FP, TN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5A6Kl5nDZXz",
        "colab_type": "text"
      },
      "source": [
        "#### **STAGE 4: Apply the fine-tuned model over the validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNPQAfcRNz27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pv3_vTjyPTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_valid = flat_classification_report(y_true = [valid_tags], y_pred = [pred_tags], labels = labels)\n",
        "print(report_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shrz_YL6zDgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_matrix_no_sent(actual, predicted):\n",
        "    #classes       = np.unique(np.concatenate((actual,predicted)))\n",
        "    classes = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR', 'O']\n",
        "    confusion_mtx = np.empty((len(classes),len(classes)),dtype=np.int)\n",
        "    for i,a in enumerate(classes):\n",
        "        for j,p in enumerate(classes):\n",
        "            value = sum([np.where((actual[word]==a)*(predicted[word]==p))[0].shape[0] \n",
        "                              for word in range(len(actual))]) \n",
        "            #confusion_mtx[i,j] = sum([np.where((actual[sent]==a)*(predicted[sent]==p))[0].shape[0] for sent in range(len(actual))])\n",
        "            confusion_mtx[i,j] = value\n",
        "    return confusion_mtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE4EHeev0VB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not done by clinical case\n",
        "actual    = np.array(valid_tags)\n",
        "predicted = np.array(pred_tags)\n",
        "confusion_matrix_valid = confusion_matrix_no_sent(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(confusion_matrix_valid, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH-zUKmYe-eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(confusion_matrix_valid,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0dSxJ7c8gz",
        "colab_type": "text"
      },
      "source": [
        "#### **STAGE 5: Apply the fine-tuned model over the development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEDrXQKHgV1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words(tokens_sent,label_indx_sent, true_label_indx_sent, start_char_pos, tag_values):\n",
        "  new_tok, new_lab, true_lab, new_start_pos = [], [], [], []\n",
        "\n",
        "  for tokens, label_indices, true_label_indices, start_chars in zip(tokens_sent, label_indx_sent, true_label_indx_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, true_lab_aux, new_start_pos_aux = [], [], [], []\n",
        "    for token, label_idx, true_label_idx, start_char_i in zip(tokens, label_indices,true_label_indices,start_chars):\n",
        "      if tag_values[true_label_idx] != \"PAD\":\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tok_aux[-1] = new_tok_aux[-1] + token[2:]\n",
        "        else:\n",
        "            new_lab_aux.append(tag_values[label_idx])\n",
        "            true_lab_aux.append(tag_values[true_label_idx])\n",
        "            new_tok_aux.append(token)\n",
        "            new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    true_lab.append(true_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, true_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GpZCewUXhLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_words_test(tokens_sent,label_indx_sent, start_char_pos, tag_values):\n",
        "  new_tok, new_lab, new_start_pos = [], [], []\n",
        "\n",
        "  for tokens, label_indices, start_chars in zip(tokens_sent, label_indx_sent, start_char_pos):\n",
        "    new_tok_aux, new_lab_aux, new_start_pos_aux = [], [], []\n",
        "    for token, label_idx, start_char_i in zip(tokens, label_indices, start_chars):\n",
        "      if token != '[PAD]':\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tok_aux[-1] = new_tok_aux[-1] + token[2:]\n",
        "        else:\n",
        "            new_lab_aux.append(tag_values[label_idx])\n",
        "            new_tok_aux.append(token)\n",
        "            new_start_pos_aux.append(start_char_i)\n",
        "\n",
        "    new_lab.append(new_lab_aux)\n",
        "    new_tok.append(new_tok_aux)\n",
        "    new_start_pos.append(new_start_pos_aux)\n",
        "\n",
        "  return new_tok, new_lab, new_start_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfiQM-c3EeYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels used in the classification report\n",
        "labels = ['B-MOR', 'I-MOR', 'E-MOR', 'S-MOR', 'V-MOR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-XydX41c_gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenized_sentence_dev = tokenizer.encode(sentences_bert_dev[0])\n",
        "#input_ids_dev2 = torch.tensor([tokenized_sentence_dev]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-U2UZGX4owo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#employ: (already defined)\n",
        "\n",
        "# text: dev_inputs = torch.tensor(dev_inputs)\n",
        "\n",
        "# tags: dev_tags = torch.tensor(dev_tags)\n",
        "\n",
        "# masks: dev_masks = torch.tensor(dev_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2-Fr0QlkaWs",
        "colab_type": "text"
      },
      "source": [
        "**Predictions over development set**\n",
        "\n",
        "The predictions over the development set are obtained by clinical case. Therefore, loop over all clinical cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7N507BQiUv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop over all the clinical cases because all do not fit in memory\n",
        "\n",
        "#input_ids_dev2 = torch.tensor(input_ids_dev[0:10]).cuda()\n",
        "label_indices_sent_cc = []\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(dev_inputs_by_cc)):\n",
        "  dev_in = torch.tensor(dev_inputs_by_cc[cc]).cuda()\n",
        "  dev_m = torch.tensor(dev_masks_by_cc[cc]).cuda()\n",
        "  with torch.no_grad(): # we don't want to optimize\n",
        "    output = model(dev_in, token_type_ids = None, attention_mask = dev_m)\n",
        "    #logits = output[1].detach().cpu().numpy()\n",
        "  label_indices_sent_cc.append(np.argmax(output[0].to('cpu').numpy(), axis=2))\n",
        "  #label_indices_sent_cc.append([list(p) for p in np.argmax(logits, axis=2)])\n",
        "\n",
        "# convert input sequence into tokens\n",
        "  tokens_sent_cc.append([tokenizer.convert_ids_to_tokens(ids.to('cpu').numpy()) for ids in dev_in])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n6yPhIYABLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, true_labels_cc, new_start_pos_cc = [], [], [], []\n",
        "new_tokens_all, new_labels_all, true_labels_all, new_start_pos_all = [], [], [], []\n",
        "\n",
        "#dev_tags_by_cc contains the true labels of each token\n",
        "\n",
        "for cc in range(len(dev_inputs_by_cc)):\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = [], [], [], []\n",
        "  new_tokens, new_labels, true_labels, new_start_pos = tokens_to_words(tokens_sent_cc[cc], \n",
        "                                        label_indices_sent_cc[cc], dev_tags_by_cc[cc], \n",
        "                                        start_dev_by_cc[cc],tag_values)\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  true_labels_cc.append(true_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  true_labels_all.extend(true_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK_LC304vryP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "74097fa7-a843-421c-db1a-de23a2196f11"
      },
      "source": [
        "!ls 'drive/My Drive/Ejemplos NER - TFM/results/dev/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cc_onco100.ann\tcc_onco106.ann\tnew_labels_cc\t  new_tokens_cc\n",
            "cc_onco101.ann\tex2.ann\t\tnew_start_pos_cc  true_labels_cc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dFlCoAnvn8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(path+'results/dev/new_tokens_cc', 'wb') as file: \n",
        "  pkl.dump(new_tokens_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results/dev/new_labels_cc', 'wb') as file: \n",
        "  pkl.dump(new_labels_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results/dev/true_labels_cc', 'wb') as file: \n",
        "  pkl.dump(true_labels_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results/dev/new_start_pos_cc', 'wb') as file: \n",
        "  pkl.dump(new_start_pos_cc, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaPNd05Mi2hj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20e2b19a-b43e-4270-b696-75357d674a8c"
      },
      "source": [
        "len(new_start_pos_cc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUpj8EtTAhVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "eccbdd45-4a9e-45e4-8cce-3c5b7ec52d6d"
      },
      "source": [
        "# Example:\n",
        "for token, label, true_label, new_start in zip(new_tokens_cc[0][0], new_labels_cc[0][0], \n",
        "                                               true_labels_cc[0][0], new_start_pos_cc[0][0]):\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(true_label, label, token,new_start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\tO\tAnamnesis\t0\n",
            "O\tO\tVarón\t10\n",
            "O\tO\tde\t16\n",
            "O\tO\t74\t19\n",
            "O\tO\taños\t22\n",
            "O\tO\t,\t26\n",
            "O\tO\texfumador\t28\n",
            "O\tO\tdesde\t38\n",
            "O\tO\thace\t44\n",
            "O\tO\t15\t49\n",
            "O\tO\taños\t52\n",
            "O\tO\t,\t56\n",
            "O\tO\tcon\t58\n",
            "O\tO\túnico\t62\n",
            "O\tO\tantecedente\t68\n",
            "O\tO\tde\t80\n",
            "O\tO\thipertensión\t83\n",
            "O\tO\t,\t95\n",
            "O\tO\tdislipemia\t97\n",
            "O\tO\ty\t108\n",
            "O\tO\tapendicectomizado\t110\n",
            "O\tO\t;\t127\n",
            "O\tO\tse\t129\n",
            "O\tO\tdiagnostica\t132\n",
            "O\tO\ten\t144\n",
            "O\tO\tmarzo\t147\n",
            "O\tO\tde\t153\n",
            "O\tO\t2013\t156\n",
            "O\tO\tde\t161\n",
            "B-MOR\tB-MOR\tcarcinoma\t164\n",
            "I-MOR\tI-MOR\tde\t174\n",
            "I-MOR\tI-MOR\tcélulas\t177\n",
            "I-MOR\tI-MOR\ttransicionales\t185\n",
            "I-MOR\tI-MOR\tde\t200\n",
            "I-MOR\tI-MOR\tvejiga\t203\n",
            "I-MOR\tI-MOR\tE\t210\n",
            "I-MOR\tI-MOR\t-\t210\n",
            "I-MOR\tE-MOR\tIV\t210\n",
            "E-MOR\tO\t(\t215\n",
            "O\tO\tpulmonares\t216\n",
            "O\tO\ty\t227\n",
            "O\tO\tóseas\t229\n",
            "O\tO\t)\t234\n",
            "O\tO\t.\t235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy6V8pPbDsGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "128c32d4-b341-4b37-d547-27eb545762cb"
      },
      "source": [
        "report_dev = flat_classification_report(y_true = true_labels_all, y_pred = new_labels_all, labels = labels)\n",
        "print(report_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-MOR       0.82      0.79      0.81      1525\n",
            "       I-MOR       0.78      0.68      0.73      2420\n",
            "       E-MOR       0.73      0.71      0.72      1550\n",
            "       S-MOR       0.90      0.90      0.90      1613\n",
            "       V-MOR       0.00      0.00      0.00        18\n",
            "\n",
            "   micro avg       0.81      0.76      0.78      7126\n",
            "   macro avg       0.65      0.62      0.63      7126\n",
            "weighted avg       0.80      0.76      0.78      7126\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9OuVokSYmst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "66e820b0-4a8c-4f0b-d2b9-c2289908cc44"
      },
      "source": [
        "# not done by clinical case\n",
        "actual    = np.array(true_labels_all)\n",
        "predicted = np.array(new_labels_all)\n",
        "confusion_matrix_dev = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(confusion_matrix_dev, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>1209</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>26</td>\n",
              "      <td>1652</td>\n",
              "      <td>125</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>1097</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>1449</td>\n",
              "      <td>0</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>193</td>\n",
              "      <td>383</td>\n",
              "      <td>246</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>208101</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR       O\n",
              "B-MOR   1209     11      1     63      0     241\n",
              "I-MOR     26   1652    125      9      0     608\n",
              "E-MOR      0     67   1097     30      0     356\n",
              "S-MOR     46      0     25   1449      0      93\n",
              "V-MOR      3      7      3      4      0       1\n",
              "O        193    383    246     60      0  208101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjH0FpSoYvDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "862e1ecf-8ee8-401e-d819-64c477febde1"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(confusion_matrix_dev,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1209, 11, 1, 63, 0], [26, 1652, 125, 9, 0], [0, 67, 1097, 30, 0], [46, 0, 25, 1449, 0], [3, 7, 3, 4, 0]]\n",
            "[241, 608, 356, 93, 1]\n",
            "[193 383 246  60   0]\n",
            "208101\n",
            "5827\n",
            "1299\n",
            "882\n",
            "208101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z01HVfRKVJt0",
        "colab_type": "text"
      },
      "source": [
        "**Predictions over development set 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Eu3-6NVQ2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e3f41dd6-072d-4a75-e8af-dd958f4ceaae"
      },
      "source": [
        "label_indices_sent_cc2 = []\n",
        "tokens_sent_cc2 = []\n",
        "for cc in range(len(dev_inputs_by_cc2)):\n",
        "  dev_in = torch.tensor(dev_inputs_by_cc2[cc]).cuda()\n",
        "  dev_m = torch.tensor(dev_masks_by_cc2[cc]).cuda()\n",
        "  with torch.no_grad(): # we don't want to optimize\n",
        "    output = model(dev_in, token_type_ids = None, attention_mask = dev_m)\n",
        "    #logits = output[1].detach().cpu().numpy()\n",
        "  label_indices_sent_cc2.append(np.argmax(output[0].to('cpu').numpy(), axis=2))\n",
        "  #label_indices_sent_cc.append([list(p) for p in np.argmax(logits, axis=2)])\n",
        "\n",
        "# convert input sequence into tokens\n",
        "  tokens_sent_cc2.append([tokenizer.convert_ids_to_tokens(ids.to('cpu').numpy()) for ids in dev_in])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5SiyEXtWRjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b395c85b-ff9e-4a5d-88ed-c980bc4f8d08"
      },
      "source": [
        "tag_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'V-MOR', 'B-MOR', 'E-MOR', 'I-MOR', 'S-MOR', 'PAD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcALqLrkWB2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, true_labels_cc = [], [], []\n",
        "new_tokens_all, new_labels_all, true_labels_all = [], [], []\n",
        "\n",
        "#dev_tags_by_cc2 contains the true labels of each token\n",
        "\n",
        "for cc in range(len(dev_inputs_by_cc2)):\n",
        "  new_tokens, new_labels, true_labels = [], [], []\n",
        "  new_tokens, new_labels, true_labels = tokens_to_words(tokens_sent_cc2[cc], \n",
        "                                        label_indices_sent_cc2[cc], dev_tags_by_cc2[cc],tag_values)\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  true_labels_cc.append(true_labels)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  true_labels_all.extend(true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evu9NHNqWXSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "242a0eab-4aee-481f-f641-b5519825a883"
      },
      "source": [
        "# Example:\n",
        "for token, label, true_label in zip(new_tokens_cc[1][5], new_labels_cc[1][5], true_labels_cc[1][5]):\n",
        "    print(\"{}\\t{}\\t{}\".format(true_label, label, token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\tO\tDiagnóstico\n",
            "O\tO\tPor\n",
            "O\tO\ttanto\n",
            "O\tO\t,\n",
            "O\tO\tnos\n",
            "O\tO\tencontramos\n",
            "O\tO\tante\n",
            "O\tO\tel\n",
            "O\tO\tcaso\n",
            "O\tO\tde\n",
            "O\tO\tun\n",
            "B-MOR\tB-MOR\tsarcoma\n",
            "I-MOR\tI-MOR\tde\n",
            "E-MOR\tE-MOR\tKaposi\n",
            "O\tO\tclásico\n",
            "O\tO\tdel\n",
            "O\tO\tadulto\n",
            "O\tO\t,\n",
            "O\tO\tconfirmado\n",
            "O\tO\tpor\n",
            "O\tO\testudio\n",
            "O\tO\tanatomopatológico\n",
            "O\tO\t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwB77B23Wmli",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "e421eb28-4fff-4d9a-a97f-dd4384fec6ca"
      },
      "source": [
        "report_dev = flat_classification_report(y_true = true_labels_all, y_pred = new_labels_all, labels = labels)\n",
        "print(report_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-MOR       0.80      0.86      0.83      1236\n",
            "       I-MOR       0.77      0.75      0.76      2144\n",
            "       E-MOR       0.72      0.74      0.73      1264\n",
            "       S-MOR       0.91      0.91      0.91      1331\n",
            "       V-MOR       0.00      0.00      0.00         3\n",
            "\n",
            "   micro avg       0.80      0.80      0.80      5978\n",
            "   macro avg       0.64      0.65      0.65      5978\n",
            "weighted avg       0.80      0.80      0.80      5978\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESuLj6G7Wp6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d1d789d1-32a5-4572-c8b8-42d0f6c0aa2a"
      },
      "source": [
        "# not done by clinical case\n",
        "actual    = np.array(true_labels_all)\n",
        "predicted = np.array(new_labels_all)\n",
        "confusion_matrix_dev = confusion_matrix(actual,predicted)\n",
        "\n",
        "df = pd.DataFrame(confusion_matrix_dev, columns = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'], index = ['B-MOR', 'I-MOR','E-MOR','S-MOR','V-MOR','O'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B-MOR</th>\n",
              "      <th>I-MOR</th>\n",
              "      <th>E-MOR</th>\n",
              "      <th>S-MOR</th>\n",
              "      <th>V-MOR</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-MOR</th>\n",
              "      <td>1058</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MOR</th>\n",
              "      <td>18</td>\n",
              "      <td>1612</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E-MOR</th>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "      <td>931</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S-MOR</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>1211</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V-MOR</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>199</td>\n",
              "      <td>373</td>\n",
              "      <td>244</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>170945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       B-MOR  I-MOR  E-MOR  S-MOR  V-MOR       O\n",
              "B-MOR   1058     13      1     48      0     116\n",
              "I-MOR     18   1612     90      3      0     421\n",
              "E-MOR      1     87    931     22      0     223\n",
              "S-MOR     48      1     27   1211      0      44\n",
              "V-MOR      1      0      1      1      0       0\n",
              "O        199    373    244     47      0  170945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7YjMfRyWt2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "05a4f36c-3b00-450f-9005-2b82b5c57aa4"
      },
      "source": [
        "TP, FN, FP, TN = metrics_from_cm(confusion_matrix_dev,labels)\n",
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print(TN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1058, 13, 1, 48, 0], [18, 1612, 90, 3, 0], [1, 87, 931, 22, 0], [48, 1, 27, 1211, 0], [1, 0, 1, 1, 0]]\n",
            "[116, 421, 223, 44, 0]\n",
            "[199 373 244  47   0]\n",
            "170945\n",
            "5174\n",
            "804\n",
            "863\n",
            "170945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-38hYStLgRYn",
        "colab_type": "text"
      },
      "source": [
        "#### **STAGE 6: Final Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2wXpeGzrUMA",
        "colab_type": "text"
      },
      "source": [
        "##### **Tokenize Complete dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98kw-jD2rSJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "581db519-3e4b-43ab-98f3-e0fa37dba80a"
      },
      "source": [
        "print(\"Number of words in the first sentence: %d\" %len(sentences_bert_complete[0]))\n",
        "\n",
        "# 1. Tokenize each sentence and keep text and labels\n",
        "tokenized_texts_and_labels_complete = [tokenize_and_preserve_labels(sent, labs,start) for sent, labs, start \n",
        "                                    in zip(sentences_bert_complete, labels_bert_complete, start_bert_complete)]\n",
        "print(\"Number of words in the first sentence after tokenizing: %d\" %len(tokenized_texts_and_labels_complete[0][0]))\n",
        "print(tokenized_texts_and_labels_complete[0][0]) # first sentence; text part\n",
        "print(tokenized_texts_and_labels_complete[0][1]) # first sentence; label part (Contains both the sentences and the labels (tags))\n",
        "print(tokenized_texts_and_labels_complete[0][2]) # first sentence; start position part (Contains both the sentences and the labels (tags))\n",
        "\n",
        "\n",
        "# 2. Split each tokenized sentence into text and labels\n",
        "tokenized_texts_complete = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_complete]\n",
        "labels_complete = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_complete]\n",
        "start_pos_complete = [token_label_pair[2] for token_label_pair in tokenized_texts_and_labels_complete]\n",
        "\n",
        "print()\n",
        "print(tokenized_texts_complete[0])\n",
        "\n",
        "\n",
        "# 3. Cut and pad the token and label sequences to the desired length. (max_len: max sequence length = 75 tokens)\n",
        "input_ids_complete = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_complete],\n",
        "                          maxlen=max_len, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags_complete = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_complete],\n",
        "                     maxlen=max_len, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "# tokenizer.convert_tokens_to_ids(txt) is a built-in function that converts a token string \n",
        "# (or a sequence of tokens) into a single integer id (or a sequence of ids),\n",
        "# using the pre-defined vocabulary in tokenizer.\n",
        "print(\"\\nID representation of the first sentence: \\n %s\" %input_ids_complete[0])\n",
        "\n",
        "\n",
        "# 4. Creation of attention masks to IGNORE certain words (padded elements)\n",
        "# If the ID of a word is 0, that word is \"PAD\", a mask must be created to IGNORE such word (to ignore padded elements)\n",
        "attention_masks_complete = [[float(i != 0.0) for i in ii] for ii in input_ids_complete] # it's an array of arrays\n",
        "print(\"\\nAttention mask of the first sentence: \\n %s\" %attention_masks_complete[0])\n",
        "\n",
        "\n",
        "# 5. Split train dataset into train and validation\n",
        "train_inputs = input_ids_complete\n",
        "train_tags = tags_complete\n",
        "train_masks = attention_masks_complete\n",
        "#train_inputs, valid_inputs, train_tags, valid_tags = train_test_split(input_ids_complete, tags_complete, random_state=0, test_size=0.2)\n",
        "#train_masks, valid_masks, _, _ = train_test_split(attention_masks_complete, input_ids_complete, random_state=0, test_size=0.2)\n",
        "# _: used to not store tags of masked words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the first sentence: 24\n",
            "Number of words in the first sentence after tokenizing: 43\n",
            "['AN', '##AM', '##NE', '##SI', '##S', 'Mujer', 'de', '67', 'años', 'con', 'ante', '##cedent', '##es', 'personales', 'de', 'hip', '##oti', '##roi', '##dis', '##mo', 'en', 'tratamiento', 'con', 'le', '##vot', '##iro', '##xin', '##a', 'y', 'fu', '##mad', '##ora', 'activa', 'de', '12', '.', '5', 'pa', '##quet', '##es', '/', 'año', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[0, 0, 0, 0, 0, 10, 16, 19, 22, 27, 31, 31, 31, 44, 55, 58, 58, 58, 58, 58, 73, 76, 88, 92, 92, 92, 92, 92, 105, 107, 107, 107, 116, 123, 126, 126, 126, 131, 131, 131, 139, 140, 143]\n",
            "\n",
            "['AN', '##AM', '##NE', '##SI', '##S', 'Mujer', 'de', '67', 'años', 'con', 'ante', '##cedent', '##es', 'personales', 'de', 'hip', '##oti', '##roi', '##dis', '##mo', 'en', 'tratamiento', 'con', 'le', '##vot', '##iro', '##xin', '##a', 'y', 'fu', '##mad', '##ora', 'activa', 'de', '12', '.', '5', 'pa', '##quet', '##es', '/', 'año', '.']\n",
            "\n",
            "ID representation of the first sentence: \n",
            " [ 50972  36535  93280  44802  10731  95391  10104  12316  11278  10173\n",
            "  15865 104101  10171  90770  10104  25377  23841  56859  17442  11033\n",
            "  10110  56708  10173  10141  63129  14213  76750  10113    193  11005\n",
            "  42998  14945  50796  10104  10186    119    126  10931  27579  10171\n",
            "    120  11734    119      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]\n",
            "\n",
            "Attention mask of the first sentence: \n",
            " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EY3sOtK6hf8N"
      },
      "source": [
        "##### **Convert data into tensors**\n",
        "**Complete dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cuGNrNaphf8O",
        "colab": {}
      },
      "source": [
        "# Convert to TENSORS (working with pytorch)\n",
        "\n",
        "# text\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "#valid_inputs = torch.tensor(valid_inputs)\n",
        "\n",
        "# tags\n",
        "train_tags = torch.tensor(train_tags)\n",
        "#valid_tags = torch.tensor(valid_tags)\n",
        "\n",
        "# masks\n",
        "train_masks = torch.tensor(train_masks)\n",
        "#valid_masks = torch.tensor(valid_masks)\n",
        "\n",
        "\n",
        "# Define DATALOADERS\n",
        "# shuffle the data of train set with RandomSampler \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
        "train_sampler = RandomSampler(train_data, ) # it just shuffles wihtout selecting a subset since num_samples = None\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "#valid_data = TensorDataset(valid_inputs, valid_masks, valid_tags)\n",
        "#valid_sampler = SequentialSampler(valid_data) # validation data is passed sequentially\n",
        "#valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBL5owMwhB17",
        "colab_type": "text"
      },
      "source": [
        "##### **Train final model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntaJ9GGCAM-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236,
          "referenced_widgets": [
            "18d49402014c4f5abaf21393b659cc88",
            "c9b34ea9d3bc4cebbf99fe9e42b96e6a",
            "548f2b2a4fb4414eb7a4afdb99186a7e",
            "fbea8b2da498496fad36d4f71190e194",
            "aaf93b3abb124b999275d7f2fcc0f70e",
            "7ad22626dea04f09acf932c3f2d6f2c6",
            "413f241afef94b8f8bd02d9fd7dd7f5c",
            "e7253fdd42014bde92b3645168efe113"
          ]
        },
        "outputId": "91fccb8c-05e7-4455-9c0a-c73c3088842b"
      },
      "source": [
        "from seqeval.metrics import f1_score, accuracy_score\n",
        "from tqdm import tnrange as trange\n",
        "\n",
        "## Store the average loss after each epoch to plot them.\n",
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        model.zero_grad() # reset gradients before backward pass\n",
        "\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18d49402014c4f5abaf21393b659cc88",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.0784052338540925\n",
            "Average train loss: 0.03301818041260141\n",
            "Average train loss: 0.018422133449737085\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfFk461tWvRU",
        "colab_type": "text"
      },
      "source": [
        "##### **Predictions over the test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUb2LCABW560",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_indices_sent_cc = [] # These are the predictions\n",
        "tokens_sent_cc = []\n",
        "for cc in range(len(test_inputs_by_cc)):\n",
        "  test_in = torch.tensor(test_inputs_by_cc[cc]).cuda()\n",
        "  test_m = torch.tensor(test_masks_by_cc[cc]).cuda()\n",
        "  with torch.no_grad(): # we don't want to optimize\n",
        "    output = model(test_in, token_type_ids = None, attention_mask = test_m)\n",
        "    #logits = output[1].detach().cpu().numpy()\n",
        "  label_indices_sent_cc.append(np.argmax(output[0].to('cpu').numpy(), axis=2))\n",
        "  #label_indices_sent_cc.append([list(p) for p in np.argmax(logits, axis=2)])\n",
        "\n",
        "# convert input sequence into tokens\n",
        "  tokens_sent_cc.append([tokenizer.convert_ids_to_tokens(ids.to('cpu').numpy()) for ids in test_in])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrdigZ6xXTVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokens_cc, new_labels_cc, new_start_pos_cc = [], [], []\n",
        "new_tokens_all, new_labels_all, new_start_pos_all = [], [], []\n",
        "\n",
        "#dev_tags_by_cc2 contains the true labels of each token\n",
        "\n",
        "for cc in range(len(test_inputs_by_cc)):\n",
        "  new_tokens, new_labels, new_start_pos = [], [], []\n",
        "  new_tokens, new_labels, new_start_pos = tokens_to_words_test(tokens_sent_cc[cc], \n",
        "                                        label_indices_sent_cc[cc], start_test_by_cc[cc], tag_values)\n",
        "  new_tokens_cc.append(new_tokens)\n",
        "  new_labels_cc.append(new_labels)\n",
        "  new_start_pos_cc.append(new_start_pos)\n",
        "\n",
        "  new_tokens_all.extend(new_tokens)\n",
        "  new_labels_all.extend(new_labels)\n",
        "  new_start_pos_all.extend(new_start_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8sKy2hOXV0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "fa353f74-305a-4d8f-a73f-739e99f54aab"
      },
      "source": [
        "# Example:\n",
        "for token, label, new_start in zip(new_tokens_cc[6][5], new_labels_cc[6][5], new_start_pos_cc[6][5]):\n",
        "    print(\"{}\\t{}\\t{}\".format(label, token,new_start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\tA\t593\n",
            "O\tpesar\t595\n",
            "O\tde\t601\n",
            "O\ttener\t604\n",
            "O\tunos\t610\n",
            "O\tíndices\t615\n",
            "O\televados\t623\n",
            "O\tde\t632\n",
            "O\tcreatina\t635\n",
            "O\t-\t635\n",
            "O\tcolina\t635\n",
            "O\t/\t650\n",
            "O\tcitrato\t651\n",
            "O\tsugestivos\t659\n",
            "O\tde\t670\n",
            "S-MOR\tneoplasia\t673\n",
            "O\t,\t682\n",
            "O\tlos\t684\n",
            "O\tvoxels\t688\n",
            "O\t36\t695\n",
            "O\ty\t698\n",
            "O\t45\t700\n",
            "O\tson\t703\n",
            "O\tpoco\t707\n",
            "O\tvalorables\t712\n",
            "O\tpuesto\t723\n",
            "O\tque\t730\n",
            "O\tcorresponden\t734\n",
            "O\ta\t747\n",
            "O\tla\t749\n",
            "O\tzona\t752\n",
            "O\tde\t757\n",
            "O\turetra\t760\n",
            "O\tprostática\t767\n",
            "O\t.\t777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVM-TS9c6eX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c62ce75-d026-432d-923c-4546f548e590"
      },
      "source": [
        "!ls 'drive/My Drive/Ejemplos NER - TFM/results_bert/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ann  predictions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJqBQyHy43UI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(path+'results_bert/predictions/new_tokens_cc', 'wb') as file: \n",
        "  pkl.dump(new_tokens_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_bert/predictions/new_labels_cc', 'wb') as file: \n",
        "  pkl.dump(new_labels_cc, file)\n",
        "file.close()\n",
        "\n",
        "with open(path+'results_bert/predictions/new_start_pos_cc', 'wb') as file: \n",
        "  pkl.dump(new_start_pos_cc, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}